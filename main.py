from flask import Flask, request, jsonify
import os
from flask_cors import CORS
import openai
from openai import OpenAI as _OpenAIClient
from FunMultiCalculos import processar_multiplos_arquivos, processar_multiplos_arquivos_comparativo
from Correlacao import *
from FunCalculos import carregar_csv, calcular_performance, calcular_day_of_week, calcular_monthly, processar_backtest_completo, calcular_dados_grafico, _normalize_trades_dataframe
import dotenv
import os.path as _path
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from typing import Dict, List, Any, Tuple, Optional
from pathlib import Path
from functools import wraps
import jwt
from supabase import create_client, Client

# Carregar vari√°veis de ambiente de m√∫ltiplas localiza√ß√µes para maior robustez
# Tenta carregar de v√°rios locais poss√≠veis
env_loaded = False
base_paths = [
    _path.dirname(__file__),  # devhubback/
    _path.join(_path.dirname(__file__), '..'),  # python-freela/
    os.getcwd(),  # diret√≥rio atual de execu√ß√£o
]

# Primeiro tenta carregar .env, depois tenta .env.backup.*
for base_path in base_paths:
    env_path = _path.join(base_path, '.env')
    if _path.exists(env_path):
        result = dotenv.load_dotenv(dotenv_path=env_path, override=False)
        if result:
            print(f"[INFO] Arquivo .env carregado de: {_path.abspath(env_path)}")
            env_loaded = True
            break

# Se n√£o encontrou .env, tenta arquivos de backup
if not env_loaded:
    import glob
    for base_path in base_paths:
        backup_pattern = _path.join(base_path, '.env.backup.*')
        backup_files = glob.glob(backup_pattern)
        if backup_files:
            # Pega o mais recente
            backup_files.sort(reverse=True)
            result = dotenv.load_dotenv(dotenv_path=backup_files[0], override=False)
            if result:
                print(f"[INFO] Arquivo .env carregado de backup: {_path.abspath(backup_files[0])}")
                env_loaded = True
                break

# Se ainda n√£o encontrou, tenta o padr√£o do python-dotenv
if not env_loaded:
    result = dotenv.load_dotenv()
    if result:
        print(f"[INFO] Arquivo .env carregado do diret√≥rio atual: {os.getcwd()}")
        env_loaded = True

if not env_loaded:
    print("[WARN] Nenhum arquivo .env encontrado. Verifique se o arquivo existe em:")
    for base_path in base_paths:
        print(f"  - {_path.abspath(_path.join(base_path, '.env'))}")

# main.py
app = Flask(__name__)

# Configura√ß√£o CORS para permitir acesso do frontend
CORS(app, 
     resources={r"/api/*": {
         "origins": [
             'http://localhost:4173',
             'http://localhost:5173',
             'http://localhost:5174',
             'http://localhost:3000',
             'https://devhubtrader.com.br',
             'https://www.devhubtrader.com.br',
             'http://devhubtrader.com.br',
             'http://www.devhubtrader.com.br'
         ],
         "methods": ["GET", "POST", "PUT", "DELETE", "OPTIONS", "PATCH"],
         "allow_headers": ["Content-Type", "Authorization", "x-openai-key", "X-Requested-With"],
         "supports_credentials": True,
         "max_age": 3600
     }},
     supports_credentials=True)

app.config['MAX_CONTENT_LENGTH'] = 16 * 1024 * 1024 
app.config['SEND_FILE_MAX_AGE_DEFAULT'] = 0

# Custom JSON provider para lidar com tipos numpy (Flask 2.3+)
from flask.json.provider import JSONProvider
import json

class NumpyJSONProvider(JSONProvider):
    def dumps(self, obj, **kwargs):
        return json.dumps(self._convert_numpy_types(obj), **kwargs)
    
    def loads(self, s, **kwargs):
        return json.loads(s, **kwargs)
    
    def _convert_numpy_types(self, obj):
        """Converte tipos numpy para tipos Python nativos"""
        if isinstance(obj, dict):
            return {str(k): self._convert_numpy_types(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [self._convert_numpy_types(item) for item in obj]
        elif isinstance(obj, (np.integer, np.int64, np.int32, np.int16, np.int8)):
            return int(obj)
        elif isinstance(obj, (np.floating, np.float64, np.float32, np.float16)):
            return float(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, np.bool_):
            return bool(obj)
        elif isinstance(obj, np.bytes_):
            return obj.decode('utf-8')
        elif pd.isna(obj) or obj is None:
            return None
        elif hasattr(obj, 'item'):
            return obj.item()
        elif isinstance(obj, (pd.Period, pd.Timestamp)):
            return str(obj)
        else:
            return obj

# Configurar o provider customizado
app.json_provider_class = NumpyJSONProvider

# Configura√ß√£o da chave da API do OpenAI (compat com SDK novo)
openai.api_key = os.getenv("OPENAI_API_KEY")
if not openai.api_key:
    print("[WARN] OPENAI_API_KEY n√£o encontrado nas vari√°veis de ambiente. Rotas que usam OpenAI ir√£o falhar at√© que seja configurado.")

# ============ CONFIGURA√á√ÉO SUPABASE ============
# Debug: verificar vari√°veis relacionadas ao Supabase no ambiente
supabase_vars = {
    "SUPABASE_URL": os.getenv("SUPABASE_URL"),
    "SUPABASE_SERVICE_ROLE_KEY": os.getenv("SUPABASE_SERVICE_ROLE_KEY"),
    "SUPABASE_ANON_KEY": os.getenv("SUPABASE_ANON_KEY"),
    "VITE_SUPABASE_URL": os.getenv("VITE_SUPABASE_URL"),  # Pode estar com prefixo VITE_
    "VITE_SUPABASE_ANON_KEY": os.getenv("VITE_SUPABASE_ANON_KEY"),
}
SUPABASE_JWT_SECRET = os.getenv("SUPABASE_JWT_SECRET")

# PRIORIDADE 1: SUPABASE_URL (vari√°vel padr√£o)
SUPABASE_URL = supabase_vars["SUPABASE_URL"]
# Se n√£o encontrou, tenta usar VITE_SUPABASE_URL
if not SUPABASE_URL and supabase_vars["VITE_SUPABASE_URL"]:
    SUPABASE_URL = supabase_vars["VITE_SUPABASE_URL"]
    print(f"[INFO] Usando VITE_SUPABASE_URL como SUPABASE_URL")

# Limpar e validar URL
if SUPABASE_URL:
    SUPABASE_URL = SUPABASE_URL.strip()
    # Validar formato b√°sico da URL
    if not SUPABASE_URL.startswith('http://') and not SUPABASE_URL.startswith('https://'):
        print(f"[ERROR] SUPABASE_URL inv√°lida (deve come√ßar com http:// ou https://): {SUPABASE_URL[:50]}...")
        SUPABASE_URL = None

# PRIORIDADE: Sempre usar SERVICE_ROLE_KEY primeiro (bypassa RLS)
# Se n√£o tiver, usar ANON_KEY como fallback (com aviso)
SUPABASE_KEY = None
if supabase_vars["SUPABASE_SERVICE_ROLE_KEY"]:
    SUPABASE_KEY = supabase_vars["SUPABASE_SERVICE_ROLE_KEY"].strip()
    print(f"[INFO] ‚úÖ Usando SUPABASE_SERVICE_ROLE_KEY (bypassa RLS)")
elif supabase_vars["SUPABASE_ANON_KEY"]:
    SUPABASE_KEY = supabase_vars["SUPABASE_ANON_KEY"].strip()
    print(f"[WARN] ‚ö†Ô∏è  Usando SUPABASE_ANON_KEY - opera√ß√µes podem falhar por RLS!")
    print(f"[WARN] Configure SUPABASE_SERVICE_ROLE_KEY no .env para bypassar RLS")
elif supabase_vars["VITE_SUPABASE_ANON_KEY"]:
    SUPABASE_KEY = supabase_vars["VITE_SUPABASE_ANON_KEY"].strip()
    print(f"[WARN] ‚ö†Ô∏è  Usando VITE_SUPABASE_ANON_KEY - opera√ß√µes podem falhar por RLS!")
    print(f"[WARN] Configure SUPABASE_SERVICE_ROLE_KEY no .env para bypassar RLS")

supabase_client: Optional[Client] = None
if SUPABASE_URL and SUPABASE_KEY:
    try:
        # Debug: mostrar URL e tamanho da chave (sem mostrar a chave completa)
        print(f"[DEBUG] Tentando conectar ao Supabase...")
        print(f"[DEBUG] URL: {SUPABASE_URL}")
        print(f"[DEBUG] Chave configurada: {'‚úì' if SUPABASE_KEY else '‚úó'} (tamanho: {len(SUPABASE_KEY) if SUPABASE_KEY else 0} caracteres)")
        
        # Verificar qual tipo de chave est√° sendo usada
        is_service_role = bool(supabase_vars["SUPABASE_SERVICE_ROLE_KEY"])
        if is_service_role:
            print("[INFO] Usando SUPABASE_SERVICE_ROLE_KEY (bypassa RLS)")
        else:
            print("[WARN] ‚ö†Ô∏è  USANDO ANON_KEY - opera√ß√µes podem falhar por RLS!")
            print("[WARN] Configure SUPABASE_SERVICE_ROLE_KEY no .env")
        
        supabase_client = create_client(SUPABASE_URL, SUPABASE_KEY)
        
        if is_service_role:
            print("[INFO] ‚úÖ Cliente Supabase inicializado com SERVICE_ROLE_KEY (bypassa RLS)")
        else:
            print("[WARN] ‚ö†Ô∏è  Cliente Supabase inicializado com ANON_KEY (respeita RLS)")
            print("[WARN] Opera√ß√µes administrativas podem falhar. Use SERVICE_ROLE_KEY.")
    except Exception as e:
        print(f"[WARN] Erro ao inicializar Supabase: {e}")
        print(f"[DEBUG] SUPABASE_URL recebida: '{SUPABASE_URL}' (tipo: {type(SUPABASE_URL)}, tamanho: {len(SUPABASE_URL) if SUPABASE_URL else 0})")
        print(f"[DEBUG] SUPABASE_KEY recebida: {'‚úì' if SUPABASE_KEY else '‚úó'} (tamanho: {len(SUPABASE_KEY) if SUPABASE_KEY else 0})")
        import traceback
        traceback.print_exc()
        print("[WARN] Continuando sem Supabase - rotas de usu√°rio n√£o funcionar√£o")
else:
    print("[WARN] Vari√°veis SUPABASE_URL ou SUPABASE_KEY n√£o encontradas.")
    print(f"[DEBUG] SUPABASE_URL: {'‚úì' if SUPABASE_URL else '‚úó'}, SUPABASE_KEY: {'‚úì' if SUPABASE_KEY else '‚úó'}")
    print("[DEBUG] Vari√°veis encontradas no ambiente:")
    for var_name, var_value in supabase_vars.items():
        print(f"  {var_name}: {'‚úì (definida)' if var_value else '‚úó (n√£o encontrada)'}")
    print("[WARN] Rotas de usu√°rio n√£o funcionar√£o.")

# ============ FUN√á√ïES HELPER PARA AUTENTICA√á√ÉO ============
def get_user_id_from_token() -> Optional[str]:
    """
    Extrai o user_id do token JWT do Supabase no header Authorization
    Retorna None se n√£o conseguir autenticar
    """
    auth_header = request.headers.get('Authorization')
    if not auth_header:
        return None
    
    try:
        # Formato: "Bearer <token>"
        token = auth_header.replace('Bearer ', '').strip()
        if not token:
            return None
        
        # Se temos JWT_SECRET, validar o token
        if SUPABASE_JWT_SECRET:
            try:
                decoded = jwt.decode(token, SUPABASE_JWT_SECRET, algorithms=['HS256'], options={"verify_signature": True})
                return decoded.get('sub')  # 'sub' √© o user_id no JWT do Supabase
            except jwt.ExpiredSignatureError:
                return None
            except jwt.InvalidTokenError:
                return None
        
        # Se n√£o temos JWT_SECRET, tentar decodificar sem valida√ß√£o (apenas para desenvolvimento)
        # Em produ√ß√£o, sempre use JWT_SECRET
        try:
            decoded = jwt.decode(token, options={"verify_signature": False})
            return decoded.get('sub')
        except:
            return None
            
    except Exception as e:
        print(f"[ERROR] Erro ao decodificar token: {e}")
        return None

def require_auth(f):
    """Decorator para rotas que requerem autentica√ß√£o"""
    @wraps(f)
    def decorated_function(*args, **kwargs):
        user_id = get_user_id_from_token()
        if not user_id:
            return jsonify({"error": "N√£o autenticado. Token inv√°lido ou ausente."}), 401
        # Adicionar user_id ao request para uso na fun√ß√£o
        request.user_id = user_id
        return f(*args, **kwargs)
    return decorated_function

def get_user_supabase_client() -> Optional[Client]:
    """
    Retorna um cliente Supabase para opera√ß√µes com RLS.
    Se SERVICE_ROLE_KEY estiver configurado, usa o cliente global (bypassa RLS).
    Caso contr√°rio, cria um cliente com ANON_KEY e passa o token do usu√°rio nos headers.
    """
    # Se estamos usando SERVICE_ROLE_KEY, o cliente global j√° bypassa RLS
    if supabase_vars.get("SUPABASE_SERVICE_ROLE_KEY"):
        return supabase_client
    
    # Se n√£o temos SERVICE_ROLE_KEY, precisamos usar o token do usu√°rio
    if not SUPABASE_URL:
        return None
    
    auth_header = request.headers.get('Authorization')
    if not auth_header:
        return None
    
    try:
        # Extrair o token
        token = auth_header.replace('Bearer ', '').strip()
        if not token:
            return None
        
        # Usar ANON_KEY para criar cliente
        anon_key = supabase_vars.get("SUPABASE_ANON_KEY") or supabase_vars.get("VITE_SUPABASE_ANON_KEY")
        if not anon_key:
            print("[WARN] ANON_KEY n√£o encontrada. Usando cliente global (pode falhar com RLS).")
            return supabase_client
        
        # Criar cliente com ANON_KEY
        # O token ser√° passado nos headers das requisi√ß√µes
        user_client = create_client(SUPABASE_URL, anon_key)
        
        # Armazenar o token para uso nas requisi√ß√µes
        # Nota: O cliente Supabase Python n√£o suporta set_session diretamente,
        # mas podemos passar o token nos headers manualmente se necess√°rio
        return user_client
    except Exception as e:
        print(f"[WARN] Erro ao criar cliente Supabase com token do usu√°rio: {e}")
        return supabase_client

# ============ MIDDLEWARE PARA LOG E CORS ============
@app.before_request
def log_request_info():
    """Log das requisi√ß√µes para debug"""
    # Silent request logging
    pass

@app.after_request
def after_request(response):
    """Adiciona headers CORS em todas as respostas"""
    origin = request.headers.get('Origin')
    allowed_origins = [
        'http://localhost:4173',
        'http://localhost:5173',
        'http://localhost:5174',
        'http://localhost:3000',
        'https://devhubtrader.com.br',
        'https://www.devhubtrader.com.br',
        'http://devhubtrader.com.br',
        'http://www.devhubtrader.com.br'
    ]
    
    # Sempre adiciona headers CORS se a origem for permitida
    if origin and origin in allowed_origins:
        response.headers['Access-Control-Allow-Origin'] = origin
        response.headers['Access-Control-Allow-Credentials'] = 'true'
    # Sempre adiciona m√©todos e headers permitidos
    response.headers['Access-Control-Allow-Methods'] = 'GET, POST, PUT, DELETE, OPTIONS, PATCH'
    response.headers['Access-Control-Allow-Headers'] = 'Content-Type, Authorization, x-openai-key, X-Requested-With'
    response.headers['Access-Control-Expose-Headers'] = 'Content-Type, Authorization'
    response.headers['Access-Control-Max-Age'] = '3600'
    
    return response

@app.before_request
def handle_preflight():
    """Trata requisi√ß√µes OPTIONS (preflight)"""
    if request.method == "OPTIONS":
        response = jsonify({})
        origin = request.headers.get('Origin')
        allowed_origins = [
            'http://localhost:4173',
            'http://localhost:5173',
            'http://localhost:5174',
            'http://localhost:3000',
            'https://devhubtrader.com.br',
            'https://www.devhubtrader.com.br',
            'http://devhubtrader.com.br',
            'http://www.devhubtrader.com.br'
        ]
        if origin and origin in allowed_origins:
            response.headers['Access-Control-Allow-Origin'] = origin
            response.headers['Access-Control-Allow-Credentials'] = 'true'
        response.headers['Access-Control-Allow-Methods'] = 'GET, POST, PUT, DELETE, OPTIONS, PATCH'
        response.headers['Access-Control-Allow-Headers'] = 'Content-Type, Authorization, x-openai-key, X-Requested-With'
        response.headers['Access-Control-Max-Age'] = '3600'
        return response

# ============ ROTA RAIZ ============
@app.route('/', methods=['GET'])
def root():
    """Rota raiz para verificar se o servidor est√° funcionando"""
    return jsonify({
        "status": "online",
        "message": "DevHub Trader Backend API",
        "version": "1.0.0",
        "endpoints": [
            "/api/tabela",
            "/api/tabela-multipla", 
            "/api/equity-curve",
            "/api/backtest-completo",
            "/api/correlacao",
            "/api/disciplina-completa",
            "/api/trades",
            "/api/trades/summary",
            "/api/trades/daily-metrics",
            "/api/trades/metrics-from-data",
            "/chat"
        ]
    })

@app.route('/health', methods=['GET'])
def health():
    """Rota de health check para monitoramento"""
    return jsonify({
        "status": "healthy",
        "timestamp": datetime.now().isoformat(),
        "service": "devhub-backend",
        "openai_key_detected": bool(os.getenv("OPENAI_API_KEY"))
    })

@app.route('/api/test-metrics', methods=['POST'])
def test_metrics():
    """Endpoint de teste para verificar se a API de m√©tricas est√° funcionando"""
    try:
        # Dados de teste simples
        test_data = {
            'trades': [
                {
                    'entry_date': '2024-01-01T10:00:00',
                    'exit_date': '2024-01-01T10:30:00',
                    'pnl': 100
                },
                {
                    'entry_date': '2024-01-01T11:00:00',
                    'exit_date': '2024-01-01T11:15:00',
                    'pnl': -50
                }
            ],
            'capital_inicial': 100000,
            'cdi': 0.12
        }
        
        # Simular o processamento
        df = pd.DataFrame(test_data['trades'])
        df['entry_date'] = pd.to_datetime(df['entry_date'])
        df['exit_date'] = pd.to_datetime(df['exit_date'])
        df['pnl'] = pd.to_numeric(df['pnl'], errors='coerce')
        
        # Testar import do FunCalculos
        try:
            from FunCalculos import processar_backtest_completo
            resultado = processar_backtest_completo(df, capital_inicial=100000, cdi=0.12)
            
            return jsonify({
                "status": "success",
                "message": "API de m√©tricas funcionando corretamente",
                "test_trades": len(df),
                "performance_metrics": resultado.get("Performance Metrics", {})
            })
        except Exception as e:
            return jsonify({
                "status": "error",
                "message": f"Erro no FunCalculos: {str(e)}"
            }), 500
            
    except Exception as e:
        return jsonify({
            "status": "error",
            "message": f"Erro no teste: {str(e)}"
        }), 500

# ============ FUN√á√ÉO AUXILIAR PARA ENCODING ============

def clean_numeric_value(value):
    """Converte valores num√©ricos brasileiros para float"""
    if pd.isna(value) or value == '':
        return np.nan
    
    # Converter para string se n√£o for
    str_value = str(value)
    
    # Remover espa√ßos em branco
    str_value = str_value.strip()
    
    # Se j√° for um n√∫mero, retornar
    if isinstance(value, (int, float)):
        return float(value)
    
    # Remover pontos (separador de milhares) e trocar v√≠rgula por ponto
    # Exemplo: "371.520,00" -> "371520.00"
    if ',' in str_value:
        # Separar parte inteira da decimal
        parts = str_value.split(',')
        if len(parts) == 2:
            integer_part = parts[0].replace('.', '')  # Remove pontos da parte inteira
            decimal_part = parts[1]
            cleaned_value = f"{integer_part}.{decimal_part}"
        else:
            cleaned_value = str_value.replace('.', '').replace(',', '.')
    else:
        # Se n√£o tem v√≠rgula, pode ser que tenha apenas pontos como separadores de milhares
        # ou seja um n√∫mero sem decimais
        if str_value.count('.') > 1:
            # M√∫ltiplos pontos = separadores de milhares
            cleaned_value = str_value.replace('.', '')
        else:
            cleaned_value = str_value
    
    try:
        return float(cleaned_value)
    except ValueError:
        return np.nan


def _parse_filters_from_request(req) -> Dict[str, Any]:
    """Extrai filtros b√°sicos do request (dire√ß√£o, datas, etc.)."""
    filters: Dict[str, Any] = {}

    # CORRE√á√ÉO: Verificar se os filtros v√™m via JSON no body
    if req.is_json:
        try:
            json_data = req.get_json(silent=True)
            if json_data and isinstance(json_data, dict):
                # Se h√° um campo 'filters' no JSON, usar ele
                if 'filters' in json_data:
                    filters.update(json_data['filters'])
                # Ou se os filtros est√£o diretamente no JSON
                else:
                    # Extrair filtros diretamente do JSON
                    for key in ('direction', 'direcao', 'directions', 'side', 'date_from', 'date_to', 
                               'data_inicio', 'data_fim', 'asset', 'symbol', 'ativo', 'simbolo', 
                               'strategy', 'estrategia'):
                        if key in json_data:
                            filters[key] = json_data[key]
        except Exception as e:
            print(f"‚ö†Ô∏è Erro ao processar filtros do JSON: {e}")

    # Extrair filtros do form (FormData)
    raw_filters = req.form.get('filters') or req.form.get('filtros')
    if raw_filters:
        try:
            parsed = json.loads(raw_filters)
            if isinstance(parsed, dict):
                filters.update(parsed)
        except json.JSONDecodeError:
            pass

    # Extrair dire√ß√£o do form
    for key in ('direction', 'direcao', 'directions', 'side'):
        value = req.form.get(key)
        if value:
            try:
                parsed = json.loads(value)
            except json.JSONDecodeError:
                parsed = value
            filters['direction'] = parsed
            break

    # Extrair filtros de data diretamente do request (form ou JSON)
    date_from = (filters.get('date_from') or 
                 req.form.get('date_from') or 
                 req.form.get('data_inicio') or 
                 req.form.get('date_start') or
                 (req.get_json(silent=True) or {}).get('date_from') or
                 (req.get_json(silent=True) or {}).get('data_inicio'))
    
    date_to = (filters.get('date_to') or 
               req.form.get('date_to') or 
               req.form.get('data_fim') or 
               req.form.get('date_end') or
               (req.get_json(silent=True) or {}).get('date_to') or
               (req.get_json(silent=True) or {}).get('data_fim'))
    
    if date_from:
        filters['date_from'] = date_from
    if date_to:
        filters['date_to'] = date_to

    # Extrair outros filtros diretamente do request (form ou JSON)
    for key in ('asset', 'symbol', 'ativo', 'simbolo', 'strategy', 'estrategia'):
        value = (filters.get(key) or 
                req.form.get(key) or
                (req.get_json(silent=True) or {}).get(key))
        if value:
            filters[key] = value

    # CORRE√á√ÉO: Extrair filtros de dia da semana (day_of_week, dia_semana, dayOfWeek)
    day_of_week_filter = (
        filters.get('day_of_week') or 
        filters.get('dia_semana') or 
        filters.get('dayOfWeek') or
        req.form.get('day_of_week') or 
        req.form.get('dia_semana') or
        req.form.get('dayOfWeek') or
        (req.get_json(silent=True) or {}).get('day_of_week') or
        (req.get_json(silent=True) or {}).get('dia_semana') or
        (req.get_json(silent=True) or {}).get('dayOfWeek')
    )
    if day_of_week_filter and day_of_week_filter not in ('Todos', 'All', 'todos', 'all', ''):
        filters['day_of_week'] = day_of_week_filter

    # CORRE√á√ÉO: Extrair filtros de m√™s (month, mes, month_filter)
    month_filter = (
        filters.get('month') or 
        filters.get('mes') or 
        filters.get('month_filter') or
        req.form.get('month') or 
        req.form.get('mes') or
        req.form.get('month_filter') or
        (req.get_json(silent=True) or {}).get('month') or
        (req.get_json(silent=True) or {}).get('mes') or
        (req.get_json(silent=True) or {}).get('month_filter')
    )
    if month_filter and month_filter not in ('Todos', 'All', 'todos', 'all', ''):
        filters['month'] = month_filter

    # CORRE√á√ÉO: Extrair filtros de hor√°rio (time_from, time_to, time_range, hora_inicio, hora_fim)
    # Tamb√©m suporta faixas pr√©-definidas (abertura, meio_dia, tarde, pos_mercado)
    time_range = (
        filters.get('time_range') or 
        filters.get('faixa_horario') or 
        filters.get('predefined_range') or
        req.form.get('time_range') or 
        req.form.get('faixa_horario') or
        req.form.get('predefined_range') or
        (req.get_json(silent=True) or {}).get('time_range') or
        (req.get_json(silent=True) or {}).get('faixa_horario') or
        (req.get_json(silent=True) or {}).get('predefined_range')
    )
    
    # Mapear faixas pr√©-definidas para hor√°rios
    predefined_ranges = {
        'abertura': ('09:00', '11:00'),
        'opening': ('09:00', '11:00'),
        'meio_dia': ('11:00', '14:00'),
        'mid_day': ('11:00', '14:00'),
        'meio-dia': ('11:00', '14:00'),
        'tarde': ('14:00', '17:30'),
        'afternoon': ('14:00', '17:30'),
        'pos_mercado': ('17:30', '21:00'),
        'after_market': ('17:30', '21:00'),
        'p√≥s-mercado': ('17:30', '21:00'),
        'pos-mercado': ('17:30', '21:00')
    }
    
    # Se tem faixa pr√©-definida, usar ela
    if time_range and time_range.lower() in predefined_ranges:
        time_from, time_to = predefined_ranges[time_range.lower()]
        filters['time_from'] = time_from
        filters['time_to'] = time_to
    else:
        # Caso contr√°rio, usar hor√°rios customizados
        time_from = (
            filters.get('time_from') or 
            filters.get('hora_inicio') or 
            filters.get('time_start') or
            req.form.get('time_from') or 
            req.form.get('hora_inicio') or
            req.form.get('time_start') or
            (req.get_json(silent=True) or {}).get('time_from') or
            (req.get_json(silent=True) or {}).get('hora_inicio') or
            (req.get_json(silent=True) or {}).get('time_start')
        )
        if time_from:
            filters['time_from'] = time_from

        time_to = (
            filters.get('time_to') or 
            filters.get('hora_fim') or 
            filters.get('time_end') or
            req.form.get('time_to') or 
            req.form.get('hora_fim') or
            req.form.get('time_end') or
            (req.get_json(silent=True) or {}).get('time_to') or
            (req.get_json(silent=True) or {}).get('hora_fim') or
            (req.get_json(silent=True) or {}).get('time_end')
        )
        if time_to:
            filters['time_to'] = time_to

    # CORRE√á√ÉO: Extrair data espec√≠fica (specific_date, data_especifica, specificDate)
    specific_date = (
        filters.get('specific_date') or 
        filters.get('data_especifica') or 
        filters.get('specificDate') or
        req.form.get('specific_date') or 
        req.form.get('data_especifica') or
        req.form.get('specificDate') or
        (req.get_json(silent=True) or {}).get('specific_date') or
        (req.get_json(silent=True) or {}).get('data_especifica') or
        (req.get_json(silent=True) or {}).get('specificDate')
    )
    if specific_date:
        filters['specific_date'] = specific_date

    # Log dos filtros extra√≠dos para debug
    if filters:
        print(f"üîç Filtros extra√≠dos do request: {filters}")

    return filters


_DIRECTION_MAP = {
    'long': {'long'},
    'short': {'short'},
    'buy': {'long'},
    'sell': {'short'},
    'compra': {'long'},
    'venda': {'short'},
    'comprado': {'long'},
    'vendido': {'short'},
    'c': {'long'},
    'v': {'short'},
    'compra+venda': {'long', 'short'},
    'compra + venda': {'long', 'short'},
    'all': set(),
    'todos': set(),
    'ambos': {'long', 'short'},
    'ambas': {'long', 'short'}
}


def aplicar_filtros_basicos(df: pd.DataFrame, filtros: Dict[str, Any]) -> pd.DataFrame:
    """Aplica filtros padr√£o (dire√ß√£o, ativo, estrat√©gia, datas, etc.) ao DataFrame."""
    # CORRE√á√ÉO: Validar se filtros n√£o est√° vazio e tem valores v√°lidos
    if df.empty:
        print(f"üîç aplicar_filtros_basicos: DataFrame vazio. Shape: {df.shape}")
        return df
    
    # Filtrar filtros vazios ou None
    filtros_validos = {k: v for k, v in filtros.items() if v is not None and v != '' and v != []}
    
    if not filtros_validos:
        print(f"üîç aplicar_filtros_basicos: Nenhum filtro v√°lido encontrado. Filtros recebidos: {filtros}")
        return df

    df_filtrado = df.copy()
    filtros_aplicados = []
    
    print(f"üîç aplicar_filtros_basicos: Aplicando filtros. Shape antes: {df_filtrado.shape}, Filtros v√°lidos: {filtros_validos}")

    # FILTRO 1: Dire√ß√£o (direction, direcao, side)
    direction_filter = (
        filtros_validos.get('direction')
        or filtros_validos.get('directions')
        or filtros_validos.get('side')
        or filtros_validos.get('direcao')
    )

    if direction_filter:
        print(f"   üîç Filtrando por dire√ß√£o: {direction_filter}")
        direction_col = None
        
        # Procurar coluna de dire√ß√£o em v√°rias varia√ß√µes
        for candidate in ('direction', 'Lado', 'lado', 'Dire√ß√£o', 'direcao', 'Side'):
            if candidate in df_filtrado.columns:
                direction_col = candidate
                break
        
        # Se n√£o encontrou, tentar criar a partir de colunas normalizadas
        if direction_col is None:
            # Verificar se tem coluna normalizada
            if 'direction' not in df_filtrado.columns:
                print(f"   ‚ö†Ô∏è Coluna de dire√ß√£o n√£o encontrada. Colunas dispon√≠veis: {list(df_filtrado.columns)[:10]}")
                # Tentar criar a partir de 'Lado' se existir
                if 'Lado' in df_filtrado.columns:
                    df_filtrado['direction'] = df_filtrado['Lado'].astype(str).str.strip().str.upper().map({
                        'C': 'long', 'COMPRA': 'long', 'COMPRADO': 'long',
                        'V': 'short', 'VENDA': 'short', 'VENDIDO': 'short'
                    }).fillna('long')
                    direction_col = 'direction'
                else:
                    print(f"   ‚ö†Ô∏è N√£o foi poss√≠vel aplicar filtro de dire√ß√£o. Continuando sem filtrar por dire√ß√£o.")
            else:
                direction_col = 'direction'

        if direction_col:
            direction_series = df_filtrado[direction_col].astype(str).str.strip()
            mapped_direction = direction_series.str.upper().map({
                'C': 'long', 'COMPRA': 'long', 'COMPRADO': 'long', 'LONG': 'long', 'BUY': 'long',
                'V': 'short', 'VENDA': 'short', 'VENDIDO': 'short', 'SHORT': 'short', 'SELL': 'short'
            })
            
            # Se algum valor n√£o foi mapeado, tentar lowercase
            if mapped_direction.isna().any():
                mapped_direction = mapped_direction.fillna(
                    direction_series.str.lower().map({
                        'c': 'long', 'compra': 'long', 'comprado': 'long', 'long': 'long', 'buy': 'long',
                        'v': 'short', 'venda': 'short', 'vendido': 'short', 'short': 'short', 'sell': 'short'
                    })
                )
            
            df_filtrado['_direction_tmp_'] = mapped_direction.fillna(direction_series.str.lower())
            direction_column_to_use = '_direction_tmp_'

            if isinstance(direction_filter, (list, tuple, set)):
                requested = [str(x).strip().lower() for x in direction_filter if x is not None]
            else:
                requested = [str(direction_filter).strip().lower()] if direction_filter else []

            allowed = set()
            for item in requested:
                if not item:
                    continue
                normalized = item.lower()
                if normalized in _DIRECTION_MAP:
                    mapped = _DIRECTION_MAP[normalized]
                    if not mapped:  # represents 'all'
                        allowed = set()
                        break
                    allowed.update(mapped)
                else:
                    # tentar correspond√™ncia parcial
                    if 'compra' in normalized or 'buy' in normalized or 'long' in normalized:
                        allowed.update(_DIRECTION_MAP.get('compra', {'long'}))
                    elif 'venda' in normalized or 'sell' in normalized or 'short' in normalized:
                        allowed.update(_DIRECTION_MAP.get('venda', {'short'}))

            if allowed:
                antes = len(df_filtrado)
                df_filtrado = df_filtrado[df_filtrado[direction_column_to_use].isin(allowed)]
                depois = len(df_filtrado)
                print(f"   ‚úÖ Filtro de dire√ß√£o aplicado: {antes} -> {depois} registros (filtro: {allowed})")
                filtros_aplicados.append(f"dire√ß√£o: {allowed}")

            df_filtrado = df_filtrado.drop(columns=['_direction_tmp_'], errors='ignore')

    # FILTRO 2: Ativo/S√≠mbolo (asset, symbol, ativo, simbolo)
    asset_filter = (
        filtros_validos.get('asset')
        or filtros_validos.get('symbol')
        or filtros_validos.get('ativo')
        or filtros_validos.get('simbolo')
    )
    
    if asset_filter and not df_filtrado.empty:
        print(f"   üîç Filtrando por ativo: {asset_filter}")
        asset_col = None
        for candidate in ('symbol', 'Ativo', 'ativo', 'asset', 'Asset', 'SYMBOL'):
            if candidate in df_filtrado.columns:
                asset_col = candidate
                break
        
        if asset_col:
            if isinstance(asset_filter, (list, tuple, set)):
                allowed_assets = [str(x).strip() for x in asset_filter if x]
            else:
                allowed_assets = [str(asset_filter).strip()] if asset_filter else []
            
            if allowed_assets:
                antes = len(df_filtrado)
                # Busca case-insensitive
                mask = df_filtrado[asset_col].astype(str).str.strip().str.upper().isin(
                    [a.upper() for a in allowed_assets]
                )
                df_filtrado = df_filtrado[mask]
                depois = len(df_filtrado)
                print(f"   ‚úÖ Filtro de ativo aplicado: {antes} -> {depois} registros (ativos: {allowed_assets})")
                filtros_aplicados.append(f"ativo: {allowed_assets}")
        else:
            print(f"   ‚ö†Ô∏è Coluna de ativo n√£o encontrada. Colunas dispon√≠veis: {list(df_filtrado.columns)[:10]}")

    # FILTRO 3: Estrat√©gia (strategy, estrategia, Estrat√©gia)
    strategy_filter = (
        filtros_validos.get('strategy')
        or filtros_validos.get('estrategia')
        or filtros_validos.get('Estrat√©gia')
    )
    
    if strategy_filter and not df_filtrado.empty:
        print(f"   üîç Filtrando por estrat√©gia: {strategy_filter}")
        strategy_col = None
        for candidate in ('strategy', 'Estrat√©gia', 'estrategia', 'Strategy', 'STRATEGY'):
            if candidate in df_filtrado.columns:
                strategy_col = candidate
                break
        
        if strategy_col:
            if isinstance(strategy_filter, (list, tuple, set)):
                allowed_strategies = [str(x).strip() for x in strategy_filter if x]
            else:
                allowed_strategies = [str(strategy_filter).strip()] if strategy_filter else []
            
            if allowed_strategies:
                antes = len(df_filtrado)
                # Busca case-insensitive
                mask = df_filtrado[strategy_col].astype(str).str.strip().isin(allowed_strategies)
                df_filtrado = df_filtrado[mask]
                depois = len(df_filtrado)
                print(f"   ‚úÖ Filtro de estrat√©gia aplicado: {antes} -> {depois} registros (estrat√©gias: {allowed_strategies})")
                filtros_aplicados.append(f"estrat√©gia: {allowed_strategies}")
        else:
            print(f"   ‚ö†Ô∏è Coluna de estrat√©gia n√£o encontrada. Colunas dispon√≠veis: {list(df_filtrado.columns)[:10]}")

    # FILTRO 4: Per√≠odo de datas (date_from, date_to, data_inicio, data_fim)
    if 'entry_date' in df_filtrado.columns and not df_filtrado.empty:
        date_from = filtros_validos.get('date_from') or filtros_validos.get('data_inicio') or filtros_validos.get('date_start')
        date_to = filtros_validos.get('date_to') or filtros_validos.get('data_fim') or filtros_validos.get('date_end')
        
        if date_from or date_to:
            print(f"   üîç Filtrando por per√≠odo: {date_from} at√© {date_to}")
            try:
                df_filtrado['entry_date'] = pd.to_datetime(df_filtrado['entry_date'], errors='coerce')
                antes = len(df_filtrado)
                
                if date_from:
                    date_from_dt = pd.to_datetime(date_from, errors='coerce')
                    if pd.notna(date_from_dt):
                        # CORRE√á√ÉO: Garantir que entry_date est√° como datetime antes de comparar
                        if not pd.api.types.is_datetime64_any_dtype(df_filtrado['entry_date']):
                            df_filtrado['entry_date'] = pd.to_datetime(df_filtrado['entry_date'], errors='coerce')
                        antes_filtro = len(df_filtrado)
                        df_filtrado = df_filtrado[df_filtrado['entry_date'] >= date_from_dt]
                        depois_filtro = len(df_filtrado)
                        print(f"      üìÖ Filtro date_from aplicado: {antes_filtro} -> {depois_filtro} registros")
                
                if date_to:
                    date_to_dt = pd.to_datetime(date_to, errors='coerce')
                    if pd.notna(date_to_dt):
                        # CORRE√á√ÉO: Garantir que entry_date est√° como datetime antes de comparar
                        if not pd.api.types.is_datetime64_any_dtype(df_filtrado['entry_date']):
                            df_filtrado['entry_date'] = pd.to_datetime(df_filtrado['entry_date'], errors='coerce')
                        # Para date_to, incluir o dia inteiro (at√© 23:59:59)
                        date_to_dt = date_to_dt + pd.Timedelta(days=1) - pd.Timedelta(seconds=1)
                        antes_filtro = len(df_filtrado)
                        df_filtrado = df_filtrado[df_filtrado['entry_date'] <= date_to_dt]
                        depois_filtro = len(df_filtrado)
                        print(f"      üìÖ Filtro date_to aplicado: {antes_filtro} -> {depois_filtro} registros")
                
                depois = len(df_filtrado)
                print(f"   ‚úÖ Filtro de data aplicado: {antes} -> {depois} registros")
                filtros_aplicados.append(f"per√≠odo: {date_from} at√© {date_to}")
            except Exception as e:
                print(f"   ‚ö†Ô∏è Erro ao aplicar filtro de data: {e}")

    # FILTRO 5: Data espec√≠fica (specific_date, data_especifica)
    if 'entry_date' in df_filtrado.columns and not df_filtrado.empty:
        specific_date = filtros_validos.get('specific_date') or filtros_validos.get('data_especifica')
        
        if specific_date:
            print(f"   üîç Filtrando por data espec√≠fica: {specific_date}")
            try:
                df_filtrado['entry_date'] = pd.to_datetime(df_filtrado['entry_date'], errors='coerce')
                specific_date_dt = pd.to_datetime(specific_date, errors='coerce')
                
                if pd.notna(specific_date_dt):
                    # Filtrar apenas o dia espec√≠fico (de 00:00:00 at√© 23:59:59)
                    date_start = specific_date_dt.replace(hour=0, minute=0, second=0, microsecond=0)
                    date_end = specific_date_dt.replace(hour=23, minute=59, second=59, microsecond=999999)
                    
                    antes = len(df_filtrado)
                    mask = (df_filtrado['entry_date'] >= date_start) & (df_filtrado['entry_date'] <= date_end)
                    df_filtrado = df_filtrado[mask]
                    depois = len(df_filtrado)
                    print(f"   ‚úÖ Filtro de data espec√≠fica aplicado: {antes} -> {depois} registros")
                    filtros_aplicados.append(f"data espec√≠fica: {specific_date}")
            except Exception as e:
                print(f"   ‚ö†Ô∏è Erro ao aplicar filtro de data espec√≠fica: {e}")

    # FILTRO 6: Dia da semana (day_of_week, dia_semana)
    if 'entry_date' in df_filtrado.columns and not df_filtrado.empty:
        day_of_week_filter = filtros_validos.get('day_of_week') or filtros_validos.get('dia_semana')
        
        if day_of_week_filter and day_of_week_filter not in ('Todos', 'All', 'todos', 'all', ''):
            print(f"   üîç Filtrando por dia da semana: {day_of_week_filter}")
            try:
                df_filtrado['entry_date'] = pd.to_datetime(df_filtrado['entry_date'], errors='coerce')
                
                # Mapear nomes de dias (portugu√™s e ingl√™s)
                day_map = {
                    'Monday': 0, 'Segunda': 0, 'Segunda-feira': 0, 'segunda': 0,
                    'Tuesday': 1, 'Ter√ßa': 1, 'Ter√ßa-feira': 1, 'ter√ßa': 1,
                    'Wednesday': 2, 'Quarta': 2, 'Quarta-feira': 2, 'quarta': 2,
                    'Thursday': 3, 'Quinta': 3, 'Quinta-feira': 3, 'quinta': 3,
                    'Friday': 4, 'Sexta': 4, 'Sexta-feira': 4, 'sexta': 4,
                    'Saturday': 5, 'S√°bado': 5, 's√°bado': 5,
                    'Sunday': 6, 'Domingo': 6, 'domingo': 6
                }
                
                target_day = day_map.get(str(day_of_week_filter).strip(), None)
                
                if target_day is not None:
                    antes = len(df_filtrado)
                    df_filtrado['_day_of_week'] = df_filtrado['entry_date'].dt.dayofweek
                    df_filtrado = df_filtrado[df_filtrado['_day_of_week'] == target_day]
                    df_filtrado = df_filtrado.drop(columns=['_day_of_week'], errors='ignore')
                    depois = len(df_filtrado)
                    print(f"   ‚úÖ Filtro de dia da semana aplicado: {antes} -> {depois} registros (dia: {day_of_week_filter})")
                    filtros_aplicados.append(f"dia da semana: {day_of_week_filter}")
                else:
                    print(f"   ‚ö†Ô∏è Dia da semana n√£o reconhecido: {day_of_week_filter}")
            except Exception as e:
                print(f"   ‚ö†Ô∏è Erro ao aplicar filtro de dia da semana: {e}")

    # FILTRO 7: M√™s (month, mes)
    if 'entry_date' in df_filtrado.columns and not df_filtrado.empty:
        month_filter = filtros_validos.get('month') or filtros_validos.get('mes')
        
        if month_filter and month_filter not in ('Todos', 'All', 'todos', 'all', ''):
            print(f"   üîç Filtrando por m√™s: {month_filter}")
            try:
                df_filtrado['entry_date'] = pd.to_datetime(df_filtrado['entry_date'], errors='coerce')
                
                # Mapear nomes de meses (portugu√™s e ingl√™s) para n√∫meros
                month_map = {
                    'January': 1, 'Janeiro': 1, 'jan': 1, 'january': 1,
                    'February': 2, 'Fevereiro': 2, 'fev': 2, 'february': 2,
                    'March': 3, 'Mar√ßo': 3, 'mar': 3, 'march': 3,
                    'April': 4, 'Abril': 4, 'abr': 4, 'april': 4,
                    'May': 5, 'Maio': 5, 'mai': 5, 'may': 5,
                    'June': 6, 'Junho': 6, 'jun': 6, 'june': 6,
                    'July': 7, 'Julho': 7, 'jul': 7, 'july': 7,
                    'August': 8, 'Agosto': 8, 'ago': 8, 'august': 8,
                    'September': 9, 'Setembro': 9, 'set': 9, 'september': 9,
                    'October': 10, 'Outubro': 10, 'out': 10, 'october': 10,
                    'November': 11, 'Novembro': 11, 'nov': 11, 'november': 11,
                    'December': 12, 'Dezembro': 12, 'dez': 12, 'december': 12
                }
                
                # Tentar converter para n√∫mero (1-12)
                target_month = None
                if isinstance(month_filter, (int, float)):
                    target_month = int(month_filter)
                elif str(month_filter).isdigit():
                    target_month = int(month_filter)
                else:
                    target_month = month_map.get(str(month_filter).strip(), None)
                
                if target_month is not None and 1 <= target_month <= 12:
                    antes = len(df_filtrado)
                    df_filtrado['_month'] = df_filtrado['entry_date'].dt.month
                    df_filtrado = df_filtrado[df_filtrado['_month'] == target_month]
                    df_filtrado = df_filtrado.drop(columns=['_month'], errors='ignore')
                    depois = len(df_filtrado)
                    print(f"   ‚úÖ Filtro de m√™s aplicado: {antes} -> {depois} registros (m√™s: {target_month})")
                    filtros_aplicados.append(f"m√™s: {target_month}")
                else:
                    print(f"   ‚ö†Ô∏è M√™s n√£o reconhecido: {month_filter}")
            except Exception as e:
                print(f"   ‚ö†Ô∏è Erro ao aplicar filtro de m√™s: {e}")

    # FILTRO 8: Faixa de hor√°rio (time_from, time_to, hora_inicio, hora_fim)
    if 'entry_date' in df_filtrado.columns and not df_filtrado.empty:
        time_from = filtros_validos.get('time_from') or filtros_validos.get('hora_inicio')
        time_to = filtros_validos.get('time_to') or filtros_validos.get('hora_fim')
        
        if time_from or time_to:
            print(f"   üîç Filtrando por faixa de hor√°rio: {time_from} at√© {time_to}")
            try:
                df_filtrado['entry_date'] = pd.to_datetime(df_filtrado['entry_date'], errors='coerce')
                
                # Extrair hora e minuto do hor√°rio fornecido
                def parse_time(time_str):
                    """Converte string de hor√°rio (HH:MM ou HH:MM:SS) para hora e minuto"""
                    if not time_str or time_str in ('--:--', ''):
                        return None
                    try:
                        # Tentar formatos HH:MM ou HH:MM:SS
                        parts = str(time_str).strip().split(':')
                        if len(parts) >= 2:
                            hour = int(parts[0])
                            minute = int(parts[1])
                            if 0 <= hour <= 23 and 0 <= minute <= 59:
                                return (hour, minute)
                    except:
                        pass
                    return None
                
                if time_from:
                    time_from_parsed = parse_time(time_from)
                    if time_from_parsed:
                        hour_from, minute_from = time_from_parsed
                        antes = len(df_filtrado)
                        # Criar m√°scara para hor√°rio >= time_from
                        mask = (df_filtrado['entry_date'].dt.hour > hour_from) | \
                               ((df_filtrado['entry_date'].dt.hour == hour_from) & 
                                (df_filtrado['entry_date'].dt.minute >= minute_from))
                        df_filtrado = df_filtrado[mask]
                        depois = len(df_filtrado)
                        print(f"      üïê Filtro time_from aplicado: {antes} -> {depois} registros (>= {hour_from:02d}:{minute_from:02d})")
                
                if time_to:
                    time_to_parsed = parse_time(time_to)
                    if time_to_parsed:
                        hour_to, minute_to = time_to_parsed
                        antes = len(df_filtrado)
                        # Criar m√°scara para hor√°rio <= time_to
                        mask = (df_filtrado['entry_date'].dt.hour < hour_to) | \
                               ((df_filtrado['entry_date'].dt.hour == hour_to) & 
                                (df_filtrado['entry_date'].dt.minute <= minute_to))
                        df_filtrado = df_filtrado[mask]
                        depois = len(df_filtrado)
                        print(f"      üïê Filtro time_to aplicado: {antes} -> {depois} registros (<= {hour_to:02d}:{minute_to:02d})")
                
                if time_from or time_to:
                    filtros_aplicados.append(f"hor√°rio: {time_from} at√© {time_to}")
            except Exception as e:
                print(f"   ‚ö†Ô∏è Erro ao aplicar filtro de hor√°rio: {e}")

    print(f"‚úÖ aplicar_filtros_basicos: Filtros aplicados: {filtros_aplicados if filtros_aplicados else 'nenhum'}")
    print(f"   Shape final: {df_filtrado.shape} (antes: {df.shape})")
    
    return df_filtrado

def carregar_csv_trades(file_path_or_file):
    """
    Carrega CSV/Excel da planilha de trades com mapeamento espec√≠fico e parsing melhorado
    CORRE√á√ÉO: Agora usa a mesma l√≥gica de carregar_csv para suportar todos os formatos (CSV, XLS, XLSX)
    """
    try:
        # CORRE√á√ÉO: Usar a fun√ß√£o unificada carregar_csv que suporta todos os formatos
        from FunCalculos import carregar_csv
        df = carregar_csv(file_path_or_file)
        
        # A fun√ß√£o carregar_csv j√° normaliza o DataFrame, ent√£o entry_date e pnl j√° devem existir
        # Mas podemos fazer mapeamentos adicionais se necess√°rio
        
        # Converter dire√ß√£o para formato padr√£o se ainda n√£o foi feito
        if 'direction' in df.columns:
            # Verificar se j√° est√° no formato correto
            sample = df['direction'].dropna().head(5)
            if len(sample) > 0:
                # Se tem valores como 'C' ou 'V', converter
                if any(val in ['C', 'V', 'c', 'v'] for val in sample.astype(str)):
                    df['direction'] = df['direction'].astype(str).str.upper().map({
                        'C': 'long', 'COMPRA': 'long', 'COMPRADO': 'long',
                        'V': 'short', 'VENDA': 'short', 'VENDIDO': 'short'
                    }).fillna(df['direction'])
        
        # Calcular dura√ß√£o em horas se n√£o existir
        if 'entry_date' in df.columns and 'exit_date' in df.columns:
            if df['entry_date'].notna().any() and df['exit_date'].notna().any():
                valid_mask = df['entry_date'].notna() & df['exit_date'].notna()
                if valid_mask.any():
                    df.loc[valid_mask, 'duration_hours'] = (
                        df.loc[valid_mask, 'exit_date'] - df.loc[valid_mask, 'entry_date']
                    ).dt.total_seconds() / 3600
        
        return df
        
    except Exception as e:
        raise ValueError(f"Erro ao processar arquivo de trades: {e}")

# Fun√ß√£o carregar_csv_safe melhorada com encoding robusto
def carregar_csv_safe(file_path_or_file):
    """
    CORRIGIDO: Fun√ß√£o auxiliar para carregar CSV/Excel com encoding seguro e suporte a m√∫ltiplos tipos de arquivo.
    Agora suporta CSV, Excel (.xlsx, .xls, .xlsm) e JSON, al√©m de validar campos obrigat√≥rios.
    CORRE√á√ÉO: Usa a mesma l√≥gica unificada de carregar_csv para garantir padroniza√ß√£o.
    """
    try:
        # CORRE√á√ÉO: Usar a fun√ß√£o carregar_csv do FunCalculos que foi melhorada e suporta todos os formatos
        from FunCalculos import carregar_csv
        
        # Resetar posi√ß√£o do arquivo se for um objeto file
        if hasattr(file_path_or_file, 'seek'):
            file_path_or_file.seek(0)
        
        # A fun√ß√£o carregar_csv j√° faz toda a normaliza√ß√£o e valida√ß√£o
        df = carregar_csv(file_path_or_file)
        
        # CORRE√á√ÉO: Validar campos obrigat√≥rios ap√≥s carregar (j√° deve estar normalizado)
        if df.empty:
            raise ValueError("O arquivo est√° vazio ou n√£o cont√©m dados v√°lidos.")
        
        # Validar que h√° pelo menos uma coluna de data e uma de PnL (j√° deve existir ap√≥s normaliza√ß√£o)
        has_date_col = 'entry_date' in df.columns
        has_pnl_col = 'pnl' in df.columns
        
        if not has_date_col:
            raise ValueError("O arquivo n√£o cont√©m coluna de data (entry_date). A normaliza√ß√£o deveria ter criado esta coluna.")
        
        if not has_pnl_col:
            raise ValueError("O arquivo n√£o cont√©m coluna de resultado (pnl). A normaliza√ß√£o deveria ter criado esta coluna.")
        
        # Validar que h√° valores v√°lidos
        entry_date_valid = df['entry_date'].notna().sum() if has_date_col else 0
        pnl_valid = df['pnl'].notna().sum() if has_pnl_col else 0
        
        if entry_date_valid == 0:
            raise ValueError("O arquivo n√£o cont√©m datas v√°lidas na coluna 'entry_date'.")
        
        if pnl_valid == 0:
            raise ValueError("O arquivo n√£o cont√©m valores v√°lidos na coluna 'pnl'.")
        
        return df
    except Exception as primary_error:
        print(f"üîç DEBUG: Fallback para leitura manual do CSV ({primary_error})")
        try:
            encodings_to_try = ['utf-8', 'latin1', 'cp1252', 'iso-8859-1']
            formats_to_try = [
                {'skiprows': 0, 'sep': ',', 'encoding': None},
                {'skiprows': 5, 'sep': ';', 'encoding': None, 'decimal': ','},
                {'skiprows': 0, 'sep': ',', 'encoding': None},
                {'skiprows': 5, 'sep': ';', 'encoding': None, 'decimal': ','}
            ]

            df = None
            last_error = primary_error

            for encoding in encodings_to_try:
                for format_config in formats_to_try:
                    try:
                        if hasattr(file_path_or_file, 'read'):
                            file_path_or_file.seek(0)
                            format_config['encoding'] = encoding
                            df = pd.read_csv(file_path_or_file, **format_config)
                        else:
                            format_config['encoding'] = encoding
                            df = pd.read_csv(file_path_or_file, **format_config)

                        expected_columns = ['entry_date', 'exit_date', 'pnl', 'Abertura', 'Fechamento', 'Res. Opera√ß√£o', 'Res. Intervalo']
                        found_columns = [col for col in expected_columns if col in df.columns]
                        if found_columns:
                            break
                    except Exception as e:
                        last_error = e
                        continue
                if df is not None and len(df.columns) > 0:
                    break

            if df is None or len(df.columns) == 0:
                raise ValueError(f"N√£o foi poss√≠vel ler o CSV com nenhum encoding/formato. √öltimo erro: {last_error}")
        except Exception as fallback_error:
            print(f"‚ùå DEBUG: Fallback falhou: {fallback_error}")
            raise ValueError(f"Erro ao processar CSV: {primary_error}. Fallback tamb√©m falhou: {fallback_error}")

    # Processar datas conforme fun√ß√£o original - com verifica√ß√£o de colunas
    if 'Abertura' in df.columns:
        df['Abertura']   = pd.to_datetime(df['Abertura'],   format="%d/%m/%Y %H:%M:%S", errors='coerce')
    if 'Fechamento' in df.columns:
        df['Fechamento'] = pd.to_datetime(df['Fechamento'], format="%d/%m/%Y %H:%M:%S", errors='coerce')

    # Usar fun√ß√£o de limpeza para valores num√©ricos
    numeric_columns = ['Res. Opera√ß√£o', 'Res. Opera√ß√£o (%)', 'Pre√ßo Compra', 'Pre√ßo Venda',
                      'Pre√ßo de Mercado', 'M√©dio', 'Res. Intervalo', 'Res. Intervalo (%)',
                      'Res. Intervalo Bruto', 'Res. Intervalo Bruto (%)',
                      'Drawdown', 'Ganho Max.', 'Perda Max.', 'Qtd Compra', 'Qtd Venda', 'Total']

    for col in numeric_columns:
        if col in df.columns:
            df[col] = df[col].apply(clean_numeric_value)

    # Renomear colunas para padronizar
    column_mapping = {
        'Ativo': 'symbol',
        'Abertura': 'entry_date',
        'Fechamento': 'exit_date',
        'Tempo Opera√ß√£o': 'duration_str',
        'Qtd Compra': 'qty_buy',
        'Qtd Venda': 'qty_sell',
        'Lado': 'direction',
        'Pre√ßo Compra': 'entry_price',
        'Pre√ßo Venda': 'exit_price',
        'Pre√ßo de Mercado': 'market_price',
        'M√©dio': 'avg_price',
        'Res. Intervalo': 'pnl',
        'Res. Intervalo (%)': 'pnl_pct',
        'Res. Intervalo Bruto': 'pnl',
        'Res. Intervalo Bruto (%)': 'pnl_pct',
        'N√∫mero Opera√ß√£o': 'trade_number',
        'Res. Opera√ß√£o': 'operation_result',
        'Res. Opera√ß√£o (%)': 'operation_result_pct',
        'Drawdown': 'drawdown',
        'Ganho Max.': 'max_gain',
        'Perda Max.': 'max_loss',
        'TET': 'tet',
        'Total': 'total'
    }

    df = df.rename(columns=column_mapping)

    if 'direction' in df.columns:
        df['direction'] = df['direction'].map({'C': 'long', 'V': 'short'}).fillna('long')

    if 'pnl' not in df.columns and 'operation_result' in df.columns:
        df['pnl'] = df['operation_result']
    if 'pnl' in df.columns:
        df['pnl'] = pd.to_numeric(df['pnl'], errors='coerce')

    if 'entry_date' in df.columns and 'exit_date' in df.columns:
        try:
            if hasattr(df['entry_date'], 'dtype') and df['entry_date'].dtype == 'object':
                df['entry_date'] = pd.to_datetime(df['entry_date'], errors='coerce')
            if hasattr(df['exit_date'], 'dtype') and df['exit_date'].dtype == 'object':
                df['exit_date'] = pd.to_datetime(df['exit_date'], errors='coerce')

            valid_dates = df['entry_date'].notna() & df['exit_date'].notna()
            if valid_dates.any():
                try:
                    duration_series = (df.loc[valid_dates, 'exit_date'] - df.loc[valid_dates, 'entry_date'])
                    df.loc[valid_dates, 'duration_hours'] = duration_series.dt.total_seconds() / 3600
                except Exception as e:
                    print(f"üîç DEBUG: Erro ao calcular dura√ß√£o: {e}")
        except Exception as e:
            print(f"üîç DEBUG: Erro ao processar datas: {e}")
            try:
                df['entry_date'] = pd.to_datetime(df['entry_date'], errors='coerce')
                df['exit_date'] = pd.to_datetime(df['exit_date'], errors='coerce')
            except Exception:
                pass

    # CORRE√á√ÉO CR√çTICA: Sempre normalizar o DataFrame antes de retornar
    # Isso garante que entry_date e pnl sempre existam no formato correto
    from FunCalculos import _normalize_trades_dataframe
    print(f"üîÑ carregar_csv_safe: Normalizando DataFrame antes de retornar...")
    df_original_len = len(df)
    df = _normalize_trades_dataframe(df)
    
    if df.empty:
        print(f"‚ö†Ô∏è carregar_csv_safe: DataFrame ficou vazio ap√≥s normaliza√ß√£o (tinha {df_original_len} linhas)")
    else:
        entry_date_valid = df['entry_date'].notna().sum() if 'entry_date' in df.columns else 0
        pnl_valid = df['pnl'].notna().sum() if 'pnl' in df.columns else 0
        print(f"‚úÖ carregar_csv_safe: DataFrame normalizado - entry_date v√°lidos: {entry_date_valid}/{len(df)}, pnl v√°lidos: {pnl_valid}/{len(df)}")
    
    print(f"üîç DEBUG: DataFrame final, shape: {df.shape}")
    print(f"üîç DEBUG: Colunas finais: {df.columns.tolist()}")
    return df

def processar_trades(df: pd.DataFrame, arquivo_para_indices: Dict[int, str] = None) -> List[Dict]:
    """Converte DataFrame em lista de trades para o frontend
    - Inclui tamb√©m opera√ß√µes em aberto (sem exit_date), usando entry_date como fallback para exit_date
    - Mant√©m PnL informado no CSV
    """
    trades = []

    print(f"üîç Processando trades - DataFrame shape: {df.shape}")
    print(f"üìÖ Colunas dispon√≠veis: {list(df.columns)}")

    # CORRE√á√ÉO CR√çTICA: Normalizar o DataFrame SEMPRE, mesmo se entry_date j√° existe
    # Quando concatenamos DataFrames, um pode ter 'Abertura' e outro 'entry_date'
    # A normaliza√ß√£o garante que todos usem as mesmas colunas padronizadas
    from FunCalculos import _normalize_trades_dataframe
    
    # Verificar se precisa normalizar (se tem colunas n√£o normalizadas como 'Abertura' ou se entry_date est√° vazio)
    needs_normalization = (
        df.empty or 
        'entry_date' not in df.columns or 
        'pnl' not in df.columns or
        ('Abertura' in df.columns and ('entry_date' not in df.columns or df['entry_date'].isna().all()))
    )
    
    if needs_normalization:
        print(f"üîÑ Normalizando DataFrame em processar_trades...")
        df = _normalize_trades_dataframe(df)
        if df.empty:
            print("‚ö†Ô∏è DataFrame vazio ap√≥s normaliza√ß√£o")
            return trades
        print(f"‚úÖ DataFrame normalizado. entry_date v√°lidos: {df['entry_date'].notna().sum() if 'entry_date' in df.columns else 0}/{len(df)}")

    # Verificar se a coluna m√≠nima necess√°ria existe
    required_columns = ['entry_date']
    missing_columns = [col for col in required_columns if col not in df.columns]
    if missing_columns:
        print(f"‚ùå Colunas faltando: {missing_columns}. Colunas dispon√≠veis: {list(df.columns)}")
        return trades

    processed_count = 0
    skipped_count = 0

    for idx, row in df.iterrows():
        # Validar entry_date
        entry_date = row.get('entry_date')
        if pd.isna(entry_date):
            skipped_count += 1
            continue

        # exit_date pode ser ausente em opera√ß√µes abertas; usar entry_date como fallback
        raw_exit_date = row.get('exit_date')
        is_open = pd.isna(raw_exit_date)
        exit_date = raw_exit_date if pd.notna(raw_exit_date) else entry_date

        # Determinar a estrat√©gia baseada no arquivo de origem (se dispon√≠vel)
        strategy = "Manual"
        filename = None
        if arquivo_para_indices and idx in arquivo_para_indices:
            filename = arquivo_para_indices[idx]
        elif 'source_file' in df.columns:
            filename = row.get('source_file')

        if filename:
            filename_str = str(filename)
            strategy = Path(filename_str).stem

        qty_buy_raw = row.get('qty_buy', 0)
        qty_sell_raw = row.get('qty_sell', 0)

        qty_buy = int(qty_buy_raw) if pd.notna(qty_buy_raw) else 0
        qty_sell = int(qty_sell_raw) if pd.notna(qty_sell_raw) else 0
        # Somar buy/sell; se ambos 0, tentar usar outras colunas
        quantity_total = qty_buy + qty_sell
        if quantity_total == 0:
            for fallback in ('quantity', 'contracts', 'position', 'Position', 'Qtd', 'Qtd Total'):
                if fallback in row.index and pd.notna(row[fallback]):
                    try:
                        quantity_total = int(float(row[fallback]))
                        break
                    except (ValueError, TypeError):
                        continue
        if quantity_total == 0 and 'qty' in row.index and pd.notna(row['qty']):
            try:
                quantity_total = int(float(row['qty']))
            except (ValueError, TypeError):
                quantity_total = 0

        trade = {
            "entry_date": entry_date.isoformat() if pd.notna(entry_date) else None,
            "exit_date": exit_date.isoformat() if pd.notna(exit_date) else None,
            "entry_price": float(row.get('entry_price', 0)) if pd.notna(row.get('entry_price')) else 0,
            "exit_price": float(row.get('exit_price', 0)) if pd.notna(row.get('exit_price')) else 0,
            "pnl": float(row.get('pnl', 0)) if pd.notna(row.get('pnl')) else 0,
            "pnl_pct": float(row.get('pnl_pct', 0)) if pd.notna(row.get('pnl_pct')) else 0,
            "direction": row.get('direction', 'long'),
            "symbol": str(row.get('symbol', 'N/A')),
            "strategy": strategy,
            "source_file": filename,
            "quantity_total": quantity_total,
            "quantity_compra": qty_buy,
            "quantity_venda": qty_sell,
            "duration": float(row.get('duration_hours', 0)) if pd.notna(row.get('duration_hours')) else 0,
            "drawdown": float(row.get('drawdown', 0)) if pd.notna(row.get('drawdown')) else 0,
            "max_gain": float(row.get('max_gain', 0)) if pd.notna(row.get('max_gain')) else 0,
            "max_loss": float(row.get('max_loss', 0)) if pd.notna(row.get('max_loss')) else 0,
            "is_open": bool(is_open)
        }
        trades.append(trade)
        processed_count += 1

    print(f"‚úÖ Trades processados: {processed_count}, pulados: {skipped_count}")
    return trades

def calcular_estatisticas_temporais(df: pd.DataFrame) -> Dict[str, Any]:
    """Calcula estat√≠sticas temporais com serializa√ß√£o JSON correta"""
    if df.empty or 'entry_date' not in df.columns:
        return {}
    
    # CORRE√á√ÉO: Detectar coluna de PnL automaticamente
    from FunCalculos import _detect_pnl_column
    pnl_col = _detect_pnl_column(df)
    if pnl_col is None:
        return {}
    
    df_valid = df.dropna(subset=['entry_date', pnl_col])
    
    if df_valid.empty:
        return {}
    
    # Por dia da semana
    df_valid['day_of_week'] = df_valid['entry_date'].dt.day_name()
    day_stats = df_valid.groupby('day_of_week')[pnl_col].agg(['count', 'sum', 'mean']).round(2)
    
    # Por m√™s - converter Period para string
    df_valid['month'] = df_valid['entry_date'].dt.to_period('M').astype(str)
    month_stats = df_valid.groupby('month')[pnl_col].agg(['count', 'sum', 'mean']).round(2)
    
    # Por hora
    df_valid['hour'] = df_valid['entry_date'].dt.hour
    hour_stats = df_valid.groupby('hour')[pnl_col].agg(['count', 'sum', 'mean']).round(2)
    
    # Converter DataFrames para dicion√°rios JSON-serializ√°veis
    def convert_stats_to_dict(stats_df):
        result = {}
        for index, row in stats_df.iterrows():
            # Garantir que o √≠ndice seja string
            key = str(index)
            result[key] = {
                'count': int(row['count']) if pd.notna(row['count']) else 0,
                'sum': float(row['sum']) if pd.notna(row['sum']) else 0.0,
                'mean': float(row['mean']) if pd.notna(row['mean']) else 0.0
            }
        return result
    
    return {
        "day_of_week": convert_stats_to_dict(day_stats),
        "monthly": convert_stats_to_dict(month_stats),
        "hourly": convert_stats_to_dict(hour_stats)
    }

# Fun√ß√£o auxiliar para garantir que todos os valores sejam JSON-serializ√°veis
def make_json_serializable(obj):
    """Converte objetos pandas/numpy para tipos Python nativos"""
    if isinstance(obj, dict):
        return {str(k): make_json_serializable(v) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [make_json_serializable(item) for item in obj]
    elif isinstance(obj, (pd.Period, pd.Timestamp)):
        return str(obj)
    elif isinstance(obj, (np.integer, np.int64, np.int32, np.int16, np.int8)):
        return int(obj)
    elif isinstance(obj, (np.floating, np.float64, np.float32, np.float16)):
        # Tratar valores especiais
        if np.isnan(obj) or np.isinf(obj):
            return None  # Retornar None em vez de Infinity
        return float(obj)
    elif isinstance(obj, np.ndarray):
        return obj.tolist()
    elif isinstance(obj, np.bool_):
        return bool(obj)
    elif isinstance(obj, np.bytes_):
        return obj.decode('utf-8')
    elif pd.isna(obj) or obj is None:
        return None
    elif hasattr(obj, 'item'):  # Para outros tipos numpy que t√™m m√©todo item()
        item_value = obj.item()
        # Tratar valores infinitos tamb√©m aqui
        if isinstance(item_value, float) and (np.isnan(item_value) or np.isinf(item_value)):
            return None
        return item_value
    elif isinstance(obj, float):
        # Tratar valores especiais para floats Python tamb√©m
        if np.isnan(obj) or np.isinf(obj):
            return None
        return obj
    else:
        return obj

# Vers√£o atualizada das outras fun√ß√µes de estat√≠sticas para garantir serializa√ß√£o
def calcular_estatisticas_gerais(df: pd.DataFrame) -> Dict[str, Any]:
    """Calcula estat√≠sticas gerais das trades com serializa√ß√£o JSON correta"""
    if df.empty:
        return {}
    
    # Filtrar trades v√°lidas
    df_valid = df.dropna(subset=['pnl'])
    
    total_trades = len(df_valid)
    if total_trades == 0:
        return {}
    
    # Resultados b√°sicos
    total_pnl = df_valid['pnl'].sum()
    winning_trades = len(df_valid[df_valid['pnl'] > 0])
    losing_trades = len(df_valid[df_valid['pnl'] < 0])
    break_even_trades = len(df_valid[df_valid['pnl'] == 0])
    
    # Win rate
    win_rate = (winning_trades / total_trades) * 100 if total_trades > 0 else 0
    
    # M√©dias
    avg_win = df_valid[df_valid['pnl'] > 0]['pnl'].mean() if winning_trades > 0 else 0
    avg_loss = df_valid[df_valid['pnl'] < 0]['pnl'].mean() if losing_trades > 0 else 0
    avg_trade = df_valid['pnl'].mean()
    
    # M√°ximos e m√≠nimos
    best_trade = df_valid['pnl'].max()
    worst_trade = df_valid['pnl'].min()
    
    # Profit Factor
    gross_profit = df_valid[df_valid['pnl'] > 0]['pnl'].sum()
    gross_loss = abs(df_valid[df_valid['pnl'] < 0]['pnl'].sum())
    profit_factor = gross_profit / gross_loss if gross_loss != 0 else None
    
    # Expectativa
    expectancy = (win_rate/100 * avg_win) + ((100-win_rate)/100 * avg_loss)
    
    # Drawdown (se dispon√≠vel)
    max_drawdown = df_valid['drawdown'].min() if 'drawdown' in df_valid.columns else 0
    
    # Criar resultado e garantir serializa√ß√£o JSON
    resultado = {
        "total_trades": int(total_trades),
        "winning_trades": int(winning_trades),
        "losing_trades": int(losing_trades),
        "break_even_trades": int(break_even_trades),
        "win_rate": float(round(win_rate, 2)),
        "total_pnl": float(round(total_pnl, 2)),
        "avg_win": float(round(avg_win, 2)),
        "avg_loss": float(round(avg_loss, 2)),
        "avg_trade": float(round(avg_trade, 2)),
        "best_trade": float(round(best_trade, 2)),
        "worst_trade": float(round(worst_trade, 2)),
        "profit_factor": float(round(profit_factor, 2)) if profit_factor is not None else None,
        "expectancy": float(round(expectancy, 2)),
        "gross_profit": float(round(gross_profit, 2)),
        "gross_loss": float(round(gross_loss, 2)),
        "max_drawdown": float(round(max_drawdown, 2))
    }
    
    return make_json_serializable(resultado)

def calcular_estatisticas_por_ativo(df: pd.DataFrame) -> Dict[str, Any]:
    """Calcula estat√≠sticas agrupadas por ativo com serializa√ß√£o JSON correta"""
    if df.empty or 'symbol' not in df.columns:
        return {}
    
    stats_by_asset = {}
    
    for symbol in df['symbol'].unique():
        if pd.isna(symbol):
            continue
            
        asset_df = df[df['symbol'] == symbol].dropna(subset=['pnl'])
        
        if len(asset_df) == 0:
            continue
            
        stats_by_asset[str(symbol)] = {
            "total_trades": int(len(asset_df)),
            "total_pnl": float(round(asset_df['pnl'].sum(), 2)),
            "win_rate": float(round((len(asset_df[asset_df['pnl'] > 0]) / len(asset_df)) * 100, 2)),
            "avg_trade": float(round(asset_df['pnl'].mean(), 2)),
            "best_trade": float(round(asset_df['pnl'].max(), 2)),
            "worst_trade": float(round(asset_df['pnl'].min(), 2))
        }
    
    return make_json_serializable(stats_by_asset)

def _extrair_taxas_do_request(req) -> tuple:
    """
    Extrai taxas de corretagem e emolumentos do request
    CORRE√á√ÉO: Suporta formato separado, formato antigo e configura√ß√µes complexas
    Suporta:
    - Taxas simples (float)
    - Configura√ß√µes com m√©todo (fixed/percentage) e valor
    - Configura√ß√µes por ativo
    Retorna (taxa_corretagem, taxa_emolumentos) onde cada uma pode ser None ou um dict com m√©todo e valor
    """
    taxa_corretagem = None
    taxa_emolumentos = None
    
    # Tentar extrair do form
    taxa_corretagem_str = (req.form.get('taxa_corretagem') or 
                          req.form.get('corretagem') or 
                          req.form.get('brokerage') or
                          req.form.get('backtest_commission'))
    taxa_emolumentos_str = (req.form.get('taxa_emolumentos') or 
                           req.form.get('emolumentos') or 
                           req.form.get('emoluments') or
                           req.form.get('backtest_fees'))
    
    # Tentar extrair do JSON
    json_data = None
    if req.is_json:
        json_data = req.get_json(silent=True)
        if json_data:
            # Extrair corretagem do JSON
            if not taxa_corretagem_str:
                # Tentar diferentes formatos
                if 'corretagem' in json_data:
                    corretagem_data = json_data['corretagem']
                    if isinstance(corretagem_data, dict):
                        # Formato: {"method": "fixed", "value": 0.5}
                        taxa_corretagem_str = str(corretagem_data.get('value', 0))
                    else:
                        taxa_corretagem_str = str(corretagem_data)
                elif 'brokerage' in json_data:
                    brokerage_data = json_data['brokerage']
                    if isinstance(brokerage_data, dict):
                        taxa_corretagem_str = str(brokerage_data.get('value', 0))
                    else:
                        taxa_corretagem_str = str(brokerage_data)
                else:
                    taxa_corretagem_str = (json_data.get('taxa_corretagem') or 
                                          json_data.get('backtest_commission'))
            
            # Extrair emolumentos do JSON
            if not taxa_emolumentos_str:
                if 'emolumentos' in json_data:
                    emolumentos_data = json_data['emolumentos']
                    if isinstance(emolumentos_data, dict):
                        taxa_emolumentos_str = str(emolumentos_data.get('value', 0))
                    else:
                        taxa_emolumentos_str = str(emolumentos_data)
                elif 'emoluments' in json_data:
                    emoluments_data = json_data['emoluments']
                    if isinstance(emoluments_data, dict):
                        taxa_emolumentos_str = str(emoluments_data.get('value', 0))
                    else:
                        taxa_emolumentos_str = str(emoluments_data)
                else:
                    taxa_emolumentos_str = (json_data.get('taxa_emolumentos') or 
                                            json_data.get('backtest_fees'))
    
    # Converter para float se fornecido
    if taxa_corretagem_str:
        try:
            taxa_corretagem = float(taxa_corretagem_str)
            print(f"üíº Taxa de corretagem extra√≠da do request: R$ {taxa_corretagem:.2f}")
        except (ValueError, TypeError):
            print(f"‚ö†Ô∏è Taxa de corretagem inv√°lida: {taxa_corretagem_str}")
    
    if taxa_emolumentos_str:
        try:
            taxa_emolumentos = float(taxa_emolumentos_str)
            print(f"üíº Taxa de emolumentos extra√≠da do request: R$ {taxa_emolumentos:.2f}")
        except (ValueError, TypeError):
            print(f"‚ö†Ô∏è Taxa de emolumentos inv√°lida: {taxa_emolumentos_str}")
    
    return taxa_corretagem, taxa_emolumentos

def calcular_custos_operacionais(df: pd.DataFrame, taxa_corretagem: float = None, taxa_emolumentos: float = None) -> Dict[str, Any]:
    """
    Calcula custos operacionais estimados
    CORRE√á√ÉO: Separa corretamente corretagem e emolumentos
    
    Args:
        df: DataFrame com trades
        taxa_corretagem: Taxa de corretagem (por roda ou por trade, dependendo do valor)
        taxa_emolumentos: Taxa de emolumentos (percentual ou fixa por roda, dependendo do valor)
    """
    if df.empty:
        return {}
    
    # CORRE√á√ÉO: Usar valores padr√£o se n√£o fornecidos
    if taxa_corretagem is None:
        taxa_corretagem = 0.50  # R$ 0,50 por roda (padr√£o mercado brasileiro)
    if taxa_emolumentos is None:
        taxa_emolumentos = 0.03  # R$ 0,03 por roda (padr√£o mercado brasileiro)
    
    # CORRE√á√ÉO: Verificar se as colunas existem antes de usar
    required_cols = []
    if 'entry_price' in df.columns and 'exit_price' in df.columns:
        required_cols = ['entry_price', 'exit_price']
    elif 'Pre√ßo Compra' in df.columns and 'Pre√ßo Venda' in df.columns:
        # Usar colunas originais se n√£o tiver as normalizadas
        required_cols = ['Pre√ßo Compra', 'Pre√ßo Venda']
    else:
        # Se n√£o tem colunas de pre√ßo, usar todas as linhas (assumir que s√£o v√°lidas)
        df_valid = df.copy()
        total_trades = len(df_valid)
        # Retornar valores b√°sicos sem c√°lculos de valor operado
        quantidade_rodas = total_trades * 2  # Assumir 2 rodas por trade
        custo_corretagem = quantidade_rodas * taxa_corretagem if taxa_corretagem < 1.0 else total_trades * taxa_corretagem
        custo_emolumentos = quantidade_rodas * taxa_emolumentos
        custo_total = custo_corretagem + custo_emolumentos
        
        return {
            "total_trades": total_trades,
            "quantidade_rodas": int(quantidade_rodas),
            "valor_total_operado": 0.0,  # N√£o foi poss√≠vel calcular
            "custo_corretagem": round(custo_corretagem, 2),
            "custo_emolumentos": round(custo_emolumentos, 2),
            "custo_total": round(custo_total, 2),
            "custo_por_trade": round(custo_total / total_trades, 2) if total_trades > 0 else 0.0
        }
    
    df_valid = df.dropna(subset=required_cols)
    total_trades = len(df_valid)
    
    if total_trades == 0:
        return {
            "total_trades": 0,
            "valor_total_operado": 0.0,
            "custo_corretagem": 0.0,
            "custo_emolumentos": 0.0,
            "custo_total": 0.0,
            "custo_por_trade": 0.0
        }
    
    # Calcular quantidade de rodas
    quantidade_rodas = 0
    if 'quantity' in df_valid.columns:
        quantidade_rodas = df_valid['quantity'].sum() * 2  # Entrada + sa√≠da
    elif 'qty_buy' in df_valid.columns and 'qty_sell' in df_valid.columns:
        quantidade_rodas = (df_valid['qty_buy'].sum() + df_valid['qty_sell'].sum())
    elif 'Qtd Compra' in df_valid.columns and 'Qtd Venda' in df_valid.columns:
        quantidade_rodas = (df_valid['Qtd Compra'].sum() + df_valid['Qtd Venda'].sum())
    else:
        # Fallback: assumir 2 rodas por trade (entrada + sa√≠da)
        quantidade_rodas = total_trades * 2
    
    # Calcular valor total operado
    # CORRE√á√ÉO: Verificar se as colunas de pre√ßo existem antes de usar
    has_entry_price = 'entry_price' in df_valid.columns
    has_exit_price = 'exit_price' in df_valid.columns
    has_preco_compra = 'Pre√ßo Compra' in df_valid.columns
    has_preco_venda = 'Pre√ßo Venda' in df_valid.columns
    
    if has_entry_price and has_exit_price:
        if 'position_size' in df_valid.columns:
            valor_entrada = df_valid['entry_price'] * df_valid['position_size']
            valor_saida = df_valid['exit_price'] * df_valid['position_size']
        elif 'quantity' in df_valid.columns:
            valor_entrada = df_valid['entry_price'] * df_valid['quantity']
            valor_saida = df_valid['exit_price'] * df_valid['quantity']
        else:
            # Fallback: assumir 1 contrato
            valor_entrada = df_valid['entry_price']
            valor_saida = df_valid['exit_price']
        valor_total_operado = float((valor_entrada + valor_saida).sum())
    elif has_preco_compra and has_preco_venda:
        # Usar colunas originais se n√£o tiver as normalizadas
        if 'Qtd Compra' in df_valid.columns:
            valor_entrada = df_valid['Pre√ßo Compra'] * df_valid['Qtd Compra']
            valor_saida = df_valid['Pre√ßo Venda'] * df_valid['Qtd Venda'] if 'Qtd Venda' in df_valid.columns else df_valid['Pre√ßo Venda'] * df_valid['Qtd Compra']
        else:
            valor_entrada = df_valid['Pre√ßo Compra']
            valor_saida = df_valid['Pre√ßo Venda']
        valor_total_operado = float((valor_entrada + valor_saida).sum())
    else:
        # Se n√£o tem colunas de pre√ßo, n√£o √© poss√≠vel calcular valor operado
        valor_total_operado = 0.0
    
    # CORRE√á√ÉO: Calcular corretagem (sempre por roda)
    # Se taxa_corretagem < 1, √© por roda. Se >= 1, pode ser por trade
    if taxa_corretagem < 1.0:
        # Taxa por roda
        custo_corretagem = quantidade_rodas * taxa_corretagem
    else:
        # Taxa por trade (assumir que √© o valor total para entrada + sa√≠da)
        custo_corretagem = total_trades * taxa_corretagem
    
    # CORRE√á√ÉO: Calcular emolumentos (pode ser percentual ou fixo por roda)
    if taxa_emolumentos < 1.0:
        # Se < 1, pode ser percentual (ex: 0.03 = 3%) ou fixo por roda (ex: 0.03 = R$ 0,03)
        # Tentar calcular como percentual primeiro
        if valor_total_operado > 0:
            # Assumir que √© percentual se o valor operado for grande
            if valor_total_operado > 10000:
                custo_emolumentos = valor_total_operado * (taxa_emolumentos / 100.0)
            else:
                # Se valor operado √© pequeno, provavelmente √© fixo por roda
                custo_emolumentos = quantidade_rodas * taxa_emolumentos
        else:
            # Se n√£o tem valor operado, usar por roda
            custo_emolumentos = quantidade_rodas * taxa_emolumentos
    else:
        # Taxa fixa por roda (valores >= 1)
        custo_emolumentos = quantidade_rodas * taxa_emolumentos
    
    custo_total = custo_corretagem + custo_emolumentos
    
    print(f"üíº calcular_custos_operacionais: Corretagem: R$ {custo_corretagem:.2f} ({quantidade_rodas:.0f} rodas √ó R$ {taxa_corretagem:.2f}), Emolumentos: R$ {custo_emolumentos:.2f}")
    
    return {
        "total_trades": total_trades,
        "quantidade_rodas": int(quantidade_rodas),
        "valor_total_operado": round(valor_total_operado, 2),
        "custo_corretagem": round(custo_corretagem, 2),
        "custo_emolumentos": round(custo_emolumentos, 2),
        "custo_total": round(custo_total, 2),
        "custo_por_trade": round(custo_total / total_trades, 2) if total_trades > 0 else 0.0
    }

# ============ FUN√á√ïES PARA M√âTRICAS DI√ÅRIAS ============

def calcular_metricas_diarias(df: pd.DataFrame) -> Dict[str, Any]:
    """Calcula m√©tricas di√°rias baseadas nas trades"""
    if df.empty:
        return {}
    
    # CORRE√á√ÉO: Normalizar o DataFrame se necess√°rio
    from FunCalculos import _normalize_trades_dataframe
    if 'entry_date' not in df.columns or 'pnl' not in df.columns:
        df = _normalize_trades_dataframe(df)
        if df.empty:
            return {}
    
    # Filtrar trades v√°lidas
    df_valid = df.dropna(subset=['pnl', 'entry_date'])
    
    if df_valid.empty:
        return {}
    
    # Agrupar por dia
    df_valid['date'] = df_valid['entry_date'].dt.date
    daily_stats = df_valid.groupby('date').agg({
        'pnl': ['sum', 'count', 'mean'],
    }).round(2)
    
    daily_stats.columns = ['total_pnl', 'total_trades', 'avg_pnl']
    daily_stats['win_rate'] = df_valid.groupby('date').apply(
        lambda x: (x['pnl'] > 0).sum() / len(x) * 100
    ).round(2)
    
    # Calcular sequ√™ncias de dias
    daily_stats['is_winner'] = daily_stats['total_pnl'] > 0
    daily_stats['is_loser'] = daily_stats['total_pnl'] < 0
    
    # Calcular drawdown
    daily_stats['cumulative_pnl'] = daily_stats['total_pnl'].cumsum()
    daily_stats['running_max'] = daily_stats['cumulative_pnl'].expanding().max()
    daily_stats['drawdown'] = daily_stats['cumulative_pnl'] - daily_stats['running_max']
    

    return daily_stats

def calcular_metricas_diarias(df: pd.DataFrame) -> pd.DataFrame:
    """
    Calcula m√©tricas di√°rias baseadas nas trades com drawdown correto
    CORRIGIDO: Sempre agrupa por data antes de calcular m√©tricas di√°rias
    """
    if df.empty:
        print("‚ö†Ô∏è DataFrame vazio para c√°lculo de m√©tricas di√°rias")
        return pd.DataFrame()
    
    print(f"üîç DEBUG - calcular_metricas_diarias:")
    print(f"  Total de trades: {len(df)}")
    print(f"  Colunas dispon√≠veis: {df.columns.tolist()}")
    
    # CORRE√á√ÉO: Normalizar o DataFrame se necess√°rio
    from FunCalculos import _normalize_trades_dataframe
    if 'entry_date' not in df.columns or 'pnl' not in df.columns:
        print("  üîÑ Normalizando DataFrame...")
        df = _normalize_trades_dataframe(df)
        if df.empty:
            print("‚ö†Ô∏è DataFrame vazio ap√≥s normaliza√ß√£o")
            return pd.DataFrame()
        print(f"  ‚úÖ DataFrame normalizado. Colunas: {df.columns.tolist()}")
    
    # Filtrar trades v√°lidas e ordenar por data
    df_valid = df.dropna(subset=['pnl', 'entry_date']).copy()
    df_valid = df_valid.sort_values('entry_date').reset_index(drop=True)
    
    print(f"  Trades v√°lidas ap√≥s filtro: {len(df_valid)}")
    
    if df_valid.empty:
        print("‚ö†Ô∏è Nenhuma trade v√°lida encontrada")
        return pd.DataFrame()
    
    # Verificar se temos as colunas necess√°rias
    if 'pnl' not in df_valid.columns:
        print("‚ùå Coluna 'pnl' n√£o encontrada. Colunas dispon√≠veis:", df_valid.columns.tolist())
        return pd.DataFrame()
    
    if 'entry_date' not in df_valid.columns:
        print("‚ùå Coluna 'entry_date' n√£o encontrada. Colunas dispon√≠veis:", df_valid.columns.tolist())
        return pd.DataFrame()
    
    # CORRE√á√ÉO 1: Garantir que sempre agrupamos por data
    df_valid['date'] = pd.to_datetime(df_valid['entry_date']).dt.date
    print(f"  Datas √∫nicas encontradas: {df_valid['date'].nunique()}")
    print(f"  Primeira data: {df_valid['date'].min()}")
    print(f"  √öltima data: {df_valid['date'].max()}")
    
    # CORRE√á√ÉO 2: Calcular saldo cumulativo por dia (n√£o por trade)
    df_valid['saldo_cumulativo'] = df_valid['pnl'].cumsum()
    df_valid['saldo_maximo'] = df_valid['saldo_cumulativo'].cummax()
    df_valid['drawdown_trade'] = df_valid['saldo_cumulativo'] - df_valid['saldo_maximo']
    
    # CORRE√á√ÉO 3: Agrupar por dia ANTES de calcular estat√≠sticas
    daily_stats = df_valid.groupby('date').agg({
        'pnl': ['sum', 'count', 'mean'],
        'saldo_cumulativo': 'last',  # Saldo final do dia
        'saldo_maximo': 'last',      # Pico at√© o final do dia
        'drawdown_trade': 'min'      # Pior drawdown do dia
    }).round(2)
    
    # Simplificar nomes das colunas
    daily_stats.columns = ['total_pnl', 'total_trades', 'avg_pnl', 'saldo_final', 'peak_final', 'drawdown_dia']
    
    # CORRE√á√ÉO 4: Calcular win rate di√°rio baseado no PnL consolidado do dia
    daily_stats['is_winner'] = daily_stats['total_pnl'] > 0
    daily_stats['is_loser'] = daily_stats['total_pnl'] < 0
    
    # CORRE√á√ÉO 5: Calcular drawdown correto para o dia (baseado no saldo final vs pico final)
    daily_stats['drawdown'] = daily_stats['saldo_final'] - daily_stats['peak_final']
    
    # CORRE√á√ÉO 6: Calcular m√°ximo hist√≥rico e drawdown cumulativo por dia
    daily_stats['running_max'] = daily_stats['saldo_final'].cummax()
    daily_stats['drawdown_cumulativo'] = daily_stats['saldo_final'] - daily_stats['running_max']
    
    # PADRONIZA√á√ÉO: Usar fun√ß√£o centralizada para calcular drawdown
    drawdown_data = calcular_drawdown_padronizado(df)
    max_drawdown_trades = drawdown_data["max_drawdown"]
    max_drawdown_pct_trades = drawdown_data["max_drawdown_pct"]
    
    # Logs de debug para verificar padroniza√ß√£o
    print(f"  PADRONIZA√á√ÉO - Drawdown m√°ximo (trades): R$ {max_drawdown_trades:.2f} ({max_drawdown_pct_trades:.2f}%)")
    print(f"  PADRONIZA√á√ÉO - Drawdown m√°ximo (dias): R$ {abs(daily_stats['drawdown_cumulativo'].min()):.2f}")
    print(f"  PADRONIZA√á√ÉO - Verifica√ß√£o: valores devem ser iguais")
    
    # Logs de debug detalhados
    print(f"  Dias com resultado positivo: {len(daily_stats[daily_stats['total_pnl'] > 0])}")
    print(f"  Dias com resultado negativo: {len(daily_stats[daily_stats['total_pnl'] < 0])}")
    print(f"  Maior ganho di√°rio: {daily_stats['total_pnl'].max()}")
    print(f"  Maior perda di√°ria: {daily_stats['total_pnl'].min()}")
    print(f"  M√©dia de trades por dia: {daily_stats['total_trades'].mean():.1f}")
    print(f"  Total de dias operados: {len(daily_stats)}")
    
    # Verificar se os dados est√£o corretos
    print(f"  Verifica√ß√£o - Soma de PnL di√°rio: {daily_stats['total_pnl'].sum()}")
    print(f"  Verifica√ß√£o - Soma de PnL original: {df_valid['pnl'].sum()}")
    
    return daily_stats.reset_index()


def calcular_metricas_principais(df: pd.DataFrame, taxa_juros_mensal: float = 0.01, capital_inicial: float = None) -> Dict[str, Any]:
    """
    Calcula as m√©tricas principais do dashboard
    CORRIGIDO: Usa a mesma l√≥gica de drawdown das outras fun√ß√µes
    E SHARPE RATIO com f√≥rmula espec√≠fica
    """
    if df.empty:
        return {}
    
    # CORRE√á√ÉO: Normalizar o DataFrame se necess√°rio
    from FunCalculos import _normalize_trades_dataframe
    if 'entry_date' not in df.columns or 'pnl' not in df.columns:
        df = _normalize_trades_dataframe(df)
        if df.empty:
            return {}
    
    # Usar a fun√ß√£o de m√©tricas di√°rias corrigida
    daily_stats = calcular_metricas_diarias(df)
    
    if daily_stats.empty:
        return {}
    
    # Calcular m√©tricas globais usando os mesmos campos das outras fun√ß√µes
    df_valid = df.dropna(subset=['pnl', 'entry_date']).copy()
    df_valid = df_valid.sort_values('entry_date').reset_index(drop=True)
    
    # Calcular saldo cumulativo (igual √†s outras fun√ß√µes)
    df_valid['Saldo'] = df_valid['pnl'].cumsum()
    df_valid['Saldo_Maximo'] = df_valid['Saldo'].cummax()
    df_valid['Drawdown'] = df_valid['Saldo'] - df_valid['Saldo_Maximo']
    
    # M√©tricas gerais
    total_pnl = df_valid['pnl'].sum()
    total_trades = len(df_valid)
    winning_trades = len(df_valid[df_valid['pnl'] > 0])
    losing_trades = len(df_valid[df_valid['pnl'] < 0])
    
    # CORRE√á√ÉO: Payoff Ratio (Ganho m√©dio / Perda m√©dia) com valida√ß√£o
    wins_pnl = df_valid[df_valid['pnl'] > 0]['pnl'].dropna()
    losses_pnl = df_valid[df_valid['pnl'] < 0]['pnl'].dropna()
    
    avg_win = wins_pnl.mean() if len(wins_pnl) > 0 else 0.0
    avg_loss = abs(losses_pnl.mean()) if len(losses_pnl) > 0 else 0.0
    
    # Garantir valores v√°lidos
    if pd.isna(avg_win) or np.isinf(avg_win):
        avg_win = 0.0
    if pd.isna(avg_loss) or np.isinf(avg_loss):
        avg_loss = 0.0
    
    # Calcular payoff corretamente
    if avg_loss > 0 and not pd.isna(avg_loss) and not np.isinf(avg_loss):
        payoff_ratio = avg_win / avg_loss if not pd.isna(avg_win) and not np.isinf(avg_win) else 0.0
    else:
        payoff_ratio = 0.0
    
    # Garantir que payoff seja v√°lido
    if pd.isna(payoff_ratio) or np.isinf(payoff_ratio):
        payoff_ratio = 0.0
    
    # PADRONIZADO: Usar fun√ß√£o centralizada para calcular drawdown
    drawdown_data = calcular_drawdown_padronizado(df)
    max_drawdown = drawdown_data["max_drawdown"]
    max_drawdown_pct = drawdown_data["max_drawdown_pct"]
    saldo_final = drawdown_data["saldo_final"]
    capital_inicial = drawdown_data["capital_inicial"]
    
    # CALCULAR DD M√âDIO - CORRE√á√ÉO ADICIONADA
    # Calcular drawdown m√©dio baseado nos trades individuais
    equity = df_valid['pnl'].cumsum()
    peak = equity.cummax()
    drawdown_series = equity - peak
    drawdown_values = drawdown_series[drawdown_series < 0].abs()  # Apenas valores negativos (drawdowns)
    avg_drawdown = drawdown_values.mean() if len(drawdown_values) > 0 else 0
    
    # CAPITAL INICIAL CORRIGIDO
    # Se n√£o fornecido, calcular baseado no drawdown m√°ximo
    if capital_inicial is None:
        # M√©todo 1: Baseado no fato de que drawdown% = drawdown$ / saldo_final
        # Se drawdown% = 66.22% e drawdown$ = 835.8
        # Ent√£o: saldo_final = drawdown$ / (drawdown% / 100)
        saldo_final = df_valid['Saldo'].iloc[-1]  # 1262.2
        
        # Para calcular capital inicial, usar: capital = saldo_final + abs(saldo_minimo)
        saldo_minimo = df_valid['Saldo'].min()  # Ponto mais baixo
        capital_estimado = saldo_final + abs(saldo_minimo) if saldo_minimo < 0 else saldo_final + max_drawdown
        
        # M√©todo alternativo: usar drawdown 3x como base m√≠nima
        capital_por_drawdown = max_drawdown * 3  # 835.8 * 3 = 2507.4
        
        # Usar o maior entre os dois m√©todos para ser conservador
        capital_inicial = max(capital_estimado, capital_por_drawdown)
    
    # SHARPE RATIO CORRIGIDO - Usar desvio padr√£o corretamente
    # Calcular retornos dos trades individuais (como no FunCalculos.py)
    returns = df_valid['pnl'].values
    mean_return = np.mean(returns) if len(returns) > 0 else 0
    # CORRE√á√ÉO: Usar desvio padr√£o amostral (ddof=1) para corre√ß√£o de Bessel
    # Isso √© importante para amostras pequenas (corre√ß√£o de vi√©s)
    std_return = np.std(returns, ddof=1) if len(returns) > 1 else 0
    
    # CORRE√á√ÉO: Calcular m√©tricas estat√≠sticas adicionais usando desvio padr√£o
    volatility = std_return
    variance = np.var(returns, ddof=1) if len(returns) > 1 else 0
    coefficient_of_variation = (volatility / abs(mean_return) * 100) if mean_return != 0 else 0
    
    # CORRE√á√ÉO: Ajustar CDI para o per√≠odo dos retornos
    # CDI √© anual (12%), mas retornos s√£o por trade
    # Ajustamos proporcionalmente ao n√∫mero de trades no per√≠odo
    cdi_annual = 0.12  # Taxa anual (12% ao ano)
    # Para retornos por trade, ajustamos o CDI baseado no n√∫mero de trades
    # Assumindo ~252 dias √∫teis por ano e m√©dia de trades por dia
    if days_traded > 0:
        trades_per_day = total_trades / days_traded
        # Ajustar CDI para retorno por trade: CDI_por_trade = CDI_anual / (252 * trades_por_dia)
        # Simplificado: usar CDI diretamente se n√£o temos informa√ß√£o suficiente
        cdi = cdi_annual / 252 if trades_per_day > 0 else cdi_annual
    else:
        cdi = cdi_annual
    
    sharpe_ratio = ((mean_return - cdi) / std_return) if std_return > 0 else 0
    
    # Fator de Recupera√ß√£o
    recovery_factor = total_pnl / max_drawdown if max_drawdown != 0 else 0
    
    # Dias operados
    days_traded = len(daily_stats)
    
    # Estat√≠sticas di√°rias CORRIGIDAS - baseadas em dias, n√£o em opera√ß√µes
    winning_days = len(daily_stats[daily_stats['total_pnl'] > 0])
    losing_days = len(daily_stats[daily_stats['total_pnl'] < 0])
    daily_win_rate = (winning_days / days_traded * 100) if days_traded > 0 else 0
    
    # Ganhos e perdas di√°rias CORRIGIDOS - baseados em dias, n√£o em opera√ß√µes
    daily_avg_win = daily_stats[daily_stats['total_pnl'] > 0]['total_pnl'].mean() if winning_days > 0 else 0
    daily_avg_loss = abs(daily_stats[daily_stats['total_pnl'] < 0]['total_pnl'].mean()) if losing_days > 0 else 0
    daily_max_win = daily_stats['total_pnl'].max() if not daily_stats.empty else 0
    daily_max_loss = daily_stats['total_pnl'].min() if not daily_stats.empty else 0  # J√° √© negativo
    
    # M√©dia de opera√ß√µes por dia
    avg_trades_per_day = total_trades / days_traded if days_traded > 0 else 0
    
    # Sequ√™ncias consecutivas
    consecutive_wins, consecutive_losses = calcular_sequencias_consecutivas(daily_stats)
    
    # Debug logs para verificar os c√°lculos
    print(f"üîç DEBUG - M√©tricas di√°rias:")
    print(f"  Dias operados: {days_traded}")
    print(f"  Dias vencedores: {winning_days}")
    print(f"  Dias perdedores: {losing_days}")
    print(f"  Taxa de acerto di√°ria: {daily_win_rate}%")
    print(f"  Ganho m√©dio di√°rio: {daily_avg_win}")
    print(f"  Perda m√©dia di√°ria: {daily_avg_loss}")
    print(f"  Ganho m√°ximo di√°rio: {daily_max_win}")
    print(f"  Perda m√°xima di√°ria: {daily_max_loss}")
    print(f"  Opera√ß√µes por dia: {avg_trades_per_day}")
    print(f"  DD M√©dio: {avg_drawdown:.2f}")
    print(f"  Sharpe Ratio (corrigido): {sharpe_ratio:.2f}")
    
    return {
        "metricas_principais": {
            "sharpe_ratio": round(sharpe_ratio, 2),  # PADRONIZADO - mesma f√≥rmula do FunCalculos.py
            "volatilidade": round(volatility, 2),
            "variancia": round(variance, 2),
            "coeficiente_variacao": round(coefficient_of_variation, 2),
            "fator_recuperacao": round(recovery_factor, 2),
            "drawdown_maximo": round(-max_drawdown, 2),  # Negativo para compatibilidade
            "drawdown_maximo_pct": round(max_drawdown_pct, 2),
            "drawdown_medio": round(avg_drawdown, 2),  # NOVO: DD M√©dio calculado
            "dias_operados": int(days_traded),
            "resultado_liquido": round(total_pnl, 2),
            # PADRONIZA√á√ÉO: Usar drawdown calculado com trades individuais (mesmo valor do original)
            "drawdown_maximo_padronizado": round(-max_drawdown, 2),  # Negativo para compatibilidade
            "drawdown_maximo_pct_padronizado": round(max_drawdown_pct, 2),
            # PADRONIZA√á√ÉO: Valores para API (positivos)
            "max_drawdown_padronizado": round(max_drawdown, 2),  # Valor positivo para API
            "max_drawdown_pct_padronizado": round(max_drawdown_pct, 2),  # Percentual para API
            # Campos adicionais para debug/transpar√™ncia
            "capital_estimado": round(capital_inicial, 2)
        },
        "ganhos_perdas": {
            "ganho_medio_diario": round(daily_avg_win, 2),
            "perda_media_diaria": round(daily_avg_loss, 2),
            "payoff_diario": round(daily_avg_win / daily_avg_loss if daily_avg_loss > 0 and not pd.isna(daily_avg_loss) and not np.isinf(daily_avg_loss) and not pd.isna(daily_avg_win) and not np.isinf(daily_avg_win) else 0.0, 2),
            "ganho_maximo_diario": round(daily_max_win, 2),
            "perda_maxima_diaria": round(abs(daily_max_loss), 2)  # Valor absoluto para compatibilidade
        },
        "estatisticas_operacao": {
            "media_operacoes_dia": round(avg_trades_per_day, 1),
            "taxa_acerto_diaria": round(daily_win_rate, 2),
            "dias_vencedores_perdedores": f"{winning_days} / {losing_days}",
            "dias_perdedores_consecutivos": consecutive_losses,
            "dias_vencedores_consecutivos": consecutive_wins
        }
    }

def calcular_sharpe_ratio_customizado(total_pnl: float, max_drawdown: float, periodo_meses: float, taxa_juros_mensal: float = 0.01, capital_inicial: float = None) -> Dict[str, float]:
    """
    Calcula o Sharpe Ratio usando a f√≥rmula espec√≠fica fornecida
    
    Args:
        total_pnl: Lucro/preju√≠zo total
        max_drawdown: Drawdown m√°ximo (valor positivo)
        periodo_meses: Per√≠odo em meses
        taxa_juros_mensal: Taxa de juros mensal (padr√£o 1% = 0.01)
        capital_inicial: Capital inicial (se None, ser√° estimado)
    
    Returns:
        Dict com os componentes do c√°lculo e o resultado final
    """
    
    # Estimar capital inicial se n√£o fornecido
    if capital_inicial is None:
        capital_inicial = max(max_drawdown * 3, abs(total_pnl) * 2, 100000)
    
    # Taxa de juros do per√≠odo
    taxa_juros_periodo = taxa_juros_mensal * periodo_meses
    
    # Rentabilidade do per√≠odo em percentual
    rentabilidade_periodo_pct = (total_pnl / capital_inicial) * 100
    
    # Numerador: (Rentabilidade per√≠odo - taxa de juros per√≠odo)
    numerador = rentabilidade_periodo_pct - (taxa_juros_periodo * 100)
    
    # Denominador: Risco (drawdown / 3x drawdown)
    drawdown_3x = max_drawdown * 3
    risco_pct = (max_drawdown / drawdown_3x) * 100 if drawdown_3x > 0 else 33.33  # Valor padr√£o em vez de 100
    
    # Sharpe Ratio
    sharpe_ratio = numerador / risco_pct if risco_pct != 0 and risco_pct != 33.33 else 0
    
    return {
        "sharpe_ratio": round(sharpe_ratio, 2),
        "total_pnl": total_pnl,
        "capital_inicial": capital_inicial,
        "rentabilidade_pct": round(rentabilidade_periodo_pct, 2),
        "taxa_juros_periodo_pct": round(taxa_juros_periodo * 100, 2),
        "numerador": round(numerador, 2),
        "max_drawdown": max_drawdown,
        "drawdown_3x": drawdown_3x,
        "risco_pct": round(risco_pct, 2),
        "periodo_meses": periodo_meses
    }


def calcular_sequencias_consecutivas(daily_stats: pd.DataFrame) -> Tuple[int, int]:
    """Calcula sequ√™ncias consecutivas de dias vencedores e perdedores"""
    if daily_stats.empty:
        return 0, 0
    
    # Sequ√™ncias de vit√≥rias
    wins = daily_stats['is_winner'].astype(int)
    win_sequences = []
    current_sequence = 0
    
    for win in wins:
        if win:
            current_sequence += 1
        else:
            if current_sequence > 0:
                win_sequences.append(current_sequence)
            current_sequence = 0
    if current_sequence > 0:
        win_sequences.append(current_sequence)
    
    # Sequ√™ncias de perdas
    losses = daily_stats['is_loser'].astype(int)
    loss_sequences = []
    current_sequence = 0
    
    for loss in losses:
        if loss:
            current_sequence += 1
        else:
            if current_sequence > 0:
                loss_sequences.append(current_sequence)
            current_sequence = 0
    if current_sequence > 0:
        loss_sequences.append(current_sequence)
    
    max_consecutive_wins = max(win_sequences) if win_sequences else 0
    max_consecutive_losses = max(loss_sequences) if loss_sequences else 0
    
    return max_consecutive_wins, max_consecutive_losses
# Adicione ao seu main.py

import pandas as pd
import numpy as np
from typing import Dict, Any
from flask import Flask, request, jsonify

def calcular_disciplina_completa(df: pd.DataFrame, fator_disciplina: float = 0.2, multiplicador_furia: float = 2.0) -> Dict[str, Any]:
    """
    Calcula TODOS os √≠ndices de disciplina em uma fun√ß√£o √∫nica:
    - Disciplina Stop (por opera√ß√£o)
    - Disciplina Perda/Dia (por dia)
    - M√©trica de F√∫ria Di√°ria (baseada em m√∫ltiplo da perda m√©dia)
    
    Args:
        df: DataFrame com as opera√ß√µes
        fator_disciplina: Fator para calcular meta m√°xima (padr√£o 20% = 0.2)
        multiplicador_furia: Multiplicador para definir "dia de f√∫ria" (padr√£o 2.0 = 2x a perda m√©dia)
    
    Returns:
        Dict com todas as m√©tricas de disciplina (JSON serializable)
    """
    if df.empty:
        return {"error": "DataFrame vazio"}
    
    # Encontrar colunas corretas
    resultado_col = None
    data_col = None
    quantidade_col = None
    
    column_candidates = [
        'operation_result', 'pnl', 'resultado',
        'Res. Opera√ß√£o', 'Res. Operacao', 'Res. Opera√ß√£o Bruta',
        'Res. Intervalo', 'Res. Intervalo Bruto', 'Total'
    ]

    for col_name in column_candidates:
        if col_name in df.columns:
            if col_name not in ['operation_result', 'pnl', 'resultado']:
                tmp_col = f"_resultado_tmp_{col_name}"
                df[tmp_col] = pd.to_numeric(df[col_name], errors='coerce')
                resultado_col = tmp_col
            else:
                resultado_col = col_name
            break
    
    for col_name in ['entry_date', 'data_abertura', 'data']:
        if col_name in df.columns:
            data_col = col_name
            break
    
    for col_name in ['qty_buy', 'Quantidade', 'qtd', 'qty', 'volume', 'contratos', 'acoes', 'size']:
        if col_name in df.columns:
            quantidade_col = col_name
            break
    
    if resultado_col is None or data_col is None:
        return {"error": "Colunas de resultado ou data n√£o encontradas"}
    
    # Quantidade √© opcional
    quantidade_disponivel = quantidade_col is not None
    
    # Filtrar opera√ß√µes v√°lidas
    if quantidade_disponivel:
        df_valid = df.dropna(subset=[resultado_col, data_col, quantidade_col]).copy()
    else:
        df_valid = df.dropna(subset=[resultado_col, data_col]).copy()
    
    if df_valid.empty:
        return {"error": "Nenhuma opera√ß√£o v√°lida encontrada"}
    
    # Converter data para datetime se necess√°rio
    if not pd.api.types.is_datetime64_any_dtype(df_valid[data_col]):
        df_valid[data_col] = pd.to_datetime(df_valid[data_col])
    
    # ===== VARI√ÅVEIS GERAIS =====
    total_operacoes = int(len(df_valid))
    
    # ===== DISCIPLINA ALAVANCAGEM =====
    if quantidade_disponivel:
        # Calcular m√©dia de quantidade
        media_quantidade = float(df_valid[quantidade_col].mean())
        limite_alavancagem = media_quantidade * 2  # 2x a m√©dia de quantidade
        
        # Identificar opera√ß√µes que ultrapassaram 2x a m√©dia
        operacoes_alavancadas = df_valid[df_valid[quantidade_col] > limite_alavancagem]
        qtd_operacoes_alavancadas = int(len(operacoes_alavancadas))
        total_operacoes_quantidade = int(len(df_valid))
        
        # Calcular √≠ndice de disciplina de alavancagem
        indice_disciplina_alavancagem = (1 - (qtd_operacoes_alavancadas / total_operacoes_quantidade)) * 100
        
        disciplina_alavancagem = {
            "disponivel": True,
            "total_operacoes": total_operacoes_quantidade,
            "media_quantidade": round(media_quantidade, 2),
            "limite_alavancagem": round(limite_alavancagem, 2),
            "operacoes_alavancadas": qtd_operacoes_alavancadas,
            "operacoes_dentro_limite": total_operacoes_quantidade - qtd_operacoes_alavancadas,
            "indice_disciplina_alavancagem": round(indice_disciplina_alavancagem, 2),
            "detalhes_alavancagem": [
                {
                    "operacao": i + 1,
                    "quantidade": int(row[quantidade_col]),
                    "excesso_limite": round(float(row[quantidade_col]) - limite_alavancagem, 2),
                    "multiplo_media": round(float(row[quantidade_col]) / media_quantidade, 2),
                    "data": row[data_col].strftime('%d/%m/%Y'),
                    "resultado": round(float(row[resultado_col]), 2)
                }
                for i, (_, row) in enumerate(operacoes_alavancadas.iterrows())
            ] if qtd_operacoes_alavancadas > 0 else []
        }
    else:
        disciplina_alavancagem = {
            "disponivel": False,
            "motivo": "Coluna de quantidade n√£o encontrada",
            "colunas_procuradas": ['Qtd Compra', 'Quantidade', 'qtd', 'qty', 'volume', 'contratos', 'acoes', 'size']
        }
    
    # ===== PREPARAR DADOS DI√ÅRIOS =====
    df_valid['Data'] = df_valid[data_col].dt.date
    
    # Agrupar por dia
    resultado_diario = df_valid.groupby('Data').agg({
        resultado_col: ['sum', 'count', 'min']
    }).round(2)
    
    resultado_diario.columns = ['PnL_Dia', 'Trades_Dia', 'Pior_Trade_Dia']
    resultado_diario = resultado_diario.reset_index()
    
    # Separar dias com perda
    dias_com_perda = resultado_diario[resultado_diario['PnL_Dia'] < 0].copy()
    
    # ===== NOVA M√âTRICA: F√öRIA DI√ÅRIA =====
    if dias_com_perda.empty:
        furia_diaria = {
            "disponivel": False,
            "motivo": "N√£o h√° dias com perda para calcular f√∫ria",
            "dias_com_perda": 0,
            "perda_media_diaria": 0.0,
            "limite_furia": 0.0,
            "dias_furia": 0,
            "total_dias_operados": int(len(resultado_diario)),
            "percentual_dias_furia": 0.0,
            "frequencia_furia": 0.0,
            "detalhes_furia": []
        }
    else:
        # Calcular perda m√©dia di√°ria
        perda_media_diaria = float(abs(dias_com_perda['PnL_Dia'].mean()))
        
        # Definir limite de f√∫ria (multiplicador da perda m√©dia)
        limite_furia = perda_media_diaria * multiplicador_furia
        
        # Identificar dias de f√∫ria (perdas maiores que o limite)
        dias_furia = dias_com_perda[abs(dias_com_perda['PnL_Dia']) > limite_furia]
        qtd_dias_furia = int(len(dias_furia))
        
        # Calcular m√©tricas
        total_dias_operados = int(len(resultado_diario))  # Total de dias que teve opera√ß√µes
        percentual_dias_furia = (qtd_dias_furia / total_dias_operados) * 100  # % em rela√ß√£o aos dias operados
        frequencia_furia = (qtd_dias_furia / len(dias_com_perda)) * 100  # Em rela√ß√£o aos dias com perda
        
        furia_diaria = {
            "disponivel": True,
            "dias_com_perda": int(len(dias_com_perda)),
            "perda_media_diaria": round(perda_media_diaria, 2),
            "limite_furia": round(limite_furia, 2),
            "multiplicador_usado": multiplicador_furia,
            "dias_furia": qtd_dias_furia,
            "total_dias_operados": total_dias_operados,
            "percentual_dias_furia": round(percentual_dias_furia, 2),
            "frequencia_furia_vs_dias_perda": round(frequencia_furia, 2),
            "detalhes_furia": [
                {
                    "data": row['Data'].strftime('%d/%m/%Y'),
                    "pnl_dia": round(float(row['PnL_Dia']), 2),
                    "perda_absoluta": round(abs(float(row['PnL_Dia'])), 2),
                    "trades_dia": int(row['Trades_Dia']),
                    "excesso_limite": round(abs(float(row['PnL_Dia'])) - limite_furia, 2),
                    "multiplo_media": round(abs(float(row['PnL_Dia'])) / perda_media_diaria, 2),
                    "pior_trade": round(float(row['Pior_Trade_Dia']), 2),
                    "intensidade": "extrema" if abs(float(row['PnL_Dia'])) > limite_furia * 1.5 else "alta"
                }
                for _, row in dias_furia.iterrows()
            ] if qtd_dias_furia > 0 else [],
            "estatisticas_intensidade": {
                "furia_alta": int(len(dias_furia[abs(dias_furia['PnL_Dia']) <= limite_furia * 1.5])),
                "furia_extrema": int(len(dias_furia[abs(dias_furia['PnL_Dia']) > limite_furia * 1.5])),
                "pior_dia_furia": round(float(dias_furia['PnL_Dia'].min()), 2) if qtd_dias_furia > 0 else 0.0,
                "media_perda_furia": round(float(dias_furia['PnL_Dia'].mean()), 2) if qtd_dias_furia > 0 else 0.0
            }
        }
    
    # ===== PROBABILIDADE DE F√öRIA (SEQUENCIAL) =====
    # Calcular sequ√™ncias de perdas consecutivas
    df_valid['eh_perda'] = df_valid[resultado_col] < 0
    df_valid = df_valid.sort_values(data_col).reset_index(drop=True)
    
    # Identificar sequ√™ncias de perdas
    sequencias_perdas = []
    sequencia_atual = 0
    
    for eh_perda in df_valid['eh_perda']:
        if eh_perda:
            sequencia_atual += 1
        else:
            if sequencia_atual > 0:
                sequencias_perdas.append(sequencia_atual)
                sequencia_atual = 0
    
    # Adicionar √∫ltima sequ√™ncia se terminou em perda
    if sequencia_atual > 0:
        sequencias_perdas.append(sequencia_atual)
    
    if sequencias_perdas:
        maior_sequencia_perdas = max(sequencias_perdas)
        total_sequencias = len(sequencias_perdas)
        media_sequencia_perdas = sum(sequencias_perdas) / len(sequencias_perdas)
        
        # Calcular probabilidade de "f√∫ria" (sequ√™ncia >= 3 perdas)
        sequencias_furia = [s for s in sequencias_perdas if s >= 3]
        qtd_episodios_furia = len(sequencias_furia)
        
        # Probabilidade = epis√≥dios de f√∫ria / total de sequ√™ncias de perda
        if total_sequencias > 0:
            probabilidade_furia = (qtd_episodios_furia / total_sequencias) * 100
        else:
            probabilidade_furia = 0.0
        
        # Calcular frequ√™ncia de f√∫ria por total de trades
        frequencia_furia_trades = (qtd_episodios_furia / total_operacoes) * 100
        
        probabilidade_furia_resultado = {
            "disponivel": True,
            "total_operacoes": total_operacoes,
            "total_operacoes_perdedoras": int(len(df_valid[df_valid['eh_perda']])),
            "total_sequencias_perda": total_sequencias,
            "maior_sequencia_perdas": maior_sequencia_perdas,
            "media_sequencia_perdas": round(media_sequencia_perdas, 2),
            "episodios_furia": qtd_episodios_furia,
            "probabilidade_furia": round(probabilidade_furia, 2),
            "frequencia_furia_por_trades": round(frequencia_furia_trades, 2),
            "detalhes_sequencias": [
                {
                    "sequencia_numero": i + 1,
                    "tamanho_sequencia": seq,
                    "eh_furia": seq >= 3,
                    "classificacao": "f√∫ria" if seq >= 3 else "normal" if seq <= 2 else "moderada"
                }
                for i, seq in enumerate(sequencias_perdas)
            ],
            "estatisticas_sequencias": {
                "sequencias_1_perda": len([s for s in sequencias_perdas if s == 1]),
                "sequencias_2_perdas": len([s for s in sequencias_perdas if s == 2]),
                "sequencias_3_ou_mais": len([s for s in sequencias_perdas if s >= 3]),
                "sequencias_5_ou_mais": len([s for s in sequencias_perdas if s >= 5])
            }
        }
    else:
        probabilidade_furia_resultado = {
            "disponivel": True,
            "total_operacoes": total_operacoes,
            "total_operacoes_perdedoras": 0,
            "total_sequencias_perda": 0,
            "maior_sequencia_perdas": 0,
            "media_sequencia_perdas": 0.0,
            "episodios_furia": 0,
            "probabilidade_furia": 0.0,
            "frequencia_furia_por_trades": 0.0,
            "detalhes_sequencias": [],
            "estatisticas_sequencias": {
                "sequencias_1_perda": 0,
                "sequencias_2_perdas": 0,
                "sequencias_3_ou_mais": 0,
                "sequencias_5_ou_mais": 0
            }
        }
    
    # ===== DISCIPLINA STOP (POR OPERA√á√ÉO) =====
    operacoes_perdedoras = df_valid[df_valid[resultado_col] < 0].copy()
    
    if operacoes_perdedoras.empty:
        disciplina_operacao = {
            "operacoes_perdedoras": 0,
            "media_perda": 0.0,
            "meta_maxima_perda": 0.0,
            "operacoes_excederam_meta": 0,
            "indice_disciplina": 100.0,
            "operacoes_dentro_meta": 0,
            "detalhes_excesso": []
        }
    else:
        # Calcular disciplina por opera√ß√£o
        media_perda = float(operacoes_perdedoras[resultado_col].mean())
        meta_maxima_perda = media_perda + (media_perda * fator_disciplina)
        
        operacoes_excederam = operacoes_perdedoras[operacoes_perdedoras[resultado_col] < meta_maxima_perda]
        num_operacoes_excederam = int(len(operacoes_excederam))
        operacoes_dentro_meta = int(len(operacoes_perdedoras) - num_operacoes_excederam)
        
        indice_disciplina_op = (operacoes_dentro_meta / len(operacoes_perdedoras)) * 100
        
        disciplina_operacao = {
            "operacoes_perdedoras": int(len(operacoes_perdedoras)),
            "media_perda": round(media_perda, 2),
            "meta_maxima_perda": round(meta_maxima_perda, 2),
            "operacoes_excederam_meta": num_operacoes_excederam,
            "indice_disciplina": round(indice_disciplina_op, 2),
            "operacoes_dentro_meta": operacoes_dentro_meta,
            "detalhes_excesso": [
                {
                    "operacao": i + 1,
                    "resultado": round(float(row[resultado_col]), 2),
                    "excesso": round(float(row[resultado_col]) - meta_maxima_perda, 2)
                }
                for i, (_, row) in enumerate(operacoes_excederam.iterrows())
            ] if num_operacoes_excederam > 0 else []
        }
    
    # ===== DISCIPLINA PERDA/DIA (M√âTODO ORIGINAL) =====
    if dias_com_perda.empty:
        disciplina_dia = {
            "dias_com_perda": 0,
            "media_perda_diaria": 0.0,
            "meta_maxima_perda_dia": 0.0,
            "dias_excederam_meta": 0,
            "indice_disciplina_diaria": 100.0,
            "dias_dentro_meta": 0,
            "detalhes_dias_excesso": []
        }
    else:
        # Calcular disciplina por dia
        media_perda_diaria = float(dias_com_perda['PnL_Dia'].mean())
        meta_maxima_perda_dia = media_perda_diaria + (media_perda_diaria * fator_disciplina)
        
        dias_excederam = dias_com_perda[dias_com_perda['PnL_Dia'] < meta_maxima_perda_dia]
        num_dias_excederam = int(len(dias_excederam))
        dias_dentro_meta = int(len(dias_com_perda) - num_dias_excederam)
        
        indice_disciplina_dia = (dias_dentro_meta / len(dias_com_perda)) * 100
        
        disciplina_dia = {
            "dias_com_perda": int(len(dias_com_perda)),
            "media_perda_diaria": round(media_perda_diaria, 2),
            "meta_maxima_perda_dia": round(meta_maxima_perda_dia, 2),
            "dias_excederam_meta": num_dias_excederam,
            "indice_disciplina_diaria": round(indice_disciplina_dia, 2),
            "dias_dentro_meta": dias_dentro_meta,
            "detalhes_dias_excesso": [
                {
                    "data": row['Data'].strftime('%d/%m/%Y'),
                    "pnl_dia": round(float(row['PnL_Dia']), 2),
                    "trades_dia": int(row['Trades_Dia']),
                    "excesso": round(float(row['PnL_Dia']) - meta_maxima_perda_dia, 2),
                    "pior_trade": round(float(row['Pior_Trade_Dia']), 2)
                }
                for _, row in dias_excederam.iterrows()
            ] if num_dias_excederam > 0 else []
        }
    
    # ===== ESTAT√çSTICAS GERAIS =====
    total_dias = int(len(resultado_diario))
    dias_com_ganho = int(len(resultado_diario[resultado_diario['PnL_Dia'] > 0]))
    dias_breakeven = int(len(resultado_diario[resultado_diario['PnL_Dia'] == 0]))
    
    pior_operacao = float(df_valid[resultado_col].min())
    melhor_operacao = float(df_valid[resultado_col].max())
    pior_dia = float(resultado_diario['PnL_Dia'].min())
    melhor_dia = float(resultado_diario['PnL_Dia'].max())
    
    # ===== RESUMO COMPARATIVO =====
    resumo = {
        "disciplina_operacao": disciplina_operacao["indice_disciplina"],
        "disciplina_dia": disciplina_dia["indice_disciplina_diaria"],
        "disciplina_alavancagem": disciplina_alavancagem["indice_disciplina_alavancagem"] if disciplina_alavancagem["disponivel"] else None,
        "probabilidade_furia_sequencial": probabilidade_furia_resultado["probabilidade_furia"],
        "percentual_dias_furia": furia_diaria["percentual_dias_furia"] if furia_diaria["disponivel"] else 0.0,
        "frequencia_furia_diaria": furia_diaria["frequencia_furia_vs_dias_perda"] if furia_diaria["disponivel"] else 0.0,
        "diferenca_operacao_dia": round(disciplina_operacao["indice_disciplina"] - disciplina_dia["indice_disciplina_diaria"], 2),
        "melhor_disciplina": "operacao" if disciplina_operacao["indice_disciplina"] > disciplina_dia["indice_disciplina_diaria"] else "dia",
        "media_perda_operacao": disciplina_operacao["media_perda"],
        "media_perda_dia": disciplina_dia["media_perda_diaria"],
        "limite_furia_diaria": furia_diaria["limite_furia"] if furia_diaria["disponivel"] else None
    }
    
    # Adicionar compara√ß√£o com alavancagem se dispon√≠vel
    if disciplina_alavancagem["disponivel"]:
        resumo["diferenca_operacao_alavancagem"] = round(disciplina_operacao["indice_disciplina"] - disciplina_alavancagem["indice_disciplina_alavancagem"], 2)
        resumo["diferenca_dia_alavancagem"] = round(disciplina_dia["indice_disciplina_diaria"] - disciplina_alavancagem["indice_disciplina_alavancagem"], 2)
        
        # Encontrar a melhor disciplina entre todas
        disciplinas = {
            "operacao": disciplina_operacao["indice_disciplina"],
            "dia": disciplina_dia["indice_disciplina_diaria"],
            "alavancagem": disciplina_alavancagem["indice_disciplina_alavancagem"]
        }
        resumo["melhor_disciplina_geral"] = max(disciplinas, key=disciplinas.get)
        resumo["pior_disciplina_geral"] = min(disciplinas, key=disciplinas.get)
    
    # Adicionar indicadores de risco baseados na f√∫ria
    resumo["risco_emocional_sequencial"] = "alto" if probabilidade_furia_resultado["probabilidade_furia"] > 50 else "medio" if probabilidade_furia_resultado["probabilidade_furia"] > 25 else "baixo"
    resumo["risco_emocional_diario"] = "alto" if furia_diaria["percentual_dias_furia"] > 15 else "medio" if furia_diaria["percentual_dias_furia"] > 5 else "baixo"
    resumo["maior_sequencia_perdas"] = probabilidade_furia_resultado["maior_sequencia_perdas"]
    
    # ===== RESULTADO FINAL =====
    resultado_final = {
        "disciplina_operacao": disciplina_operacao,
        "disciplina_dia": disciplina_dia,
        "disciplina_alavancagem": disciplina_alavancagem,
        "probabilidade_furia_sequencial": probabilidade_furia_resultado,
        "furia_diaria": furia_diaria,
        "estatisticas_gerais": {
            "total_operacoes": total_operacoes,
            "total_dias": total_dias,
            "dias_com_ganho": dias_com_ganho,
            "dias_com_perda": disciplina_dia["dias_com_perda"],
            "dias_breakeven": dias_breakeven,
            "operacoes_ganhadoras": total_operacoes - disciplina_operacao["operacoes_perdedoras"],
            "operacoes_perdedoras": disciplina_operacao["operacoes_perdedoras"],
            "pior_operacao": round(pior_operacao, 2) if total_operacoes > 0 else 0.0,
            "melhor_operacao": round(melhor_operacao, 2) if total_operacoes > 0 else 0.0,
            "pior_dia": round(pior_dia, 2) if total_dias > 0 else 0.0,
            "melhor_dia": round(melhor_dia, 2) if total_dias > 0 else 0.0,
            "media_trades_por_dia": round(total_operacoes / total_dias, 1),
            "fator_disciplina_usado": float(fator_disciplina),
            "multiplicador_furia_usado": float(multiplicador_furia),
            "coluna_quantidade_encontrada": quantidade_col if quantidade_disponivel else None
        },
        "resumo_comparativo": resumo,
        "resultado_diario_completo": [
            {
                "data": row['Data'].strftime('%d/%m/%Y'),
                "pnl_dia": round(float(row['PnL_Dia']), 2),
                "trades_dia": int(row['Trades_Dia']),
                "pior_trade": round(float(row['Pior_Trade_Dia']), 2),
                "status": "ganho" if row['PnL_Dia'] > 0 else "perda" if row['PnL_Dia'] < 0 else "breakeven",
                "dentro_meta": bool(row['PnL_Dia'] >= disciplina_dia["meta_maxima_perda_dia"] if row['PnL_Dia'] < 0 else True),
                "eh_furia": bool(abs(row['PnL_Dia']) > furia_diaria["limite_furia"] if furia_diaria["disponivel"] and row['PnL_Dia'] < 0 else False)
            }
            for _, row in resultado_diario.iterrows()
        ]
    }

    for colname in list(df.columns):
        if str(colname).startswith("_resultado_tmp_") or str(colname).startswith("_data_tmp_"):
            df.drop(columns=[colname], inplace=True, errors='ignore')

    return resultado_final

# ============ API √öNICA SIMPLIFICADA PARA M√öLTIPLOS ARQUIVOS ============

@app.route('/api/disciplina-completa', methods=['POST'])
def api_disciplina_completa():
    """
    Endpoint √öNICO para calcular TODAS as m√©tricas de disciplina
    Suporta tanto um arquivo ('file') quanto m√∫ltiplos arquivos ('files')
    """
    try:
        # Par√¢metros opcionais
        fator_disciplina = float(request.form.get('fator_disciplina', 0.2))
        multiplicador_furia = float(request.form.get('multiplicador_furia', 2.0))
        
        # Lista para armazenar todos os DataFrames
        dataframes = []
        arquivos_processados = []
        
        # Verificar se tem arquivo √∫nico
        if 'file' in request.files:
            arquivo = request.files['file']
            if arquivo.filename != '':
                df = carregar_csv_safe(arquivo)
                dataframes.append(df)
                arquivos_processados.append(arquivo.filename)
        
        # Verificar se tem m√∫ltiplos arquivos
        if 'files' in request.files:
            arquivos = request.files.getlist('files')
            for arquivo in arquivos:
                if arquivo.filename != '':
                    df = carregar_csv_safe(arquivo)
                    dataframes.append(df)
                    arquivos_processados.append(arquivo.filename)
        
        # Verificar se tem caminho de arquivo
        if 'path' in request.form:
            path = request.form['path']
            if not os.path.exists(path):
                return jsonify({"error": "Arquivo n√£o encontrado"}), 404
            df = carregar_csv_safe(path)
            dataframes.append(df)
            arquivos_processados.append(os.path.basename(path))
        
        # Se n√£o tem nenhum arquivo
        if not dataframes:
            return jsonify({"error": "Nenhum arquivo enviado. Use 'file' para um arquivo ou 'files' para m√∫ltiplos"}), 400
        
        # Concatenar todos os DataFrames em um s√≥
        df_consolidado = pd.concat(dataframes, ignore_index=True)
        
        # CORRE√á√ÉO: Normalizar o DataFrame consolidado antes de calcular disciplina
        from FunCalculos import _normalize_trades_dataframe
        if 'entry_date' not in df_consolidado.columns or 'pnl' not in df_consolidado.columns:
            print(f"üîÑ api_disciplina_completa: Normalizando DataFrame consolidado (shape: {df_consolidado.shape})...")
            df_consolidado = _normalize_trades_dataframe(df_consolidado)
            if df_consolidado.empty:
                return jsonify({"error": "Ap√≥s normaliza√ß√£o, o arquivo ficou vazio. Verifique os dados."}), 400
        
        # Calcular disciplina no DataFrame consolidado
        resultado = calcular_disciplina_completa(df_consolidado, fator_disciplina, multiplicador_furia)
        
        if 'error' in resultado:
            return jsonify(resultado), 400
        
        # Adicionar informa√ß√µes sobre os arquivos processados
        resultado['info_arquivos'] = {
            "total_arquivos": len(arquivos_processados),
            "nomes_arquivos": arquivos_processados,
            "total_registros_consolidados": len(df_consolidado)
        }
        
        return jsonify(make_json_serializable(resultado))

    except Exception as e:
        return jsonify({"error": str(e)}), 500
# ============ FUN√á√ÉO AUXILIAR PARA DEBUG ============

def debug_json_serializable(obj, path=""):
    """
    Fun√ß√£o para identificar valores n√£o serializ√°veis em JSON
    """
    import json
    import numpy as np
    
    try:
        if isinstance(obj, dict):
            for key, value in obj.items():
                debug_json_serializable(value, f"{path}.{key}")
        elif isinstance(obj, (list, tuple)):
            for i, value in enumerate(obj):
                debug_json_serializable(value, f"{path}[{i}]")
        else:
            # Tentar serializar o valor individual
            json.dumps(obj)
    except TypeError as e:
        print(f"Erro em {path}: {type(obj)} - {obj}")
        print(f"Erro: {e}")
        
        # Sugerir corre√ß√£o
        if isinstance(obj, np.bool_):
            print(f"Corre√ß√£o: bool({obj})")
        elif isinstance(obj, np.int64):
            print(f"Corre√ß√£o: int({obj})")
        elif isinstance(obj, np.float64):
            print(f"Corre√ß√£o: float({obj})")
        elif hasattr(obj, 'item'):
            print(f"Corre√ß√£o: {obj}.item()")

# ============ FUN√á√ÉO AUXILIAR PARA DEBUG ============


#Rota para receber o CSV e retornar as m√©tricas
@app.route('/api/tabela-multipla', methods=['POST'])
def api_tabela_multipla():
    """
    Endpoint para processar m√∫ltiplos arquivos de backtest
    Garantindo que retorne TODOS os dados incluindo Equity Curve Data
    """
    try:
        # Lista para armazenar todos os DataFrames
        dataframes = []
        arquivos_processados = []
        
        # CORRE√á√ÉO CR√çTICA: Normalizar CADA DataFrame individualmente logo ap√≥s carregar
        # Isso garante que todos tenham entry_date e pnl ANTES de serem processados individualmente
        from FunCalculos import _normalize_trades_dataframe
        
        # Verificar se tem arquivo √∫nico
        if 'file' in request.files:
            arquivo = request.files['file']
            if arquivo.filename != '':
                df = carregar_csv_safe(arquivo)
                # Normalizar imediatamente ap√≥s carregar
                print(f"üîÑ api_tabela_multipla: Normalizando arquivo √∫nico '{arquivo.filename}' ap√≥s carregar...")
                if 'entry_date' not in df.columns or 'pnl' not in df.columns or (hasattr(df, 'entry_date') and df['entry_date'].isna().all() if 'entry_date' in df.columns else False):
                    df = _normalize_trades_dataframe(df)
                    entry_date_valid = df['entry_date'].notna().sum() if 'entry_date' in df.columns else 0
                    print(f"   ‚úÖ Arquivo √∫nico normalizado: entry_date v√°lidos: {entry_date_valid}/{len(df)}")
                dataframes.append(df)
                arquivos_processados.append(arquivo.filename)
        
        # Verificar se tem m√∫ltiplos arquivos
        if 'files' in request.files:
            arquivos = request.files.getlist('files')
            for arquivo in arquivos:
                if arquivo.filename != '':
                    df = carregar_csv_safe(arquivo)
                    # CORRE√á√ÉO CR√çTICA: Normalizar imediatamente ap√≥s carregar CADA arquivo
                    print(f"üîÑ api_tabela_multipla: Normalizando '{arquivo.filename}' ap√≥s carregar...")
                    needs_norm = 'entry_date' not in df.columns or 'pnl' not in df.columns
                    if not needs_norm and 'entry_date' in df.columns:
                        needs_norm = df['entry_date'].isna().all()
                    if needs_norm:
                        df_before = df.copy()
                        df = _normalize_trades_dataframe(df)
                        if df.empty:
                            print(f"   ‚ö†Ô∏è Arquivo '{arquivo.filename}' ficou vazio ap√≥s normaliza√ß√£o (tinha {len(df_before)} linhas)")
                        else:
                            entry_date_valid = df['entry_date'].notna().sum() if 'entry_date' in df.columns else 0
                            pnl_valid = df['pnl'].notna().sum() if 'pnl' in df.columns else 0
                            print(f"   ‚úÖ Arquivo normalizado: entry_date v√°lidos: {entry_date_valid}/{len(df)}, pnl v√°lidos: {pnl_valid}/{len(df)}")
                    dataframes.append(df)
                    arquivos_processados.append(arquivo.filename)
        
        # Verificar se tem caminho de arquivo
        if 'path' in request.form:
            path = request.form['path']
            if not os.path.exists(path):
                return jsonify({"error": "Arquivo n√£o encontrado"}), 404
            df = carregar_csv_safe(path)
            # Normalizar imediatamente ap√≥s carregar
            print(f"üîÑ api_tabela_multipla: Normalizando arquivo por path '{path}' ap√≥s carregar...")
            if 'entry_date' not in df.columns or 'pnl' not in df.columns or (hasattr(df, 'entry_date') and df['entry_date'].isna().all() if 'entry_date' in df.columns else False):
                df = _normalize_trades_dataframe(df)
            dataframes.append(df)
            arquivos_processados.append(os.path.basename(path))
        
        # Se n√£o tem nenhum arquivo
        if not dataframes:
            return jsonify({"error": "Nenhum arquivo enviado. Use 'file' para um arquivo ou 'files' para m√∫ltiplos"}), 400
        
        # CORRE√á√ÉO: Extrair filtros ANTES de processar
        filtros = _parse_filters_from_request(request)
        print(f"üîç Filtros recebidos: {filtros}")
        
        # Par√¢metros opcionais
        capital_inicial = float(request.form.get('capital_inicial', 100000))
        cdi = float(request.form.get('cdi', 0.12))
        
        # CORRE√á√ÉO: Extrair taxas usando fun√ß√£o auxiliar
        taxa_corretagem, taxa_emolumentos = _extrair_taxas_do_request(request)
        
        # Processar cada arquivo individualmente
        resultados_individuais = {}
        print(f"üîç Processando {len(dataframes)} arquivos individualmente:")
        for i, (df, nome_arquivo) in enumerate(zip(dataframes, arquivos_processados)):
            try:
                print(f"  üìÅ Arquivo {i+1}/{len(dataframes)}: {nome_arquivo}")
                print(f"     üìä Registros: {len(df)}")
                print(f"     üìÖ Colunas: {list(df.columns)}")
                
                # CORRE√á√ÉO CR√çTICA: Normalizar o DataFrame SEMPRE, n√£o apenas se faltar
                # Isso garante que sempre tenhamos entry_date, pnl, etc. no formato correto
                from FunCalculos import _normalize_trades_dataframe
                try:
                    print(f"     üîÑ Normalizando DataFrame (shape antes: {df.shape})...")
                    print(f"        Colunas antes: {list(df.columns)[:5]}...")  # Primeiras 5 colunas
                    
                    # SEMPRE normalizar, mesmo se as colunas j√° existem (para garantir formato correto)
                    df_original_len = len(df)
                    df = _normalize_trades_dataframe(df)
                    
                    if df.empty:
                        print(f"     ‚ö†Ô∏è DataFrame vazio ap√≥s normaliza√ß√£o (tinha {df_original_len} linhas antes)")
                        resultados_individuais[nome_arquivo] = {
                            "error": f"Ap√≥s normaliza√ß√£o, o arquivo ficou vazio. Verifique se h√° valores v√°lidos nas colunas 'Abertura' e de resultado (Res. Intervalo, Res. Opera√ß√£o, etc.).",
                            "info_arquivo": {
                                "nome_arquivo": nome_arquivo,
                                "total_registros": df_original_len
                            }
                        }
                        continue
                    
                    # Verificar se entry_date foi criado e tem valores v√°lidos
                    has_entry_date = 'entry_date' in df.columns
                    has_pnl = 'pnl' in df.columns
                    entry_date_valid = df['entry_date'].notna().sum() if has_entry_date else 0
                    pnl_valid = df['pnl'].notna().sum() if has_pnl else 0
                    
                    print(f"     ‚úÖ DataFrame normalizado (shape depois: {df.shape})")
                    print(f"        entry_date existe: {has_entry_date}, v√°lidos: {entry_date_valid}/{len(df)}")
                    print(f"        pnl existe: {has_pnl}, v√°lidos: {pnl_valid}/{len(df)}")
                    print(f"        Colunas depois: {list(df.columns)}")
                    
                    # Se n√£o tem entry_date v√°lido, tentar diagnosticar
                    if not has_entry_date or entry_date_valid == 0:
                        print(f"     ‚ö†Ô∏è AVISO: entry_date n√£o criado ou sem valores v√°lidos!")
                        if 'Abertura' in df.columns:
                            print(f"        Coluna 'Abertura' existe. Primeiros valores:")
                            print(f"        {df['Abertura'].head(3).tolist()}")
                        else:
                            print(f"        Coluna 'Abertura' N√ÉO existe. Colunas dispon√≠veis: {list(df.columns)}")
                    
                except Exception as e:
                    import traceback
                    error_details = traceback.format_exc()
                    print(f"     ‚ùå Erro ao normalizar DataFrame: {e}")
                    print(f"     Detalhes: {error_details}")
                    resultados_individuais[nome_arquivo] = {
                        "error": f"Erro ao normalizar dados: {str(e)}. Verifique se o arquivo est√° no formato correto.",
                        "info_arquivo": {
                            "nome_arquivo": nome_arquivo,
                            "total_registros": len(df) if 'df' in locals() else 0,
                            "colunas_originais": list(df.columns) if 'df' in locals() and not df.empty else []
                        }
                    }
                    continue

                # DEBUG: Verificar padroniza√ß√£o do drawdown (ap√≥s normaliza√ß√£o)
                try:
                    debug_drawdown_calculation(df)
                except Exception as e:
                    print(f"     ‚ö†Ô∏è Erro no debug_drawdown_calculation: {e}")
                    # Continuar mesmo se o debug falhar
                
                # Ap√≥s normaliza√ß√£o, entry_date e pnl j√° devem existir
                # Validar que temos as colunas necess√°rias
                has_entry_date_col = 'entry_date' in df.columns
                has_pnl_col = 'pnl' in df.columns
                entry_date_valid_count = df['entry_date'].notna().sum() if has_entry_date_col else 0
                pnl_valid_count = df['pnl'].notna().sum() if has_pnl_col else 0
                
                # CORRE√á√ÉO: Se entry_date existe mas est√° vazio, tentar recriar a partir de Abertura
                if has_entry_date_col and entry_date_valid_count == 0:
                    print(f"     ‚ö†Ô∏è entry_date existe mas est√° vazio. Tentando recriar a partir de 'Abertura'...")
                    if 'Abertura' in df.columns:
                        try:
                            # Tentar m√∫ltiplos formatos
                            for fmt in ["%d/%m/%Y %H:%M:%S", "%d/%m/%Y %H:%M", "%d/%m/%Y", "%Y-%m-%d %H:%M:%S", "%Y-%m-%d"]:
                                df['entry_date'] = pd.to_datetime(df['Abertura'], format=fmt, errors='coerce')
                                if df['entry_date'].notna().any():
                                    entry_date_valid_count = df['entry_date'].notna().sum()
                                    print(f"     ‚úÖ Recriado entry_date usando formato '{fmt}' ({entry_date_valid_count} valores v√°lidos)")
                                    break
                            # Se ainda n√£o funcionou, tentar detec√ß√£o autom√°tica
                            if entry_date_valid_count == 0:
                                df['entry_date'] = pd.to_datetime(df['Abertura'], errors='coerce', infer_datetime_format=True)
                                entry_date_valid_count = df['entry_date'].notna().sum()
                                if entry_date_valid_count > 0:
                                    print(f"     ‚úÖ Recriado entry_date via detec√ß√£o autom√°tica ({entry_date_valid_count} valores v√°lidos)")
                        except Exception as e:
                            print(f"     ‚ö†Ô∏è Erro ao recriar entry_date: {e}")
                
                # Validar colunas obrigat√≥rias
                if not has_entry_date_col:
                    print(f"     ‚ùå Coluna 'entry_date' n√£o existe ap√≥s normaliza√ß√£o!")
                    print(f"        Colunas dispon√≠veis: {list(df.columns)}")
                    resultados_individuais[nome_arquivo] = {
                        "error": "O arquivo n√£o cont√©m a coluna 'entry_date' obrigat√≥ria. O backend tenta mapear automaticamente a coluna 'Abertura' para 'entry_date', mas isso pode ter falhado. Verifique se: A coluna 'Abertura' cont√©m datas v√°lidas; O formato da data est√° correto; N√£o h√° valores nulos na coluna de data.",
                        "info_arquivo": {
                            "nome_arquivo": nome_arquivo,
                            "total_registros": len(df),
                            "colunas_disponiveis": list(df.columns),
                            "tem_abertura": 'Abertura' in df.columns
                        }
                    }
                    continue
                
                if not has_pnl_col:
                    print(f"     ‚ùå Coluna 'pnl' n√£o existe ap√≥s normaliza√ß√£o!")
                    resultados_individuais[nome_arquivo] = {
                        "error": "Coluna 'pnl' n√£o encontrada ap√≥s normaliza√ß√£o.",
                        "info_arquivo": {
                            "nome_arquivo": nome_arquivo,
                            "total_registros": len(df),
                            "colunas_disponiveis": list(df.columns)
                        }
                    }
                    continue
                
                # CORRE√á√ÉO CR√çTICA: Se entry_date existe mas est√° vazio, tentar UMA √öLTIMA VEZ recriar
                # Isso √© importante porque a normaliza√ß√£o pode ter falhado silenciosamente
                if entry_date_valid_count == 0:
                    print(f"     ‚ö†Ô∏è AVISO: entry_date existe mas est√° vazio (todos NaT). Tentando recriar UMA √öLTIMA VEZ...")
                    
                    # Tentar recriar usando a coluna Abertura original (se ainda existir)
                    if 'Abertura' in df.columns:
                        print(f"     üîÑ Tentativa final: recriando entry_date a partir de 'Abertura'...")
                        try:
                            # Verificar se Abertura j√° √© datetime
                            if pd.api.types.is_datetime64_any_dtype(df['Abertura']):
                                df['entry_date'] = df['Abertura']
                                entry_date_valid_count = df['entry_date'].notna().sum()
                                if entry_date_valid_count > 0:
                                    print(f"     ‚úÖ SUCESSO! entry_date recriado diretamente de 'Abertura' ({entry_date_valid_count} valores v√°lidos)")
                                else:
                                    print(f"     ‚ùå Abertura √© datetime mas est√° vazia")
                            else:
                                # Tentar todos os formatos novamente
                                for fmt in ["%d/%m/%Y %H:%M:%S", "%d/%m/%Y %H:%M", "%d/%m/%Y", "%Y-%m-%d %H:%M:%S", "%Y-%m-%d", "%d-%m-%Y %H:%M:%S", "%d-%m-%Y"]:
                                    df['entry_date'] = pd.to_datetime(df['Abertura'], format=fmt, errors='coerce')
                                    entry_date_valid_count = df['entry_date'].notna().sum()
                                    if entry_date_valid_count > 0:
                                        print(f"     ‚úÖ SUCESSO! entry_date recriado usando formato '{fmt}' ({entry_date_valid_count} valores v√°lidos)")
                                        break
                                
                                # Se ainda n√£o funcionou, tentar detec√ß√£o autom√°tica
                                if entry_date_valid_count == 0:
                                    df['entry_date'] = pd.to_datetime(df['Abertura'], errors='coerce')
                                    entry_date_valid_count = df['entry_date'].notna().sum()
                                    if entry_date_valid_count > 0:
                                        print(f"     ‚úÖ SUCESSO! entry_date recriado via detec√ß√£o autom√°tica ({entry_date_valid_count} valores v√°lidos)")
                        except Exception as e:
                            print(f"     ‚ùå Erro na tentativa final: {e}")
                    
                    # Se ainda n√£o tem valores v√°lidos ap√≥s todas as tentativas, bloquear
                    if entry_date_valid_count == 0:
                        print(f"     ‚ùå Todas as tentativas falharam. entry_date continua vazio.")
                        resultados_individuais[nome_arquivo] = {
                            "error": "O arquivo n√£o cont√©m a coluna 'entry_date' obrigat√≥ria. O backend tenta mapear automaticamente a coluna 'Abertura' para 'entry_date', mas isso pode ter falhado. Verifique se: A coluna 'Abertura' cont√©m datas v√°lidas; O formato da data est√° correto; N√£o h√° valores nulos na coluna de data.",
                            "info_arquivo": {
                                "nome_arquivo": nome_arquivo,
                                "total_registros": len(df),
                                "colunas_disponiveis": list(df.columns),
                                "tem_abertura": 'Abertura' in df.columns,
                                "entry_date_vazio": True,
                                "abertura_sample": df['Abertura'].head(3).tolist() if 'Abertura' in df.columns else None
                            }
                        }
                        continue
                    else:
                        print(f"     ‚úÖ entry_date recriado com sucesso! Continuando processamento...")

                # CORRE√á√ÉO CR√çTICA: Aplicar filtros ANTES de processar
                # Os filtros devem ser aplicados ap√≥s normaliza√ß√£o mas antes de processar
                if filtros:
                    print(f"     üîç Aplicando filtros ao arquivo {nome_arquivo}...")
                    df_antes_filtro = len(df)
                    df = aplicar_filtros_basicos(df, filtros)
                    df = df.reset_index(drop=True)
                    df_depois_filtro = len(df)
                    print(f"     ‚úÖ Filtros aplicados: {df_antes_filtro} -> {df_depois_filtro} registros")
                    
                    # Se ap√≥s filtros o DataFrame ficou vazio, pular este arquivo
                    if df.empty:
                        print(f"     ‚ö†Ô∏è DataFrame ficou vazio ap√≥s aplicar filtros. Pulando arquivo {nome_arquivo}.")
                        resultados_individuais[nome_arquivo] = {
                            "error": "Nenhum registro corresponde aos filtros aplicados.",
                            "info_arquivo": {
                                "nome_arquivo": nome_arquivo,
                                "total_registros_antes_filtro": df_antes_filtro
                            }
                        }
                        continue

                # CORRE√á√ÉO: Passar taxas customizadas para processar_backtest_completo
                # Se taxas foram fornecidas, pass√°-las. Caso contr√°rio, None (c√°lculo autom√°tico)
                resultado_individual = processar_backtest_completo(
                    df, 
                    capital_inicial=capital_inicial, 
                    cdi=cdi,
                    taxa_corretagem=taxa_corretagem,
                    taxa_emolumentos=taxa_emolumentos
                )

                # Garantir compatibilidade de chaves no resultado individual (para o frontend)
                try:
                    # Copiar em camelCase as se√ß√µes principais
                    if 'Day of Week Analysis' in resultado_individual:
                        resultado_individual['day_of_week'] = resultado_individual['Day of Week Analysis']
                    if 'Monthly Analysis' in resultado_individual:
                        resultado_individual['monthly'] = resultado_individual['Monthly Analysis']
                    if 'Equity Curve Data' in resultado_individual:
                        resultado_individual['equity_curve_data'] = resultado_individual['Equity Curve Data']
                    if 'Position Sizing' in resultado_individual:
                        resultado_individual['position_sizing'] = resultado_individual['Position Sizing']
                        resultado_individual['positionSizing'] = resultado_individual['Position Sizing']
                    if 'Trade Duration' in resultado_individual:
                        resultado_individual['trade_duration'] = resultado_individual['Trade Duration']
                        resultado_individual['tradeDuration'] = resultado_individual['Trade Duration']
                    if 'Operational Costs' in resultado_individual:
                        resultado_individual['operational_costs'] = resultado_individual['Operational Costs']
                        resultado_individual['operationalCosts'] = resultado_individual['Operational Costs']
                except Exception as e:
                    print(f"‚ö†Ô∏è DEBUG: Falha ao padronizar chaves camelCase: {e}")
                
                if 'equity_curve_data' not in resultado_individual:
                    print(f"     ‚ö° Gerando equity curve data para {nome_arquivo}")
                    equity_data = gerar_equity_curve_data(df, capital_inicial)
                    resultado_individual['equity_curve_data'] = equity_data
                
                # Processar trades individuais para este arquivo
                print(f"     üìä Processando trades para {nome_arquivo}")
                print(f"        üìã DataFrame shape: {df.shape}")
                print(f"        üìÖ Colunas dispon√≠veis: {list(df.columns)}")
                trades_individual = processar_trades(df, {i: nome_arquivo})
                print(f"        ‚úÖ Trades processados: {len(trades_individual)}")
                resultado_individual['trades'] = trades_individual
                
                resultado_individual['info_arquivo'] = {
                    "nome_arquivo": nome_arquivo,
                    "total_registros": len(df)
                }
                
                resultados_individuais[nome_arquivo] = make_json_serializable(resultado_individual)
                print(f"     ‚úÖ Processado com sucesso: {nome_arquivo}")
                
            except Exception as e:
                error_msg = str(e)
                error_type = type(e).__name__
                
                print(f"‚ùå Erro ao processar arquivo {nome_arquivo}: {error_type} - {error_msg}")
                
                # CORRE√á√ÉO: Mensagem de erro mais espec√≠fica baseada no tipo de erro
                if 'entry_date' in error_msg.lower() or "'entry_date'" in error_msg:
                    # Tentar normalizar novamente para diagn√≥stico
                    try:
                        from FunCalculos import _normalize_trades_dataframe
                        df_test = _normalize_trades_dataframe(df.copy())
                        has_entry_date = 'entry_date' in df_test.columns
                        has_pnl = 'pnl' in df_test.columns
                        entry_date_valid = df_test['entry_date'].notna().sum() if has_entry_date else 0
                        pnl_valid = df_test['pnl'].notna().sum() if has_pnl else 0
                        
                        error_msg = (
                            f"Coluna 'entry_date' n√£o encontrada ou inv√°lida. "
                            f"entry_date existe: {has_entry_date}, v√°lidos: {entry_date_valid}, "
                            f"pnl existe: {has_pnl}, v√°lidos: {pnl_valid}. "
                            f"Verifique se o arquivo cont√©m a coluna 'Abertura' com datas v√°lidas."
                        )
                    except Exception as diag_error:
                        error_msg = f"Erro ao processar arquivo: {error_msg}. Diagn√≥stico adicional falhou: {diag_error}"
                
                resultados_individuais[nome_arquivo] = {
                    "error": error_msg,
                    "error_type": error_type,
                    "info_arquivo": {
                        "nome_arquivo": nome_arquivo,
                        "total_registros": len(df) if 'df' in locals() else 0,
                        "colunas_disponiveis": list(df.columns) if 'df' in locals() and not df.empty else []
                    }
                }
        
        print(f"üìã Resultados individuais processados: {list(resultados_individuais.keys())}")
        
        # Concatenar todos os DataFrames em um s√≥ para an√°lise consolidada
        print(f"üîó Processando dados consolidados:")
        print(f"   üìä Total de registros consolidados: {sum(len(df) for df in dataframes)}")
        
        df_consolidado = pd.concat(dataframes, ignore_index=True)
        print(f"   üìã DataFrame consolidado criado com {len(df_consolidado)} registros")
        print(f"   üìÖ Colunas consolidadas ANTES da normaliza√ß√£o: {list(df_consolidado.columns)[:10]}...")
        
        # CORRE√á√ÉO CR√çTICA: Normalizar o DataFrame consolidado SEMPRE, n√£o apenas se faltar
        # Quando concatenamos DataFrames, eles podem ter colunas diferentes (um tem 'Abertura', outro tem 'entry_date')
        # A normaliza√ß√£o garante que todos tenham as mesmas colunas padronizadas
        from FunCalculos import _normalize_trades_dataframe
        print(f"   üîÑ Normalizando DataFrame consolidado SEMPRE (shape: {df_consolidado.shape})...")
        print(f"      Colunas antes: {list(df_consolidado.columns)[:15]}...")
        
        df_consolidado = _normalize_trades_dataframe(df_consolidado)
        
        if df_consolidado.empty:
            print(f"   ‚ö†Ô∏è DataFrame consolidado ficou vazio ap√≥s normaliza√ß√£o")
        else:
            entry_date_valid = df_consolidado['entry_date'].notna().sum() if 'entry_date' in df_consolidado.columns else 0
            pnl_valid = df_consolidado['pnl'].notna().sum() if 'pnl' in df_consolidado.columns else 0
            print(f"   ‚úÖ DataFrame consolidado normalizado: entry_date v√°lidos: {entry_date_valid}/{len(df_consolidado)}, pnl v√°lidos: {pnl_valid}/{len(df_consolidado)}")
            print(f"      Colunas depois: {list(df_consolidado.columns)[:15]}...")
        
        # CORRE√á√ÉO CR√çTICA: Aplicar filtros no DataFrame consolidado ANTES de processar
        if filtros:
            print(f"   üîç Aplicando filtros ao DataFrame consolidado...")
            df_consolidado_antes = len(df_consolidado)
            df_consolidado = aplicar_filtros_basicos(df_consolidado, filtros)
            df_consolidado = df_consolidado.reset_index(drop=True)
            df_consolidado_depois = len(df_consolidado)
            print(f"   ‚úÖ Filtros aplicados ao consolidado: {df_consolidado_antes} -> {df_consolidado_depois} registros")
        
        # CORRE√á√ÉO: Passar taxas customizadas tamb√©m para o consolidado
        resultado_consolidado = processar_backtest_completo(
            df_consolidado, 
            capital_inicial=capital_inicial, 
            cdi=cdi,
            taxa_corretagem=taxa_corretagem,
            taxa_emolumentos=taxa_emolumentos
        )
        # Padronizar chaves tamb√©m no consolidado
        try:
            if 'Day of Week Analysis' in resultado_consolidado:
                resultado_consolidado['day_of_week'] = resultado_consolidado['Day of Week Analysis']
            if 'Monthly Analysis' in resultado_consolidado:
                resultado_consolidado['monthly'] = resultado_consolidado['Monthly Analysis']
            if 'Equity Curve Data' in resultado_consolidado:
                resultado_consolidado['equity_curve_data'] = resultado_consolidado['Equity Curve Data']
            if 'Position Sizing' in resultado_consolidado:
                resultado_consolidado['position_sizing'] = resultado_consolidado['Position Sizing']
                resultado_consolidado['positionSizing'] = resultado_consolidado['Position Sizing']
            if 'Trade Duration' in resultado_consolidado:
                resultado_consolidado['trade_duration'] = resultado_consolidado['Trade Duration']
                resultado_consolidado['tradeDuration'] = resultado_consolidado['Trade Duration']
            if 'Operational Costs' in resultado_consolidado:
                resultado_consolidado['operational_costs'] = resultado_consolidado['Operational Costs']
                resultado_consolidado['operationalCosts'] = resultado_consolidado['Operational Costs']
        except Exception as e:
            print(f"‚ö†Ô∏è DEBUG: Falha ao padronizar chaves no consolidado: {e}")
        if 'equity_curve_data' not in resultado_consolidado:
            print(f"   ‚ö° Gerando equity curve data consolidada")
            equity_data = gerar_equity_curve_data(df_consolidado, capital_inicial)
            resultado_consolidado['equity_curve_data'] = equity_data
        
        # Processar trades consolidados
        print(f"   üìä Processando trades consolidados")
        arquivo_para_indices = {}
        for i, nome_arquivo in enumerate(arquivos_processados):
            arquivo_para_indices[i] = nome_arquivo
        trades_consolidados = processar_trades(df_consolidado, arquivo_para_indices)
        resultado_consolidado['trades'] = trades_consolidados
        
        resultado_consolidado['info_arquivos'] = {
            "total_arquivos": len(arquivos_processados),
            "nomes_arquivos": arquivos_processados,
            "total_registros_consolidados": len(df_consolidado)
        }
        print(f"   ‚úÖ Dados consolidados processados com sucesso")
        
        # Adicionar an√°lises complementares ao consolidado
        if len(arquivos_processados) > 1:
            resultado_consolidado['day_of_week'] = calcular_day_of_week(df_consolidado)
            resultado_consolidado['monthly'] = calcular_monthly(df_consolidado)
        
        # Retornar estrutura com dados individuais e consolidados
        resultado_final = {
            "consolidado": make_json_serializable(resultado_consolidado),
            "individuais": resultados_individuais,
            "info_geral": {
                "total_arquivos": len(arquivos_processados),
                "nomes_arquivos": arquivos_processados,
                "modo_analise": "individual_e_consolidado"
            }
        }
        
        print(f"üéØ Resposta final preparada:")
        print(f"   üìä Arquivos individuais: {len(resultados_individuais)}")
        print(f"   üîó Dados consolidados: ‚úÖ")
        print(f"   üìã Estrutura: {list(resultado_final.keys())}")
        
        return jsonify(resultado_final)

    except Exception as e:
        return jsonify({"error": str(e)}), 500

def gerar_equity_curve_data(df, capital_inicial=100000):
    """
    Fun√ß√£o auxiliar para garantir que os dados da equity curve sejam gerados
    PADRONIZADO: Usa exatamente a mesma l√≥gica do FunCalculos.py
    """
    try:
        # CORRE√á√ÉO: Normalizar o DataFrame se necess√°rio
        from FunCalculos import _normalize_trades_dataframe
        if 'entry_date' not in df.columns or 'pnl' not in df.columns:
            df = _normalize_trades_dataframe(df)
            if df.empty:
                return []
        
        # Encontrar coluna de resultado (j√° deve estar normalizada como 'pnl')
        resultado_col = 'pnl' if 'pnl' in df.columns else None
        data_col = 'entry_date' if 'entry_date' in df.columns else None
        
        # Fallback para outras colunas se normaliza√ß√£o falhou
        if resultado_col is None:
            for col_name in ['operation_result', 'resultado', 'Res. Intervalo', 'Res. Opera√ß√£o']:
                if col_name in df.columns:
                    resultado_col = col_name
                    break
        
        if data_col is None:
            for col_name in ['data_abertura', 'Abertura', 'data']:
                if col_name in df.columns:
                    data_col = col_name
                    break
        
        if resultado_col is None or data_col is None:
            return []
        
        # Filtrar dados v√°lidos
        df_valid = df.dropna(subset=[resultado_col, data_col]).copy()
        
        if df_valid.empty:
            return []
        
        # Converter data se necess√°rio
        if not pd.api.types.is_datetime64_any_dtype(df_valid[data_col]):
            df_valid[data_col] = pd.to_datetime(df_valid[data_col])
        
        # Ordenar por data
        df_valid = df_valid.sort_values(data_col).reset_index(drop=True)
        
        # PADRONIZADO: Usar exatamente a mesma l√≥gica do FunCalculos.py
        # Calcular equity curve trade por trade (PADRONIZADO: apenas saldo cumulativo)
        df_valid['Saldo'] = df_valid[resultado_col].cumsum()
        df_valid['Saldo_Maximo'] = df_valid['Saldo'].cummax()
        df_valid['Drawdown'] = df_valid['Saldo'] - df_valid['Saldo_Maximo']
        
        # Calcular valor da carteira (para compatibilidade, mas n√£o usado no drawdown)
        df_valid['Valor_Carteira'] = capital_inicial + df_valid['Saldo']
        df_valid['Peak_Carteira'] = capital_inicial + df_valid['Saldo_Maximo']
        
        # PADRONIZADO: Drawdown baseado apenas no saldo cumulativo (sem capital inicial)
        df_valid['Drawdown_Carteira'] = df_valid['Drawdown']  # Usar o mesmo drawdown do saldo
        df_valid['Drawdown_Percentual'] = (df_valid['Drawdown'] / df_valid['Saldo_Maximo'] * 100).fillna(0) if df_valid['Saldo_Maximo'].max() != 0 else 0
        
        # Preparar dados para o gr√°fico (igual ao FunCalculos.py)
        equity_curve = []
        
        # Ponto inicial
        equity_curve.append({
            "date": df_valid[data_col].iloc[0].strftime('%Y-%m-%d'),
            "fullDate": df_valid[data_col].iloc[0].strftime('%d/%m/%Y'),
            "saldo": 0.0,  # Saldo inicial sempre 0
            "valor": float(capital_inicial),  # Patrim√¥nio inicial
            "resultado": 0.0,  # Resultado inicial sempre 0
            "drawdown": 0.0,
            "drawdownPercent": 0.0,
            "peak": float(capital_inicial),
            "trades": 0,
            "isStart": True
        })
        
        # Dados para cada trade (igual ao FunCalculos.py)
        for i, row in df_valid.iterrows():
            equity_curve.append({
                "date": row[data_col].strftime('%Y-%m-%d'),
                "fullDate": row[data_col].strftime('%d/%m/%Y %H:%M'),
                "saldo": float(row['Saldo']),  # ESTE √© o valor que voc√™ quer mostrar
                "valor": float(row['Valor_Carteira']),  # Patrim√¥nio total (saldo + capital)
                "resultado": float(row['Saldo']),  # Mant√©m compatibilidade
                "drawdown": float(abs(row['Drawdown_Carteira'])),  # Sempre positivo
                "drawdownPercent": float(abs(row['Drawdown_Percentual'])),
                "peak": float(row['Peak_Carteira']),
                "trades": int(i + 1),
                "trade_result": float(row[resultado_col]),  # Incluir mesmo se for 0
                "isStart": False
            })
        
        return equity_curve
        
    except Exception as e:
        print(f"Erro ao gerar equity curve data: {e}")
        return []

@app.route('/api/tabela', methods=['POST'])
def api_tabela():
    """
    Endpoint para processar arquivo √∫nico de backtest
    Suporta tanto arquivo √∫nico quanto m√∫ltiplos arquivos
    """
    print("üîç DEBUG: api_tabela chamada!")
    print(f"üîç DEBUG: request.files: {list(request.files.keys())}")
    print(f"üîç DEBUG: request.form: {list(request.form.keys())}")
    
    try:
        # Lista para armazenar todos os DataFrames
        dataframes = []
        arquivos_processados = []
        
        # Verificar se tem arquivo √∫nico
        if 'file' in request.files:
            arquivo = request.files['file']
            print(f"üîç DEBUG: Arquivo recebido: {arquivo.filename}")
            print(f"üîç DEBUG: Tipo do arquivo: {type(arquivo)}")
            if arquivo.filename != '':
                try:
                    df = carregar_csv_safe(arquivo)
                    dataframes.append(df)
                    arquivos_processados.append(arquivo.filename)
                    print(f"üîç DEBUG: Arquivo processado com sucesso")
                except Exception as e:
                    print(f"üîç DEBUG: Erro ao processar arquivo: {e}")
                    raise e
        
        # Verificar se tem m√∫ltiplos arquivos
        if 'files' in request.files:
            arquivos = request.files.getlist('files')
            for arquivo in arquivos:
                if arquivo.filename != '':
                    df = carregar_csv_safe(arquivo)
                    dataframes.append(df)
                    arquivos_processados.append(arquivo.filename)
        
        # Verificar se tem caminho de arquivo
        if 'path' in request.form:
            path = request.form['path']
            if not os.path.exists(path):
                return jsonify({"error": "Arquivo n√£o encontrado"}), 404
            df = carregar_csv_safe(path)
            dataframes.append(df)
            arquivos_processados.append(os.path.basename(path))
        
        # Se n√£o tem nenhum arquivo
        if not dataframes:
            return jsonify({"error": "Envie um arquivo ou caminho via POST"}), 400
        
        print(f"üîç DEBUG: dataframes encontrados: {len(dataframes)}")
        for i, df in enumerate(dataframes):
            print(f"üîç DEBUG: DataFrame {i}: shape={df.shape}, columns={df.columns.tolist()}")
        
        # CORRE√á√ÉO CR√çTICA: Normalizar CADA DataFrame individualmente antes de concatenar
        # Isso garante que todos tenham entry_date e pnl no formato correto
        from FunCalculos import _normalize_trades_dataframe
        dataframes_normalizados = []
        for i, df in enumerate(dataframes):
            print(f"üîÑ api_tabela: Normalizando DataFrame {i+1}/{len(dataframes)}...")
            df_original_len = len(df)
            df_normalized = _normalize_trades_dataframe(df)
            if df_normalized.empty:
                print(f"   ‚ö†Ô∏è DataFrame {i+1} ficou vazio ap√≥s normaliza√ß√£o (tinha {df_original_len} linhas)")
            else:
                entry_date_valid = df_normalized['entry_date'].notna().sum() if 'entry_date' in df_normalized.columns else 0
                pnl_valid = df_normalized['pnl'].notna().sum() if 'pnl' in df_normalized.columns else 0
                print(f"   ‚úÖ DataFrame {i+1} normalizado: entry_date v√°lidos: {entry_date_valid}/{len(df_normalized)}, pnl v√°lidos: {pnl_valid}/{len(df_normalized)}")
            dataframes_normalizados.append(df_normalized)
        
        # Concatenar todos os DataFrames normalizados em um s√≥
        df_consolidado = pd.concat(dataframes_normalizados, ignore_index=True)
        
        # Validar que temos dados ap√≥s normaliza√ß√£o
        if df_consolidado.empty:
            return jsonify({"error": "Ap√≥s normaliza√ß√£o, todos os arquivos ficaram vazios. Verifique se h√° dados v√°lidos nas colunas 'Abertura' e de resultado."}), 400
        
        # Validar colunas obrigat√≥rias
        if 'entry_date' not in df_consolidado.columns:
            return jsonify({"error": "N√£o foi poss√≠vel criar coluna 'entry_date'. Verifique se o arquivo cont√©m a coluna 'Abertura' com datas v√°lidas."}), 400
        
        if 'pnl' not in df_consolidado.columns:
            return jsonify({"error": "N√£o foi poss√≠vel criar coluna 'pnl'. Verifique se o arquivo cont√©m coluna de resultado (Res. Intervalo, Res. Opera√ß√£o, etc.)."}), 400
        
        # CORRE√á√ÉO: Aplicar filtros de per√≠odo personalizado
        filtros = _parse_filters_from_request(request)
        if filtros:
            df_consolidado = aplicar_filtros_basicos(df_consolidado, filtros)
            df_consolidado = df_consolidado.reset_index(drop=True)
        
        # Par√¢metros opcionais
        capital_inicial = float(request.form.get('capital_inicial', 100000))
        cdi = float(request.form.get('cdi', 0.12))
        
        # Usar processar_backtest_completo
        print(f"üîç DEBUG: DataFrame shape: {df_consolidado.shape}")
        print(f"üîç DEBUG: DataFrame columns: {df_consolidado.columns.tolist()}")
        print(f"üîç DEBUG: Primeiras linhas: {df_consolidado.head()}")
        
        resultado = processar_backtest_completo(df_consolidado, capital_inicial=capital_inicial, cdi=cdi)
        
        print(f"üîç DEBUG: Resultado keys: {resultado.keys()}")
        if 'Performance Metrics' in resultado:
            print(f"üîç DEBUG: Performance Metrics: {resultado['Performance Metrics']}")
        else:
            print("üîç DEBUG: Performance Metrics n√£o encontrado")

        # Padronizar chaves adicionais
        if 'Position Sizing' in resultado:
            resultado['position_sizing'] = resultado['Position Sizing']
            resultado['positionSizing'] = resultado['Position Sizing']
        if 'Trade Duration' in resultado:
            resultado['trade_duration'] = resultado['Trade Duration']
            resultado['tradeDuration'] = resultado['Trade Duration']
        if 'Operational Costs' in resultado:
            resultado['operational_costs'] = resultado['Operational Costs']
            resultado['operationalCosts'] = resultado['Operational Costs']
        
        # Verificar se equity_curve_data existe, se n√£o, gerar
        if 'equity_curve_data' not in resultado:
            equity_data = gerar_equity_curve_data(df_consolidado, capital_inicial)
            resultado['equity_curve_data'] = equity_data
        
        # Adicionar informa√ß√µes dos arquivos se m√∫ltiplos
        if len(arquivos_processados) > 1:
            resultado['info_arquivos'] = {
                "total_arquivos": len(arquivos_processados),
                "nomes_arquivos": arquivos_processados,
                "total_registros_consolidados": len(df_consolidado)
            }

        return jsonify(make_json_serializable(resultado))

    except Exception as e:
        return jsonify({"error": str(e)}), 500
# ============ NOVA ROTA ESPEC√çFICA PARA DADOS DO GR√ÅFICO ============

@app.route('/api/equity-curve', methods=['POST'])
def api_equity_curve():
    """Endpoint espec√≠fico para dados da curva de equity"""
    try:
        if 'file' in request.files:
            df = carregar_csv_safe(request.files['file'])
        elif 'path' in request.form:
            path = request.form['path']
            if not os.path.exists(path):
                return jsonify({"error": "Arquivo n√£o encontrado"}), 404
            df = carregar_csv_safe(path)
        else:
            return jsonify({"error": "Envie um arquivo ou caminho via POST"}), 400

        # CORRE√á√ÉO: Aplicar filtros de per√≠odo personalizado
        filtros = _parse_filters_from_request(request)
        if filtros:
            df = aplicar_filtros_basicos(df, filtros)
            df = df.reset_index(drop=True)

        # Par√¢metros opcionais
        capital_inicial = float(request.form.get('capital_inicial', 100000))
        tipo_agrupamento = request.form.get('tipo', 'daily')  # 'trade', 'daily', 'weekly', 'monthly'
        
        # Importar as fun√ß√µes espec√≠ficas do gr√°fico
        from FunCalculos import calcular_dados_grafico, calcular_dados_grafico_agrupado
        
        # Calcular dados baseado no tipo solicitado
        if tipo_agrupamento == 'trade':
            dados = calcular_dados_grafico(df, capital_inicial)
        else:
            dados = calcular_dados_grafico_agrupado(df, capital_inicial, tipo_agrupamento)
        
        resultado = {
            "equity_curve_data": dados,
            "tipo": tipo_agrupamento,
            "capital_inicial": capital_inicial,
            "total_pontos": len(dados)
        }

        return jsonify(make_json_serializable(resultado))

    except Exception as e:
        return jsonify({"error": str(e)}), 500

# ============ ROTA PARA BACKTEST COMPLETO ============

@app.route('/api/backtest-completo', methods=['POST'])
def api_backtest_completo():
    """Endpoint para backtest completo com todos os dados incluindo gr√°fico"""
    try:
        if 'file' in request.files:
            df = carregar_csv_safe(request.files['file'])
        elif 'path' in request.form:
            path = request.form['path']
            if not os.path.exists(path):
                return jsonify({"error": "Arquivo n√£o encontrado"}), 404
            df = carregar_csv_safe(path)
        else:
            return jsonify({"error": "Envie um arquivo ou caminho via POST"}), 400

        # CORRE√á√ÉO: Aplicar filtros de per√≠odo personalizado
        filtros = _parse_filters_from_request(request)
        if filtros:
            df = aplicar_filtros_basicos(df, filtros)
            df = df.reset_index(drop=True)

        # Par√¢metros opcionais
        capital_inicial = float(request.form.get('capital_inicial', 100000))
        cdi = float(request.form.get('cdi', 0.12))
        
        # CORRE√á√ÉO: Extrair taxas usando fun√ß√£o auxiliar
        taxa_corretagem, taxa_emolumentos = _extrair_taxas_do_request(request)
        
        # Usar a fun√ß√£o completa com taxas customizadas
        resultado = processar_backtest_completo(
            df, 
            capital_inicial=capital_inicial, 
            cdi=cdi,
            taxa_corretagem=taxa_corretagem,
            taxa_emolumentos=taxa_emolumentos
        )

        if 'Position Sizing' in resultado:
            resultado['position_sizing'] = resultado['Position Sizing']
            resultado['positionSizing'] = resultado['Position Sizing']
        if 'Trade Duration' in resultado:
            resultado['trade_duration'] = resultado['Trade Duration']
            resultado['tradeDuration'] = resultado['Trade Duration']
        if 'Operational Costs' in resultado:
            resultado['operational_costs'] = resultado['Operational Costs']
            resultado['operationalCosts'] = resultado['Operational Costs']
        
        # Adicionar metadados √∫teis
        resultado["metadata"] = {
            "total_trades": len(df),
            "capital_inicial": capital_inicial,
            "cdi": cdi,
            "filters": filtros,
            "periodo": {
                "inicio": df['entry_date'].min().isoformat() if not df.empty and 'entry_date' in df.columns else None,
                "fim": df['entry_date'].max().isoformat() if not df.empty and 'entry_date' in df.columns else None
            }
        }

        return jsonify(make_json_serializable(resultado))

    except Exception as e:
        return jsonify({"error": str(e)}), 500

# ============ ROTA PARA TABELA M√öLTIPLA CORRIGIDA ============

@app.route('/api/correlacao', methods=['POST'])
def api_correlacao_data_direcao():
    try:
        arquivos_processados = []
        
        # Verificar se recebeu dados JSON
        if request.is_json:
            data = request.get_json()
            
            # Verificar se tem dados de arquivos no JSON
            if 'arquivo1' in data and 'arquivo2' in data:
                # Processar dados JSON (quando frontend envia dados j√° processados)
                try:
                    # Aqui voc√™ pode processar os dados JSON se necess√°rio
                    # Por enquanto, vamos retornar um erro informativo
                    return jsonify({"error": "API de correla√ß√£o espera arquivos CSV, n√£o dados JSON. Use FormData com arquivos."}), 400
                except Exception as e:
                    return jsonify({"error": f"Erro ao processar dados JSON: {str(e)}"}), 500
        
        # Verificar se recebeu arquivos
        if 'files' not in request.files:
            return jsonify({"error": "Nenhum arquivo enviado. Envie arquivos CSV via FormData."}), 400
        
        files = request.files.getlist('files')
        
        if len(files) < 2:
            return jsonify({"error": "Precisa de pelo menos 2 arquivos"}), 400
        
        # Processar cada arquivo
        for file in files:
            try:
                df = carregar_csv_safe(file)  # Usar fun√ß√£o com encoding seguro
                nome = file.filename.replace('.csv', '').replace('.xlsx', '')
                arquivos_processados.append({
                    'nome': nome,
                    'df': df
                })
            except Exception as e:
                return jsonify({"error": f"Erro ao processar {file.filename}: {str(e)}"}), 500
        
        # Calcular correla√ß√£o por data e dire√ß√£o
        resultado = calcular_correlacao_por_data_e_direcao(arquivos_processados)
        
        return jsonify(make_json_serializable(resultado))
        
    except Exception as e:
        import traceback
        error_details = traceback.format_exc()
        print(f"‚ùå Erro em api_correlacao: {e}")
        print(f"Detalhes: {error_details}")
        return jsonify({"error": f"Erro ao processar correla√ß√£o: {str(e)}"}), 500


@app.route('/api/hourly-results', methods=['POST'])
def api_hourly_results():
    """
    Endpoint para an√°lise de resultados por hora
    """
    try:
        # Verificar se tem arquivo
        if 'file' not in request.files and 'files' not in request.files:
            return jsonify({"error": "Nenhum arquivo enviado"}), 400
        
        dataframes = []
        arquivos_processados = []
        
        # Processar arquivo √∫nico
        if 'file' in request.files:
            arquivo = request.files['file']
            if arquivo.filename != '':
                df = carregar_csv_safe(arquivo)
                dataframes.append(df)
                arquivos_processados.append(arquivo.filename)
        
        # Processar m√∫ltiplos arquivos
        if 'files' in request.files:
            arquivos = request.files.getlist('files')
            for arquivo in arquivos:
                if arquivo.filename != '':
                    df = carregar_csv_safe(arquivo)
                    dataframes.append(df)
                    arquivos_processados.append(arquivo.filename)
        
        if not dataframes:
            return jsonify({"error": "Nenhum arquivo v√°lido encontrado"}), 400
        
        # Concatenar DataFrames
        df_consolidado = pd.concat(dataframes, ignore_index=True)
        
        # CORRE√á√ÉO: Normalizar antes de processar
        from FunCalculos import _normalize_trades_dataframe, _detect_pnl_column
        if 'entry_date' not in df_consolidado.columns or 'pnl' not in df_consolidado.columns:
            df_consolidado = _normalize_trades_dataframe(df_consolidado)
            if df_consolidado.empty:
                return jsonify({"error": "Ap√≥s normaliza√ß√£o, o arquivo ficou vazio."}), 400
        
        # Validar colunas necess√°rias
        if 'entry_date' not in df_consolidado.columns:
            return jsonify({"error": "Coluna obrigat√≥ria 'entry_date' n√£o encontrada"}), 400
        
        # CORRE√á√ÉO: Detectar coluna de PnL corretamente
        pnl_col = _detect_pnl_column(df_consolidado)
        if pnl_col is None:
            return jsonify({"error": "Coluna de PnL n√£o encontrada"}), 400
        
        # CORRE√á√ÉO: Aplicar filtros de per√≠odo personalizado
        filtros = _parse_filters_from_request(request)
        if filtros:
            df_consolidado = aplicar_filtros_basicos(df_consolidado, filtros)
            df_consolidado = df_consolidado.reset_index(drop=True)
        
        # Filtrar dados v√°lidos
        df_valid = df_consolidado.dropna(subset=['entry_date', pnl_col]).copy()
        if df_valid.empty:
            return jsonify({"error": "Nenhum dado v√°lido encontrado ap√≥s filtros"}), 400
        
        # Garantir que entry_date √© datetime
        df_valid['entry_date'] = pd.to_datetime(df_valid['entry_date'], errors='coerce')
        df_valid = df_valid[df_valid['entry_date'].notna()].copy()
        
        if df_valid.empty:
            return jsonify({"error": "Nenhuma data v√°lida encontrada"}), 400
        
        # Extrair hora e minutos da entrada para processar per√≠odos customizados
        df_valid['hour'] = df_valid['entry_date'].dt.hour
        df_valid['minute'] = df_valid['entry_date'].dt.minute
        df_valid['total_minutes'] = df_valid['hour'] * 60 + df_valid['minute']
        
        # Processar per√≠odos customizados se fornecidos
        custom_periods = []
        custom_periods_str = request.form.get('custom_periods')
        if custom_periods_str:
            try:
                import json
                custom_periods = json.loads(custom_periods_str)
            except:
                pass
        
        # Se n√£o h√° per√≠odos customizados, usar per√≠odos padr√£o
        if not custom_periods:
            custom_periods = [
                {"start": "09:00", "end": "11:00", "label": "Abertura"},
                {"start": "11:00", "end": "14:00", "label": "Meio-dia"},
                {"start": "14:00", "end": "17:30", "label": "Tarde"},
                {"start": "17:30", "end": "21:00", "label": "P√≥s-mercado"}
            ]
        
        # Processar cada per√≠odo customizado
        period_results = []
        for period in custom_periods:
            start_time = period.get('start', '09:00')
            end_time = period.get('end', '11:00')
            label = period.get('label', f"{start_time} - {end_time}")
            
            # Converter hor√°rios para minutos
            start_hour, start_min = map(int, start_time.split(':'))
            end_hour, end_min = map(int, end_time.split(':'))
            start_minutes = start_hour * 60 + start_min
            end_minutes = end_hour * 60 + end_min
            
            # Filtrar trades dentro do per√≠odo
            if start_minutes <= end_minutes:
                # Per√≠odo normal (n√£o cruza meia-noite)
                period_trades = df_valid[
                    (df_valid['total_minutes'] >= start_minutes) & 
                    (df_valid['total_minutes'] < end_minutes)
                ]
            else:
                # Per√≠odo que cruza meia-noite
                period_trades = df_valid[
                    (df_valid['total_minutes'] >= start_minutes) | 
                    (df_valid['total_minutes'] < end_minutes)
                ]
            
            # Sempre adicionar o per√≠odo, mesmo que n√£o tenha trades
            total_trades = len(period_trades)
            
            if total_trades > 0:
                # Calcular m√©tricas do per√≠odo quando h√° trades
                total_pnl = float(period_trades[pnl_col].sum())
                winning_trades = period_trades[period_trades[pnl_col] > 0]
                losing_trades = period_trades[period_trades[pnl_col] < 0]
                win_rate = (len(winning_trades) / total_trades * 100) if total_trades > 0 else 0
                
                gross_profit = float(winning_trades[pnl_col].sum()) if len(winning_trades) > 0 else 0.0
                gross_loss = abs(float(losing_trades[pnl_col].sum())) if len(losing_trades) > 0 else 0.0
                profit_factor = (gross_profit / gross_loss) if gross_loss > 0 else (999.99 if gross_profit > 0 else 0.0)
                
                avg_win = round(gross_profit / len(winning_trades), 2) if len(winning_trades) > 0 else 0.0
                avg_loss = round(gross_loss / len(losing_trades), 2) if len(losing_trades) > 0 else 0.0
            else:
                # Per√≠odo sem trades - valores zerados
                total_pnl = 0.0
                win_rate = 0.0
                profit_factor = 0.0
                avg_win = 0.0
                avg_loss = 0.0
            
            # Sempre adicionar o per√≠odo √† lista de resultados
            period_results.append({
                "period": f"{start_time}-{end_time}",
                "label": label,
                "trades": total_trades,
                "pnl_total": round(total_pnl, 2),
                "win_rate": round(win_rate, 1),
                "profit_factor": round(profit_factor, 2),
                "avg_win": avg_win,
                "avg_loss": avg_loss
            })
        
        # Calcular resumo
        total_pnl = sum(r['pnl_total'] for r in period_results)
        # Melhor e pior per√≠odo apenas entre os que t√™m trades
        periods_with_trades = [r for r in period_results if r['trades'] > 0]
        best_period = max(periods_with_trades, key=lambda x: x['pnl_total']) if periods_with_trades else None
        worst_period = min(periods_with_trades, key=lambda x: x['pnl_total']) if periods_with_trades else None
        
        # Retornar no formato esperado pelo frontend
        resultado = {
            "summary": {
                "total_periods": len(period_results),  # Total de per√≠odos configurados (incluindo sem trades)
                "total_pnl": round(total_pnl, 2),
                "best_period": best_period,
                "worst_period": worst_period
            },
            "results": period_results,
            "custom_periods": custom_periods,
            "info_arquivos": {
                "total_arquivos": len(arquivos_processados),
                "nomes_arquivos": arquivos_processados,
                "total_registros": len(df_consolidado),
                "registros_apos_filtros": len(df_valid)
            }
        }
        
        return jsonify(make_json_serializable(resultado))
        
    except Exception as e:
        import traceback
        error_details = traceback.format_exc()
        print(f"‚ùå Erro em api_hourly_results: {e}")
        print(f"Detalhes: {error_details}")
        return jsonify({"error": f"Erro ao processar resultados hor√°rios: {str(e)}"}), 500


# Sistema de eventos em mem√≥ria (pode ser substitu√≠do por banco de dados no futuro)
_events_storage = []
_event_id_counter = 1

@app.route('/api/admin/events', methods=['GET', 'POST', 'PUT', 'DELETE'])
def api_admin_events():
    """
    Endpoint para gerenciar eventos administrativos
    GET: Lista eventos (com filtros opcionais)
    POST: Cria novo evento
    PUT: Atualiza evento existente
    DELETE: Remove evento
    """
    global _events_storage, _event_id_counter
    
    try:
        if request.method == 'GET':
            # Listar eventos com filtros opcionais
            event_type = request.args.get('type')
            status = request.args.get('status')
            date_from = request.args.get('date_from')
            date_to = request.args.get('date_to')
            special_only = request.args.get('special_only', 'false').lower() == 'true'
            event_category = request.args.get('event_category')  # Para eventos especiais
            event_date = request.args.get('event_date')  # Data do evento especial
            
            filtered_events = _events_storage.copy()
            
            # Aplicar filtros
            if event_type:
                filtered_events = [e for e in filtered_events if e.get('type') == event_type]
            
            if status:
                filtered_events = [e for e in filtered_events if e.get('status') == status]
            
            # Filtro para eventos especiais
            if special_only:
                filtered_events = [e for e in filtered_events if e.get('is_special', False)]
            
            # Filtro por categoria de evento especial
            if event_category:
                filtered_events = [
                    e for e in filtered_events 
                    if e.get('is_special', False) and 
                    e.get('special_event', {}).get('event_category') == event_category
                ]
            
            # Filtro por data do evento especial
            if event_date:
                try:
                    event_date_dt = pd.to_datetime(event_date).date()
                    filtered_events = [
                        e for e in filtered_events
                        if e.get('is_special', False) and
                        e.get('special_event', {}).get('event_date') and
                        pd.to_datetime(e.get('special_event', {}).get('event_date')).date() == event_date_dt
                    ]
                except:
                    pass
            
            # Filtro por data de cria√ß√£o
            if date_from:
                try:
                    date_from_dt = pd.to_datetime(date_from)
                    filtered_events = [
                        e for e in filtered_events 
                        if pd.to_datetime(e.get('created_at', '2000-01-01')) >= date_from_dt
                    ]
                except:
                    pass
            
            if date_to:
                try:
                    date_to_dt = pd.to_datetime(date_to)
                    filtered_events = [
                        e for e in filtered_events 
                        if pd.to_datetime(e.get('created_at', '2099-12-31')) <= date_to_dt
                    ]
                except:
                    pass
            
            # Ordenar por data (mais recente primeiro) ou por data do evento especial se for especial
            def sort_key(event):
                if event.get('is_special', False) and event.get('special_event', {}).get('event_date'):
                    try:
                        return pd.to_datetime(event.get('special_event', {}).get('event_date'))
                    except:
                        return pd.to_datetime(event.get('created_at', ''))
                return pd.to_datetime(event.get('created_at', ''))
            
            filtered_events.sort(key=sort_key, reverse=True)
            
            return jsonify({
                "events": filtered_events,
                "total": len(filtered_events),
                "special_count": len([e for e in filtered_events if e.get('is_special', False)]),
                "message": "Eventos listados com sucesso"
            })
        
        elif request.method == 'POST':
            # Criar novo evento
            data = request.get_json() if request.is_json else request.form.to_dict()
            
            # Validar campos obrigat√≥rios
            required_fields = ['title', 'type', 'description']
            missing_fields = [field for field in required_fields if not data.get(field)]
            
            if missing_fields:
                return jsonify({
                    "error": f"Campos obrigat√≥rios faltando: {', '.join(missing_fields)}"
                }), 400
            
            # Verificar se √© evento especial
            is_special = data.get('is_special', False) or data.get('special', False)
            if isinstance(is_special, str):
                is_special = is_special.lower() in ('true', '1', 'yes', 'sim')
            
            # Criar evento base
            new_event = {
                "id": _event_id_counter,
                "title": data.get('title'),
                "type": data.get('type'),  # 'info', 'warning', 'error', 'success', 'maintenance', 'special'
                "description": data.get('description'),
                "status": data.get('status', 'active'),  # 'active', 'resolved', 'archived'
                "priority": data.get('priority', 'medium'),  # 'low', 'medium', 'high', 'critical'
                "created_at": datetime.now().isoformat(),
                "updated_at": datetime.now().isoformat(),
                "created_by": data.get('created_by', 'system'),
                "metadata": data.get('metadata', {}),
                "is_special": is_special
            }
            
            # Adicionar campos espec√≠ficos para eventos especiais
            if is_special:
                # Processar array de datas se fornecido
                dates = data.get('dates', [])
                if dates and isinstance(dates, list) and len(dates) > 0:
                    # Se h√° m√∫ltiplas datas, usar a primeira como event_date e salvar todas em dates
                    new_event["dates"] = dates
                    event_date = dates[0] if dates else (data.get('event_date') or data.get('data_ocorrencia'))
                else:
                    # Se n√£o h√° array, usar event_date ou date
                    event_date = data.get('event_date') or data.get('date') or data.get('data_ocorrencia')
                    new_event["dates"] = [event_date] if event_date else []
                
                new_event["special_event"] = {
                    "event_date": event_date,  # Data do evento especial (primeira data se m√∫ltiplas)
                    "event_category": data.get('event_category') or data.get('categoria', 'market'),  # 'market', 'holiday', 'economic', 'corporate', 'other'
                    "impact": data.get('impact', 'medium'),  # 'low', 'medium', 'high', 'critical'
                    "market_affected": data.get('market_affected', []),  # Lista de mercados afetados
                    "recurring": data.get('recurring', False),  # Se √© evento recorrente (ex: feriados)
                    "recurrence_pattern": data.get('recurrence_pattern'),  # 'yearly', 'monthly', 'weekly', etc.
                    "tags": data.get('tags', []),  # Tags para categoriza√ß√£o
                    "related_events": data.get('related_events', []),  # IDs de eventos relacionados
                    "notes": data.get('notes', '')  # Notas adicionais sobre o evento
                }
            
            _events_storage.append(new_event)
            _event_id_counter += 1
            
            return jsonify({
                "message": "Evento criado com sucesso",
                "event": new_event
            }), 201
        
        elif request.method == 'PUT':
            # Atualizar evento existente
            data = request.get_json() if request.is_json else request.form.to_dict()
            event_id = data.get('id') or request.args.get('id')
            
            if not event_id:
                return jsonify({"error": "ID do evento √© obrigat√≥rio"}), 400
            
            try:
                event_id = int(event_id)
            except:
                return jsonify({"error": "ID do evento inv√°lido"}), 400
            
            # Encontrar evento
            event_index = None
            for i, event in enumerate(_events_storage):
                if event.get('id') == event_id:
                    event_index = i
                    break
            
            if event_index is None:
                return jsonify({"error": "Evento n√£o encontrado"}), 404
            
            # Atualizar campos permitidos
            allowed_fields = ['title', 'type', 'description', 'status', 'priority', 'metadata', 'is_special', 'special']
            for field in allowed_fields:
                if field in data:
                    if field == 'special':
                        _events_storage[event_index]['is_special'] = data[field]
                    else:
                        _events_storage[event_index][field] = data[field]
            
            # Atualizar campos de evento especial se for especial
            if data.get('is_special') or data.get('special') or _events_storage[event_index].get('is_special', False):
                if 'special_event' not in _events_storage[event_index]:
                    _events_storage[event_index]['special_event'] = {}
                
                special_fields = [
                    'event_date', 'data_ocorrencia', 'event_category', 'categoria',
                    'impact', 'market_affected', 'recurring', 'recurrence_pattern',
                    'tags', 'related_events', 'notes'
                ]
                
                for field in special_fields:
                    if field in data:
                        if field == 'data_ocorrencia':
                            _events_storage[event_index]['special_event']['event_date'] = data[field]
                        elif field == 'categoria':
                            _events_storage[event_index]['special_event']['event_category'] = data[field]
                        else:
                            _events_storage[event_index]['special_event'][field] = data[field]
            
            _events_storage[event_index]['updated_at'] = datetime.now().isoformat()
            
            return jsonify({
                "message": "Evento atualizado com sucesso",
                "event": _events_storage[event_index]
            })
        
        elif request.method == 'DELETE':
            # Remover evento
            event_id = request.args.get('id') or (request.get_json() if request.is_json else {}).get('id')
            
            if not event_id:
                return jsonify({"error": "ID do evento √© obrigat√≥rio"}), 400
            
            try:
                event_id = int(event_id)
            except:
                return jsonify({"error": "ID do evento inv√°lido"}), 400
            
            # Encontrar e remover evento
            event_index = None
            for i, event in enumerate(_events_storage):
                if event.get('id') == event_id:
                    event_index = i
                    break
            
            if event_index is None:
                return jsonify({"error": "Evento n√£o encontrado"}), 404
            
            removed_event = _events_storage.pop(event_index)
            
            return jsonify({
                "message": "Evento removido com sucesso",
                "event": removed_event
            })
        
    except Exception as e:
        print(f"‚ùå Erro ao processar eventos: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({"error": f"Erro ao processar eventos: {str(e)}"}), 500


@app.route('/api/admin/events/special', methods=['GET', 'POST'])
def api_admin_events_special():
    """
    Endpoint espec√≠fico para eventos especiais
    GET: Lista apenas eventos especiais (com filtros opcionais)
    POST: Cria novo evento especial
    """
    global _events_storage, _event_id_counter
    
    try:
        if request.method == 'GET':
            # Listar apenas eventos especiais com filtros opcionais
            event_category = request.args.get('event_category')
            event_date_from = request.args.get('event_date_from')
            event_date_to = request.args.get('event_date_to')
            impact = request.args.get('impact')
            recurring = request.args.get('recurring')
            
            # Filtrar apenas eventos especiais
            special_events = [e for e in _events_storage if e.get('is_special', False)]
            
            # Aplicar filtros espec√≠ficos
            if event_category:
                special_events = [
                    e for e in special_events
                    if e.get('special_event', {}).get('event_category') == event_category
                ]
            
            if impact:
                special_events = [
                    e for e in special_events
                    if e.get('special_event', {}).get('impact') == impact
                ]
            
            if recurring:
                recurring_bool = recurring.lower() == 'true'
                special_events = [
                    e for e in special_events
                    if e.get('special_event', {}).get('recurring') == recurring_bool
                ]
            
            # Filtro por data do evento
            if event_date_from:
                try:
                    date_from_dt = pd.to_datetime(event_date_from)
                    special_events = [
                        e for e in special_events
                        if e.get('special_event', {}).get('event_date') and
                        pd.to_datetime(e.get('special_event', {}).get('event_date')) >= date_from_dt
                    ]
                except:
                    pass
            
            if event_date_to:
                try:
                    date_to_dt = pd.to_datetime(event_date_to)
                    special_events = [
                        e for e in special_events
                        if e.get('special_event', {}).get('event_date') and
                        pd.to_datetime(e.get('special_event', {}).get('event_date')) <= date_to_dt
                    ]
                except:
                    pass
            
            # Ordenar por data do evento especial
            def sort_key(event):
                if event.get('special_event', {}).get('event_date'):
                    try:
                        return pd.to_datetime(event.get('special_event', {}).get('event_date'))
                    except:
                        return pd.to_datetime(event.get('created_at', ''))
                return pd.to_datetime(event.get('created_at', ''))
            
            special_events.sort(key=sort_key, reverse=True)
            
            # Normalizar eventos para facilitar uso no frontend
            normalized_events = []
            for event in special_events:
                special_event_data = event.get('special_event', {})
                # Extrair todas as datas do evento (pode estar em dates, date, ou event_date)
                event_dates = event.get('dates', []) or event.get('datas', [])
                if not event_dates:
                    # Se n√£o h√° array de dates, usar event_date como array com um elemento
                    event_date = special_event_data.get('event_date') or event.get('date')
                    event_dates = [event_date] if event_date else []
                
                normalized_event = {
                    "id": event.get('id'),
                    "title": event.get('title'),
                    "name": event.get('title'),  # Alias para compatibilidade
                    "date": event_dates[0] if event_dates else (special_event_data.get('event_date') or event.get('created_at')),
                    "dates": event_dates,  # Array com todas as datas
                    "event_date": special_event_data.get('event_date') or (event_dates[0] if event_dates else None),
                    "data_ocorrencia": special_event_data.get('event_date') or (event_dates[0] if event_dates else None),
                    "impact": special_event_data.get('impact', 'medium'),
                    "description": event.get('description') or special_event_data.get('notes', ''),
                    "descricao": event.get('description') or special_event_data.get('notes', ''),
                    "event_category": special_event_data.get('event_category', 'market'),
                    "categoria": special_event_data.get('event_category', 'market'),
                    "market_affected": special_event_data.get('market_affected', []),
                    "mercados_afetados": special_event_data.get('market_affected', []),
                    "recurring": special_event_data.get('recurring', False),
                    "recorrente": special_event_data.get('recurring', False),
                    "recurrence_pattern": special_event_data.get('recurrence_pattern'),
                    "padrao_recorrencia": special_event_data.get('recurrence_pattern'),
                    "tags": special_event_data.get('tags', []),
                    "etiquetas": special_event_data.get('tags', []),
                    "created_at": event.get('created_at'),
                    "updated_at": event.get('updated_at'),
                    # Manter estrutura original para compatibilidade
                    "special_event": special_event_data,
                    "is_special": True
                }
                normalized_events.append(normalized_event)
            
            # Agrupar por categoria para estat√≠sticas
            categories = {}
            for event in special_events:
                cat = event.get('special_event', {}).get('event_category', 'other')
                categories[cat] = categories.get(cat, 0) + 1
            
            return jsonify({
                "events": normalized_events,
                "total": len(normalized_events),
                "statistics": {
                    "by_category": categories,
                    "by_impact": {
                        impact: len([e for e in special_events if e.get('special_event', {}).get('impact') == impact])
                        for impact in ['low', 'medium', 'high', 'critical']
                    },
                    "recurring_count": len([e for e in special_events if e.get('special_event', {}).get('recurring', False)])
                },
                "message": "Eventos especiais listados com sucesso"
            })
        
        elif request.method == 'POST':
            # Criar novo evento especial
            data = request.get_json() if request.is_json else request.form.to_dict()
            
            # Validar campos obrigat√≥rios para evento especial
            required_fields = ['title', 'description', 'event_date']
            missing_fields = [
        field for field in required_fields 
        if (field == 'event_date' and not data.get(field) and not data.get('data_ocorrencia')) 
        or (field != 'event_date' and not data.get(field))
    ]
            
            if missing_fields:
                return jsonify({
                    "error": f"Campos obrigat√≥rios faltando: {', '.join(missing_fields)}"
                }), 400
            
            # Criar evento especial
            new_event = {
                "id": _event_id_counter,
                "title": data.get('title'),
                "type": data.get('type', 'special'),
                "description": data.get('description'),
                "status": data.get('status', 'active'),
                "priority": data.get('priority', 'medium'),
                "created_at": datetime.now().isoformat(),
                "updated_at": datetime.now().isoformat(),
                "created_by": data.get('created_by', 'system'),
                "metadata": data.get('metadata', {}),
                "is_special": True,
                "special_event": {
                    "event_date": data.get('event_date') or data.get('data_ocorrencia'),
                    "event_category": data.get('event_category') or data.get('categoria', 'market'),
                    "impact": data.get('impact', 'medium'),
                    "market_affected": data.get('market_affected', []),
                    "recurring": data.get('recurring', False),
                    "recurrence_pattern": data.get('recurrence_pattern'),
                    "tags": data.get('tags', []),
                    "related_events": data.get('related_events', []),
                    "notes": data.get('notes', '')
                }
            }
            
            _events_storage.append(new_event)
            _event_id_counter += 1
            
            return jsonify({
                "message": "Evento especial criado com sucesso",
                "event": new_event
            }), 201
        
    except Exception as e:
        print(f"‚ùå Erro ao processar eventos especiais: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({"error": f"Erro ao processar eventos especiais: {str(e)}"}), 500


@app.route('/api/admin/events/upcoming-special', methods=['GET'])
def api_admin_events_upcoming_special():
    """
    Endpoint para listar eventos especiais futuros (pr√≥ximos eventos)
    """
    global _events_storage
    
    try:
        days_ahead = int(request.args.get('days', 30))  # Pr√≥ximos 30 dias por padr√£o
        
        now = datetime.now()
        future_date = now + timedelta(days=days_ahead)
        
        # Filtrar eventos especiais futuros
        upcoming_events = []
        for event in _events_storage:
            if not event.get('is_special', False):
                continue
            
            event_date_str = event.get('special_event', {}).get('event_date')
            if not event_date_str:
                continue
            
            try:
                event_date = pd.to_datetime(event_date_str)
                if now <= event_date <= future_date and event.get('status') == 'active':
                    upcoming_events.append(event)
            except:
                continue
        
        # Ordenar por data do evento
        upcoming_events.sort(key=lambda x: pd.to_datetime(x.get('special_event', {}).get('event_date', '')))
        
        return jsonify({
            "events": upcoming_events,
            "total": len(upcoming_events),
            "days_ahead": days_ahead,
            "message": f"Pr√≥ximos {len(upcoming_events)} eventos especiais nos pr√≥ximos {days_ahead} dias"
        })
        
    except Exception as e:
        print(f"‚ùå Erro ao buscar eventos especiais futuros: {e}")
        return jsonify({"error": f"Erro ao buscar eventos especiais futuros: {str(e)}"}), 500


# ============ SISTEMA DE MAILING ============
import smtplib
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
from email.mime.base import MIMEBase
from email import encoders

def _send_email(to_email: str, subject: str, body: str, html_body: str = None, attachments: List[Dict[str, Any]] = None) -> Dict[str, Any]:
    """
    Fun√ß√£o auxiliar para enviar emails
    Retorna dict com status da opera√ß√£o
    """
    try:
        # Configura√ß√µes de email a partir de vari√°veis de ambiente
        smtp_server = os.getenv('SMTP_SERVER', 'smtp.gmail.com')
        smtp_port = int(os.getenv('SMTP_PORT', '587'))
        smtp_username = os.getenv('SMTP_USERNAME', '')
        smtp_password = os.getenv('SMTP_PASSWORD', '')
        from_email = os.getenv('SMTP_FROM_EMAIL', smtp_username)
        
        # Se n√£o tem configura√ß√µes, retornar erro
        if not smtp_username or not smtp_password:
            return {
                "success": False,
                "error": "Configura√ß√µes de email n√£o encontradas. Configure SMTP_USERNAME e SMTP_PASSWORD nas vari√°veis de ambiente."
            }
        
        # Criar mensagem
        msg = MIMEMultipart('alternative')
        msg['From'] = from_email
        msg['To'] = to_email
        msg['Subject'] = subject
        
        # Adicionar corpo do email
        if html_body:
            part1 = MIMEText(body, 'plain')
            part2 = MIMEText(html_body, 'html')
            msg.attach(part1)
            msg.attach(part2)
        else:
            msg.attach(MIMEText(body, 'plain'))
        
        # Adicionar anexos se houver
        if attachments:
            for attachment in attachments:
                part = MIMEBase('application', 'octet-stream')
                part.set_payload(attachment['content'])
                encoders.encode_base64(part)
                part.add_header(
                    'Content-Disposition',
                    f'attachment; filename= {attachment["filename"]}'
                )
                msg.attach(part)
        
        # Conectar e enviar
        server = smtplib.SMTP(smtp_server, smtp_port)
        server.starttls()
        server.login(smtp_username, smtp_password)
        text = msg.as_string()
        server.sendmail(from_email, to_email, text)
        server.quit()
        
        return {
            "success": True,
            "message": f"Email enviado com sucesso para {to_email}"
        }
    
    except Exception as e:
        print(f"‚ùå Erro ao enviar email: {e}")
        import traceback
        traceback.print_exc()
        return {
            "success": False,
            "error": str(e)
        }

@app.route('/api/mailing/send', methods=['POST'])
def api_mailing_send():
    """
    Endpoint para enviar emails
    POST: Envia email para destinat√°rio(s)
    """
    try:
        data = request.get_json() if request.is_json else request.form.to_dict()
        
        # Validar campos obrigat√≥rios
        to_email = data.get('to') or data.get('email') or data.get('to_email')
        subject = data.get('subject') or data.get('titulo')
        body = data.get('body') or data.get('message') or data.get('mensagem')
        
        if not to_email:
            return jsonify({"error": "Campo 'to' (destinat√°rio) √© obrigat√≥rio"}), 400
        
        if not subject:
            return jsonify({"error": "Campo 'subject' (assunto) √© obrigat√≥rio"}), 400
        
        if not body:
            return jsonify({"error": "Campo 'body' (corpo do email) √© obrigat√≥rio"}), 400
        
        # Processar m√∫ltiplos destinat√°rios (separados por v√≠rgula)
        recipients = [email.strip() for email in str(to_email).split(',')]
        
        # HTML body opcional
        html_body = data.get('html_body') or data.get('html')
        
        # Anexos opcionais (lista de objetos com 'filename' e 'content' em base64)
        attachments = data.get('attachments', [])
        processed_attachments = []
        
        if attachments:
            import base64
            for att in attachments:
                if isinstance(att, dict) and 'filename' in att and 'content' in att:
                    try:
                        # Decodificar base64 se necess√°rio
                        content = att['content']
                        if isinstance(content, str):
                            content = base64.b64decode(content)
                        processed_attachments.append({
                            'filename': att['filename'],
                            'content': content
                        })
                    except Exception as e:
                        print(f"‚ö†Ô∏è Erro ao processar anexo {att.get('filename', 'unknown')}: {e}")
        
        # Enviar email para cada destinat√°rio
        results = []
        for recipient in recipients:
            result = _send_email(
                to_email=recipient,
                subject=subject,
                body=body,
                html_body=html_body,
                attachments=processed_attachments if processed_attachments else None
            )
            results.append({
                "recipient": recipient,
                "success": result.get("success", False),
                "message": result.get("message"),
                "error": result.get("error")
            })
        
        # Verificar se todos foram enviados com sucesso
        all_success = all(r["success"] for r in results)
        
        return jsonify({
            "success": all_success,
            "results": results,
            "total_sent": sum(1 for r in results if r["success"]),
            "total_failed": sum(1 for r in results if not r["success"])
        }), 200 if all_success else 207  # 207 Multi-Status se alguns falharam
    
    except Exception as e:
        print(f"‚ùå Erro ao processar envio de email: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({"error": f"Erro ao processar envio de email: {str(e)}"}), 500

@app.route('/api/mailing/test', methods=['POST'])
def api_mailing_test():
    """
    Endpoint para testar configura√ß√£o de email
    POST: Envia email de teste
    """
    try:
        data = request.get_json() if request.is_json else request.form.to_dict()
        
        # Email de teste (ou usar o fornecido)
        test_email = data.get('email') or data.get('to') or os.getenv('SMTP_TEST_EMAIL', '')
        
        if not test_email:
            return jsonify({
                "error": "Email de teste n√£o fornecido. Envie 'email' no body da requisi√ß√£o ou configure SMTP_TEST_EMAIL nas vari√°veis de ambiente."
            }), 400
        
        # Enviar email de teste
        result = _send_email(
            to_email=test_email,
            subject="Teste de Email - DevHub Trader",
            body="Este √© um email de teste do sistema DevHub Trader.\n\nSe voc√™ recebeu este email, a configura√ß√£o de email est√° funcionando corretamente.",
            html_body="""
            <html>
                <body>
                    <h2>Teste de Email - DevHub Trader</h2>
                    <p>Este √© um email de teste do sistema DevHub Trader.</p>
                    <p>Se voc√™ recebeu este email, a configura√ß√£o de email est√° funcionando corretamente.</p>
                    <hr>
                    <p><small>Enviado automaticamente pelo sistema DevHub Trader</small></p>
                </body>
            </html>
            """
        )
        
        if result.get("success"):
            return jsonify({
                "success": True,
                "message": "Email de teste enviado com sucesso",
                "details": result
            })
        else:
            return jsonify({
                "success": False,
                "error": result.get("error", "Erro desconhecido ao enviar email"),
                "details": result
            }), 500
    
    except Exception as e:
        print(f"‚ùå Erro ao processar teste de email: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({"error": f"Erro ao processar teste de email: {str(e)}"}), 500

@app.route('/api/mailing/config', methods=['GET'])
def api_mailing_config():
    """
    Endpoint para verificar configura√ß√£o de email (sem expor senhas)
    GET: Retorna status da configura√ß√£o de email
    """
    try:
        smtp_server = os.getenv('SMTP_SERVER', 'smtp.gmail.com')
        smtp_port = int(os.getenv('SMTP_PORT', '587'))
        smtp_username = os.getenv('SMTP_USERNAME', '')
        smtp_from_email = os.getenv('SMTP_FROM_EMAIL', '')
        
        # Verificar se est√° configurado (sem expor senha)
        is_configured = bool(smtp_username and os.getenv('SMTP_PASSWORD', ''))
        
        return jsonify({
            "configured": is_configured,
            "smtp_server": smtp_server,
            "smtp_port": smtp_port,
            "smtp_username": smtp_username if smtp_username else None,
            "smtp_from_email": smtp_from_email if smtp_from_email else None,
            "message": "Configura√ß√£o de email verificada (senha n√£o exposta)"
        })
    
    except Exception as e:
        return jsonify({"error": f"Erro ao verificar configura√ß√£o: {str(e)}"}), 500


@app.route('/chat', methods=['POST'])
def chat():
    data = request.get_json(silent=True) or {}
    # Debug leve: registrar headers e presen√ßa de chave (mascarando valor)
    try:
        hdr_key = request.headers.get('x-openai-key') or (request.headers.get('Authorization') or '').replace('Bearer ', '')
        masked = (hdr_key[:6] + '...' + hdr_key[-4:]) if hdr_key else None
        print(f"[/chat] headers received: x-openai-key present={bool(request.headers.get('x-openai-key'))}, auth_present={bool(request.headers.get('Authorization'))}, key={masked}")
        print(f"[/chat] body keys: {list(data.keys())}")
    except Exception:
        pass
    messages = data.get('messages', [])

    try:
        # SDK v1.x requer cliente expl√≠cito quando vari√°vel de ambiente n√£o est√° carregada no processo
        # Prioridades para obter a chave: Header -> Authorization Bearer -> Body -> Env
        api_key = (
            request.headers.get('x-openai-key')
            or (request.headers.get('Authorization') or '').replace('Bearer ', '').strip() or None
            or (data.get('apiKey') if isinstance(data, dict) else None)
            or os.getenv("OPENAI_API_KEY")
        )
        if not api_key:
            return jsonify({"error": "OPENAI_API_KEY n√£o dispon√≠vel. Envie no header 'x-openai-key' ou configure no backend."}), 500
        client = _OpenAIClient(api_key=api_key)
        # usar modelo dispon√≠vel (gpt-4o-mini √© mais acess√≠vel)
        resp = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=messages,
            stream=False
        )
        # extrai role + content
        choice = resp.choices[0].message
        return jsonify({
            "role": choice.role,
            "content": choice.content
        }), 200

    except Exception as e:
        return jsonify({"error": str(e)}), 500

# ============ NOVAS ROTAS PARA TRADES ============

@app.route('/api/trades', methods=['POST'])
def api_trades():
    """Endpoint principal para an√°lise de trades - suporta arquivo √∫nico ou m√∫ltiplos arquivos"""
    try:
        # Obter par√¢metros opcionais
        # CORRE√á√ÉO: Extrair taxas usando fun√ß√£o auxiliar (com defaults)
        taxa_corretagem, taxa_emolumentos = _extrair_taxas_do_request(request)
        if taxa_corretagem is None:
            taxa_corretagem = 0.5  # Default: R$ 0,50 por roda
        if taxa_emolumentos is None:
            taxa_emolumentos = 0.03  # Default: R$ 0,03 por roda
        
        # Lista para armazenar todos os DataFrames
        dataframes = []
        arquivos_processados: List[str] = []
        filtros = _parse_filters_from_request(request)
        
        # Verificar se tem arquivo √∫nico
        if 'file' in request.files:
            arquivo = request.files['file']
            if arquivo.filename != '':
                df = carregar_csv_trades(arquivo)
                df['source_file'] = arquivo.filename
                dataframes.append(df)
                arquivos_processados.append(arquivo.filename)
        
        # Verificar se tem m√∫ltiplos arquivos
        if 'files' in request.files:
            arquivos = request.files.getlist('files')
            for arquivo in arquivos:
                if arquivo.filename != '':
                    df = carregar_csv_trades(arquivo)
                    df['source_file'] = arquivo.filename
                    dataframes.append(df)
                    arquivos_processados.append(arquivo.filename)
        
        # Verificar se tem caminho de arquivo
        if 'path' in request.form:
            path = request.form['path']
            if not os.path.exists(path):
                return jsonify({"error": "Arquivo n√£o encontrado"}), 404
            df = carregar_csv_trades(path)
            df['source_file'] = os.path.basename(path)
            dataframes.append(df)
            arquivos_processados.append(os.path.basename(path))
        
        # Se n√£o tem nenhum arquivo
        if not dataframes:
            return jsonify({"error": "Nenhum arquivo enviado. Use 'file' para um arquivo ou 'files' para m√∫ltiplos"}), 400
        
        # Concatenar todos os DataFrames em um s√≥
        df_consolidado = pd.concat(dataframes, ignore_index=True)
        
        # CORRE√á√ÉO: Normalizar ANTES de aplicar filtros para garantir colunas corretas
        from FunCalculos import _normalize_trades_dataframe
        print(f"üîÑ Normalizando DataFrame consolidado antes de aplicar filtros...")
        df_consolidado = _normalize_trades_dataframe(df_consolidado)
        
        if df_consolidado.empty:
            return jsonify({"error": "Ap√≥s normaliza√ß√£o, todos os arquivos ficaram vazios."}), 400

        # CORRE√á√ÉO: Aplicar filtros AP√ìS normaliza√ß√£o
        if filtros:
            print(f"üîç Aplicando filtros ao DataFrame consolidado (shape antes: {df_consolidado.shape})...")
            df_consolidado = aplicar_filtros_basicos(df_consolidado, filtros)
            print(f"‚úÖ Filtros aplicados (shape depois: {df_consolidado.shape})")

        df_consolidado = df_consolidado.reset_index(drop=True)

        arquivo_para_indices = {}
        if 'source_file' in df_consolidado.columns:
            arquivo_para_indices = {
                idx: df_consolidado.at[idx, 'source_file']
                for idx in df_consolidado.index
            }
        
        # Processar dados consolidados com mapeamento de arquivos
        trades = processar_trades(df_consolidado, arquivo_para_indices)
        estatisticas_gerais = calcular_estatisticas_gerais(df_consolidado)
        estatisticas_por_ativo = calcular_estatisticas_por_ativo(df_consolidado)
        estatisticas_temporais = calcular_estatisticas_temporais(df_consolidado)
        custos = calcular_custos_operacionais(df_consolidado, taxa_corretagem, taxa_emolumentos)
        
        # Extrair listas √∫nicas para filtros
        available_assets = sorted([str(symbol) for symbol in df_consolidado['symbol'].unique() if pd.notna(symbol)])
        # Extrair estrat√©gias √∫nicas dos trades processados
        available_strategies = sorted(list(set([trade['strategy'] for trade in trades if trade['strategy']])))

        trades_por_arquivo = {}
        if 'source_file' in df_consolidado.columns:
            trades_por_arquivo = (
                df_consolidado['source_file']
                .fillna('Desconhecido')
                .value_counts()
                .to_dict()
            )

        resultado = {
            "trades": trades,
            "statistics": {
                "general": estatisticas_gerais,
                "by_asset": estatisticas_por_ativo,
                "temporal": estatisticas_temporais,
                "costs": custos
            },
            "filters": {
                "available_assets": available_assets,
                "available_strategies": available_strategies,
                "current": filtros
            },
            "metadata": {
                "total_records": len(df_consolidado),
                "valid_trades": len(trades),
                "date_range": {
                    "start": df_consolidado['entry_date'].min().isoformat() if df_consolidado['entry_date'].notna().any() else None,
                    "end": df_consolidado['entry_date'].max().isoformat() if df_consolidado['entry_date'].notna().any() else None
                },
                "info_arquivos": {
                    "total_arquivos": len(arquivos_processados),
                    "nomes_arquivos": arquivos_processados,
                    "trades_por_arquivo": trades_por_arquivo,
                    "total_registros_consolidados": len(df_consolidado)
                }
            }
        }

        return jsonify(make_json_serializable(resultado))

    except Exception as e:
        return jsonify({"error": str(e)}), 500
@app.route('/api/trades/summary', methods=['POST'])
def api_trades_summary():
    """Endpoint para obter apenas um resumo das estat√≠sticas"""
    try:
        # Carregar arquivo
        if 'file' in request.files:
            df = carregar_csv_trades(request.files['file'])
        elif 'path' in request.form:
            path = request.form['path']
            if not os.path.exists(path):
                return jsonify({"error": "Arquivo n√£o encontrado"}), 404
            df = carregar_csv_trades(path)
        else:
            return jsonify({"error": "Envie um arquivo ou caminho via POST"}), 400

        filtros = _parse_filters_from_request(request)
        if filtros:
            df = aplicar_filtros_basicos(df, filtros).reset_index(drop=True)

        # Calcular apenas estat√≠sticas essenciais
        estatisticas_gerais = calcular_estatisticas_gerais(df)
        custos = calcular_custos_operacionais(df)
        
        resultado = {
            "summary": estatisticas_gerais,
            "costs": custos,
            "total_records": len(df),
            "filters": filtros
        }

        return jsonify(make_json_serializable(resultado))

    except Exception as e:
        return jsonify({"error": str(e)}), 500

# ============ NOVAS ROTAS PARA M√âTRICAS DI√ÅRIAS ============

@app.route('/api/trades/daily-metrics', methods=['POST'])
def api_daily_metrics():
    """Endpoint para obter m√©tricas di√°rias usando FunCalculos.py"""
    try:
        # Carregar arquivo
        if 'file' in request.files:
            df = carregar_csv_trades(request.files['file'])
        elif 'path' in request.form:
            path = request.form['path']
            if not os.path.exists(path):
                return jsonify({"error": "Arquivo n√£o encontrado"}), 400
            df = carregar_csv_trades(path)
        else:
            return jsonify({"error": "Envie um arquivo ou caminho via POST"}), 400

        filtros = _parse_filters_from_request(request)
        if filtros:
            df = aplicar_filtros_basicos(df, filtros).reset_index(drop=True)

        # Par√¢metros opcionais
        capital_inicial = float(request.form.get('capital_inicial', 100000))
        cdi = float(request.form.get('cdi', 0.12))
        
        # Usar FunCalculos.py para garantir consist√™ncia
        from FunCalculos import processar_backtest_completo
        
        # CORRE√á√ÉO: Extrair taxas usando fun√ß√£o auxiliar
        taxa_corretagem, taxa_emolumentos = _extrair_taxas_do_request(request)
        
        # Processar backtest completo usando FunCalculos.py
        resultado = processar_backtest_completo(
            df, 
            capital_inicial=capital_inicial, 
            cdi=cdi,
            taxa_corretagem=taxa_corretagem,
            taxa_emolumentos=taxa_emolumentos
        )
        
        # Extrair apenas as m√©tricas principais do resultado
        performance_metrics = resultado.get("Performance Metrics", {})
        
        # CORRE√á√ÉO: Extrair custos operacionais corretamente
        operational_costs = resultado.get("Operational Costs", {})
        
        # Garantir que temos um dict v√°lido
        if not isinstance(operational_costs, dict):
            operational_costs = {}
        
        # Extrair corretagem e emolumentos separadamente
        corretagem_total = float(operational_costs.get("corretagem", 0.0))
        emolumentos_total = float(operational_costs.get("emolumentos", 0.0))
        
        # Se n√£o encontrou nos custos operacionais, buscar nas m√©tricas de performance
        if corretagem_total == 0.0:
            corretagem_total = float(performance_metrics.get("Total Brokerage", performance_metrics.get("Corretagem Total", 0.0)))
        if emolumentos_total == 0.0:
            emolumentos_total = float(performance_metrics.get("Total Fees", performance_metrics.get("Emolumentos Totais", 0.0)))
        
        # Garantir que os valores s√£o n√∫meros v√°lidos
        if not isinstance(corretagem_total, (int, float)) or pd.isna(corretagem_total):
            corretagem_total = 0.0
        if not isinstance(emolumentos_total, (int, float)) or pd.isna(emolumentos_total):
            emolumentos_total = 0.0
        
        print(f"üîç api_daily_metrics: Custos operacionais extra√≠dos:")
        print(f"   üìä Operational Costs keys: {list(operational_costs.keys())}")
        print(f"   üíº Corretagem: R$ {corretagem_total:.2f}")
        print(f"   üíº Emolumentos: R$ {emolumentos_total:.2f}")
        print(f"   üíº Total: R$ {corretagem_total + emolumentos_total:.2f}")
        
        # Converter para formato esperado pelo frontend
        metricas_principais = {
            "sharpe_ratio": performance_metrics.get("Sharpe Ratio", 0),
            "fator_recuperacao": performance_metrics.get("Recovery Factor", 0),
            "drawdown_maximo": -performance_metrics.get("Max Drawdown ($)", 0),  # Negativo para compatibilidade
            "drawdown_maximo_pct": performance_metrics.get("Max Drawdown (%)", 0),
            "drawdown_medio": performance_metrics.get("Average Drawdown ($)", 0),  # NOVO: DD M√©dio
            "dias_operados": performance_metrics.get("Active Days", 0),
            "resultado_liquido": performance_metrics.get("Net Profit", 0),
            "fator_lucro": performance_metrics.get("Profit Factor", 0),
            "win_rate": performance_metrics.get("Win Rate (%)", 0),
            "roi": (performance_metrics.get("Net Profit", 0) / capital_inicial * 100) if capital_inicial > 0 else 0,
            "corretagem_total": round(corretagem_total, 2),
            "emolumentos_total": round(emolumentos_total, 2),
            "custo_total_operacional": round(corretagem_total + emolumentos_total, 2),
            # Campos adicionais para compatibilidade
            "drawdown_maximo_padronizado": -performance_metrics.get("Max Drawdown ($)", 0),
            "drawdown_maximo_pct_padronizado": performance_metrics.get("Max Drawdown (%)", 0),
            "max_drawdown_padronizado": performance_metrics.get("Max Drawdown ($)", 0),
            "max_drawdown_pct_padronizado": performance_metrics.get("Max Drawdown (%)", 0),
            "capital_estimado": capital_inicial
        }
        
        # Estrutura de resposta compat√≠vel
        metricas = {
            "metricas_principais": metricas_principais,
            "ganhos_perdas": {
                "ganho_medio_diario": performance_metrics.get("Average Win", 0),
                "perda_media_diaria": performance_metrics.get("Average Loss", 0),
                "payoff_diario": performance_metrics.get("Payoff", 0),
                "ganho_maximo_diario": performance_metrics.get("Max Trade Gain", 0),
                "perda_maxima_diaria": abs(performance_metrics.get("Max Trade Loss", 0))
            },
            "estatisticas_operacao": {
                "media_operacoes_dia": performance_metrics.get("Avg Trades/Active Day", 0),
                "taxa_acerto_diaria": performance_metrics.get("Win Rate (%)", 0),
                "dias_vencedores_perdedores": "N/A",  # N√£o dispon√≠vel no FunCalculos.py
                "dias_perdedores_consecutivos": performance_metrics.get("Max Consecutive Losses", 0),
                "dias_vencedores_consecutivos": performance_metrics.get("Max Consecutive Wins", 0)
            },
            # CORRE√á√ÉO: Adicionar se√ß√£o de custos operacionais separada
            "custos_operacionais": {
                "corretagem": round(corretagem_total, 2),
                "emolumentos": round(emolumentos_total, 2),
                "total": round(corretagem_total + emolumentos_total, 2),
                "taxa_corretagem_aplicada": taxa_corretagem if taxa_corretagem is not None else None,
                "taxa_emolumentos_aplicada": taxa_emolumentos if taxa_emolumentos is not None else None
            }
        }
        
        if not metricas:
            return jsonify({"error": "N√£o foi poss√≠vel calcular m√©tricas"}), 400

        metricas["filters"] = filtros

        return jsonify(make_json_serializable(metricas))

    except Exception as e:
        return jsonify({"error": str(e)}), 500

@app.route('/api/trades/metrics-from-data', methods=['POST'])
def api_metrics_from_data():
    """Endpoint para calcular m√©tricas a partir de dados JSON j√° processados"""
    try:
        print(f"üîç DEBUG: Iniciando /api/trades/metrics-from-data")
        print(f"üîç DEBUG: Content-Type: {request.content_type}")
        print(f"üîç DEBUG: Content-Length: {request.content_length}")
        
        # Verificar se h√° dados no request
        if not request.data:
            print(f"‚ùå DEBUG: Request sem dados")
            return jsonify({"error": "Request sem dados"}), 400
        
        # Tentar obter JSON
        try:
            data = request.get_json()
            print(f"üîç DEBUG: JSON parseado com sucesso")
        except Exception as json_error:
            print(f"‚ùå DEBUG: Erro ao fazer parse do JSON: {json_error}")
            print(f"üîç DEBUG: Dados brutos: {request.data[:500]}...")
            return jsonify({"error": f"Erro ao fazer parse do JSON: {str(json_error)}"}), 400
        
        if not data:
            print(f"‚ùå DEBUG: Data √© None ap√≥s parse")
            return jsonify({"error": "Dados JSON inv√°lidos"}), 400
        
        print(f"üîç DEBUG: Chaves no data: {list(data.keys()) if isinstance(data, dict) else 'N/A'}")
        
        if not isinstance(data, dict) or 'trades' not in data:
            print(f"‚ùå DEBUG: 'trades' n√£o encontrado no data")
            return jsonify({"error": "Dados de trades n√£o fornecidos"}), 400
        
        # Converter trades JSON para DataFrame
        trades_data = data['trades']
        
        if not trades_data:
            print(f"‚ùå DEBUG: Lista de trades vazia")
            return jsonify({"error": "Lista de trades vazia"}), 400
        
        print(f"üîç DEBUG: N√∫mero de trades recebidos: {len(trades_data)}")
        
        # ‚úÖ CORRE√á√ÉO: Criar DataFrame com otimiza√ß√µes
        try:
            df = pd.DataFrame(trades_data)
            print(f"üîç DEBUG: DataFrame criado com {len(df)} linhas e {len(df.columns)} colunas")
            print(f"üîç DEBUG: Colunas: {list(df.columns)}")
        except Exception as df_error:
            print(f"‚ùå DEBUG: Erro ao criar DataFrame: {df_error}")
            return jsonify({"error": f"Erro ao criar DataFrame: {str(df_error)}"}), 400
        
        # ‚úÖ CORRE√á√ÉO: Converter datas com otimiza√ß√µes
        try:
            df['entry_date'] = pd.to_datetime(df['entry_date'])
            df['exit_date'] = pd.to_datetime(df['exit_date'])
            print(f"üîç DEBUG: Datas convertidas com sucesso")
        except Exception as date_error:
            print(f"‚ùå DEBUG: Erro ao converter datas: {date_error}")
            return jsonify({"error": f"Erro ao converter datas: {str(date_error)}"}), 400
        
        # ‚úÖ CORRE√á√ÉO: Garantir que pnl seja num√©rico com otimiza√ß√µes
        try:
            df['pnl'] = pd.to_numeric(df['pnl'], errors='coerce')
            print(f"üîç DEBUG: PnL convertido para num√©rico")
        except Exception as pnl_error:
            print(f"‚ùå DEBUG: Erro ao converter PnL: {pnl_error}")
            return jsonify({"error": f"Erro ao converter PnL: {str(pnl_error)}"}), 400
        
        # ‚úÖ CORRE√á√ÉO: Par√¢metros opcionais com valores padr√£o otimizados
        capital_inicial = float(data.get('capital_inicial', 100000))
        cdi = float(data.get('cdi', 0.12))
        
        print(f"üîç DEBUG: Processando {len(df)} trades")
        print(f"üîç DEBUG: Capital inicial: {capital_inicial}")
        print(f"üîç DEBUG: CDI: {cdi}")
        
        # ‚úÖ CORRE√á√ÉO: Usar FunCalculos.py para garantir consist√™ncia com cache
        try:
            from FunCalculos import processar_backtest_completo
            print(f"üîç DEBUG: FunCalculos importado com sucesso")
        except Exception as import_error:
            print(f"‚ùå DEBUG: Erro ao importar FunCalculos: {import_error}")
            return jsonify({"error": f"Erro ao importar FunCalculos: {str(import_error)}"}), 500
        
        # ‚úÖ CORRE√á√ÉO: Processar backtest completo usando FunCalculos.py com otimiza√ß√µes
        try:
            resultado = processar_backtest_completo(df, capital_inicial=capital_inicial, cdi=cdi)
            print(f"üîç DEBUG: Backtest processado com sucesso")
        except Exception as backtest_error:
            print(f"‚ùå DEBUG: Erro ao processar backtest: {backtest_error}")
            return jsonify({"error": f"Erro ao processar backtest: {str(backtest_error)}"}), 500
        
        # ‚úÖ CORRE√á√ÉO: Extrair apenas as m√©tricas principais do resultado com otimiza√ß√µes
        performance_metrics = resultado.get("Performance Metrics", {})
        
        print(f"üîç DEBUG: Performance Metrics recebidas:")
        for key, value in performance_metrics.items():
            print(f"  {key}: {value}")
        
        # ‚úÖ CORRE√á√ÉO: Converter para formato esperado pelo frontend com otimiza√ß√µes
        metricas_principais = {
            "sharpe_ratio": performance_metrics.get("Sharpe Ratio", 0),
            "fator_recuperacao": performance_metrics.get("Recovery Factor", 0),
            "drawdown_maximo": -performance_metrics.get("Max Drawdown ($)", 0),  # Negativo para compatibilidade
            "drawdown_maximo_pct": performance_metrics.get("Max Drawdown (%)", 0),
            "drawdown_medio": performance_metrics.get("Average Drawdown ($)", 0),  # NOVO: DD M√©dio
            "dias_operados": performance_metrics.get("Active Days", 0),
            "resultado_liquido": performance_metrics.get("Net Profit", 0),
            "fator_lucro": performance_metrics.get("Profit Factor", 0),
            "win_rate": performance_metrics.get("Win Rate (%)", 0),
            "roi": (performance_metrics.get("Net Profit", 0) / capital_inicial * 100) if capital_inicial > 0 else 0,
            # Campos adicionais para compatibilidade
            "drawdown_maximo_padronizado": -performance_metrics.get("Max Drawdown ($)", 0),
            "drawdown_maximo_pct_padronizado": performance_metrics.get("Max Drawdown (%)", 0),
            "max_drawdown_padronizado": performance_metrics.get("Max Drawdown ($)", 0),
            "max_drawdown_pct_padronizado": performance_metrics.get("Max Drawdown (%)", 0),
            "capital_estimado": capital_inicial
        }
        
        print(f"üîç DEBUG: M√©tricas principais mapeadas:")
        for key, value in metricas_principais.items():
            print(f"  {key}: {value}")
        
        # ‚úÖ CORRE√á√ÉO: Estrutura de resposta compat√≠vel com otimiza√ß√µes
        metricas = {
            "metricas_principais": metricas_principais,
            "ganhos_perdas": {
                "ganho_medio_diario": performance_metrics.get("Average Win", 0),
                "perda_media_diaria": performance_metrics.get("Average Loss", 0),
                "payoff_diario": performance_metrics.get("Payoff", 0),
                "ganho_maximo_diario": performance_metrics.get("Max Trade Gain", 0),
                "perda_maxima_diaria": abs(performance_metrics.get("Max Trade Loss", 0))
            },
            "estatisticas_operacao": {
                "media_operacoes_dia": performance_metrics.get("Avg Trades/Active Day", 0),
                "taxa_acerto_diaria": performance_metrics.get("Win Rate (%)", 0),
                "dias_vencedores_perdedores": "N/A",  # N√£o dispon√≠vel no FunCalculos.py
                "dias_perdedores_consecutivos": performance_metrics.get("Max Consecutive Losses", 0),
                "dias_vencedores_consecutivos": performance_metrics.get("Max Consecutive Wins", 0)
            }
        }
        
        print(f"üîç DEBUG: Resposta final preparada")
        print(f"üîç DEBUG: DD M√©dio na resposta: {metricas['metricas_principais']['drawdown_medio']}")
        
        if not metricas:
            print(f"‚ùå DEBUG: M√©tricas vazias")
            return jsonify({"error": "N√£o foi poss√≠vel calcular m√©tricas"}), 400
        
        # ‚úÖ CORRE√á√ÉO: Tentar serializar a resposta com otimiza√ß√µes
        try:
            response_data = make_json_serializable(metricas)
            print(f"üîç DEBUG: Resposta serializada com sucesso")
            return jsonify(response_data)
        except Exception as serialize_error:
            print(f"‚ùå DEBUG: Erro ao serializar resposta: {serialize_error}")
            return jsonify({"error": f"Erro ao serializar resposta: {str(serialize_error)}"}), 500

    except Exception as e:
        print(f"‚ùå Erro na API: {e}")
        import traceback
        print(f"‚ùå Traceback completo:")
        traceback.print_exc()
        return jsonify({"error": str(e)}), 500

@app.route('/api/position-sizing', methods=['POST'])
def api_position_sizing():
    """Endpoint espec√≠fico para calcular m√©tricas de position sizing"""
    try:
        if 'file' not in request.files:
            return jsonify({"error": "Nenhum arquivo enviado"}), 400
        
        file = request.files['file']
        if file.filename == '':
            return jsonify({"error": "Nenhum arquivo selecionado"}), 400
        
        # Carregar CSV
        print(f"üìä Processando arquivo: {file.filename}")
        
        # Carregar CSV com headers corretos
        try:
            df = pd.read_csv(file, skiprows=5, sep=';', encoding='latin1', decimal=',', header=None)
            
            # Definir headers corretos
            expected_headers = [
                'Ativo', 'Abertura', 'Fechamento', 'Tempo Opera√ß√£o', 'Qtd Compra', 'Qtd Venda',
                'Lado', 'Pre√ßo Compra', 'Pre√ßo Venda', 'Pre√ßo de Mercado', 'M√©dio',
                'Res. Intervalo', 'Res. Intervalo (%)', 'N√∫mero Opera√ß√£o', 'Res. Opera√ß√£o', 'Res. Opera√ß√£o (%)',
                'Drawdown', 'Ganho Max.', 'Perda Max.', 'TET', 'Total'
            ]
            
            if len(df.columns) == len(expected_headers):
                df.columns = expected_headers
                print(f"üìä Headers atribu√≠dos corretamente")
            else:
                print(f"‚ö†Ô∏è N√∫mero de colunas ({len(df.columns)}) n√£o corresponde aos headers esperados ({len(expected_headers)})")
                return jsonify({"error": f"Formato de CSV inv√°lido. Esperado {len(expected_headers)} colunas, encontrado {len(df.columns)}"}), 400
            
            # Processar datas com tratamento de NaT
            print(f"üìä Processando datas - DataFrame shape inicial: {df.shape}")
            
            if 'Abertura' in df.columns:
                print(f"üìä Processando coluna 'Abertura'")
                print(f"üìä Amostra de valores 'Abertura': {df['Abertura'].head(3).tolist()}")
                df['Abertura'] = pd.to_datetime(df['Abertura'], format="%d/%m/%Y %H:%M:%S", errors='coerce')
                print(f"üìä Ap√≥s convers√£o - valores NaT: {df['Abertura'].isna().sum()}")
                # Remover linhas com datas inv√°lidas
                df_antes = len(df)
                df = df.dropna(subset=['Abertura'])
                df_depois = len(df)
                print(f"üìä Linhas removidas de 'Abertura': {df_antes - df_depois}")
                
            if 'Fechamento' in df.columns:
                print(f"üìä Processando coluna 'Fechamento'")
                df['Fechamento'] = pd.to_datetime(df['Fechamento'], format="%d/%m/%Y %H:%M:%S", errors='coerce')
                # Remover linhas com datas inv√°lidas
                df_antes = len(df)
                df = df.dropna(subset=['Fechamento'])
                df_depois = len(df)
                print(f"üìä Linhas removidas de 'Fechamento': {df_antes - df_depois}")
            
            print(f"üìä DataFrame ap√≥s processamento de datas: {df.shape}")
            
            # Limpar valores num√©ricos
            numeric_columns = ['Res. Opera√ß√£o', 'Res. Opera√ß√£o (%)', 'Pre√ßo Compra', 'Pre√ßo Venda', 
                              'Pre√ßo de Mercado', 'M√©dio', 'Res. Intervalo', 'Res. Intervalo (%)',
                              'Drawdown', 'Ganho Max.', 'Perda Max.', 'Qtd Compra', 'Qtd Venda']
            
            for col in numeric_columns:
                if col in df.columns:
                    df[col] = df[col].apply(clean_numeric_value)
            
            # Renomear colunas
            column_mapping = {
                'Ativo': 'symbol', 'Abertura': 'entry_date', 'Fechamento': 'exit_date',
                'Tempo Opera√ß√£o': 'duration_str', 'Qtd Compra': 'qty_buy', 'Qtd Venda': 'qty_sell',
                'Lado': 'direction', 'Pre√ßo Compra': 'entry_price', 'Pre√ßo Venda': 'exit_price',
                'Pre√ßo de Mercado': 'market_price', 'M√©dio': 'avg_price', 'Res. Intervalo': 'pnl',
                'Res. Intervalo (%)': 'pnl_pct', 'N√∫mero Opera√ß√£o': 'trade_number',
                'Res. Opera√ß√£o': 'operation_result', 'Res. Opera√ß√£o (%)': 'operation_result_pct',
                'Drawdown': 'drawdown', 'Ganho Max.': 'max_gain', 'Perda Max.': 'max_loss',
                'TET': 'tet', 'Total': 'total'
            }
            df = df.rename(columns=column_mapping)
            
            # Converter dire√ß√£o
            if 'direction' in df.columns:
                df['direction'] = df['direction'].map({'C': 'long', 'V': 'short'}).fillna('long')
            
            # Usar operation_result como pnl
            if 'operation_result' in df.columns:
                df['pnl'] = df['operation_result']
            
            print(f"üìä DataFrame processado - Shape: {df.shape}, Colunas: {list(df.columns)}")
            
        except Exception as e:
            print(f"‚ùå Erro ao processar CSV: {e}")
            return jsonify({"error": f"Erro ao processar CSV: {e}"}), 400
        
        # Processar trades
        trades = processar_trades(df)
        print(f"üìä Trades processados: {len(trades)}")
        
        if not trades:
            print(f"‚ùå Nenhum trade v√°lido encontrado")
            print(f"üìä DataFrame info:")
            print(f"   - Shape: {df.shape}")
            print(f"   - Colunas: {list(df.columns)}")
            print(f"   - Primeiras linhas:")
            if not df.empty:
                print(df.head(3).to_string())
            return jsonify({
                "error": "Nenhum trade v√°lido encontrado",
                "debug": {
                    "dataframe_shape": df.shape,
                    "dataframe_columns": list(df.columns),
                    "sample_data": df.head(3).to_dict('records') if not df.empty else []
                }
            }), 400
        
        print(f"üìä Calculando position sizing para {len(trades)} trades")
        
        # Extrair dados de posi√ß√£o
        position_data = []
        for trade in trades:
            # Tentar diferentes campos de quantidade
            quantity = (trade.get('quantity_total', 0) or 
                       trade.get('quantity_compra', 0) or 
                       trade.get('quantity_venda', 0) or
                       trade.get('qty_buy', 0) or 
                       trade.get('qty_sell', 0) or 0)
            
            if quantity > 0:
                position_data.append({
                    'quantity': quantity,
                    'pnl': trade.get('pnl', 0),
                    'entry_price': trade.get('entry_price', 0),
                    'exit_price': trade.get('exit_price', 0)
                })
        
        print(f"üìä Dados de posi√ß√£o encontrados: {len(position_data)} trades com quantidade")
        
        if not position_data:
            return jsonify({
                "error": "Nenhum dado de posi√ß√£o encontrado nos trades",
                "available_fields": list(trades[0].keys()) if trades else []
            }), 400
        
        # Calcular estat√≠sticas de posi√ß√£o
        quantities = [p['quantity'] for p in position_data]
        max_position = max(quantities) if quantities else 0
        avg_position = sum(quantities) / len(quantities) if quantities else 0
        median_position = sorted(quantities)[len(quantities)//2] if quantities else 0
        
        # Calcular risco por trade (baseado na perda m√©dia)
        losses = [abs(p['pnl']) for p in position_data if p['pnl'] < 0]
        avg_trade_risk = sum(losses) / len(losses) if losses else 0
        
        # Calcular account risk (2% do capital total)
        total_pnl = sum(t['pnl'] for t in trades)
        account_risk = max(0, total_pnl) * 0.02  # 2% rule
        
        # Calcular posi√ß√£o recomendada
        recommended_position = int(account_risk / avg_trade_risk) if avg_trade_risk > 0 else 0
        
        # Determinar tipo de ativo (a√ß√µes vs futuros) com l√≥gica melhorada
        avg_trade_value = abs(sum(t['pnl'] for t in trades) / len(trades))
        
        # L√≥gica melhorada para determinar se √© a√ß√µes ou futuros
        # Se tem posi√ß√µes > 100 ou trade value > 1000, provavelmente √© a√ß√µes
        is_stocks = avg_position > 100 or avg_trade_value > 1000
        
        # Se n√£o tem dados de posi√ß√£o, usar trade value como crit√©rio
        if avg_position == 0:
            is_stocks = avg_trade_value > 500  # Se trade value > 500, provavelmente a√ß√µes
        
        # Calcular dados para AMBOS os tipos de ativo (sempre)
        # Para A√ß√µes - usar dados reais ou estimar baseado no trade value
        stocks_avg_position = avg_position if is_stocks else max(1, int(avg_trade_value * 10))  # Estimativa para a√ß√µes
        stocks_max_position = max_position if is_stocks else stocks_avg_position * 2
        stocks_median_position = median_position if is_stocks else stocks_avg_position
        stocks_recommended = recommended_position if is_stocks else max(1, int(account_risk / (avg_trade_risk * 10)))  # A√ß√µes t√™m menor risco
        
        print(f"üìä An√°lise de tipo de ativo:")
        print(f"   - Posi√ß√£o m√©dia: {avg_position}")
        print(f"   - Trade value m√©dio: {avg_trade_value}")
        print(f"   - Tipo determinado: {'A√ß√µes' if is_stocks else 'Futuros'}")
        print(f"üìä C√°lculos para A√ß√µes:")
        print(f"   - Posi√ß√£o m√©dia estimada: {stocks_avg_position}")
        print(f"   - Posi√ß√£o m√°xima: {stocks_max_position}")
        print(f"   - Posi√ß√£o recomendada: {stocks_recommended}")
        print(f"üìä C√°lculos para Futuros:")
        print(f"   - Posi√ß√£o m√©dia real: {avg_position}")
        print(f"   - Posi√ß√£o m√°xima: {max_position}")
        print(f"   - Posi√ß√£o recomendada: {recommended_position}")
        
        # Calcular posi√ß√µes abertas m√°ximas
        trades_by_date = {}
        for trade in trades:
            # Usar entry_date que j√° foi renomeado de 'Abertura'
            entry_date = trade.get('entry_date', '')
            if entry_date:
                date = entry_date[:10]  # YYYY-MM-DD
                if date not in trades_by_date:
                    trades_by_date[date] = []
                trades_by_date[date].append(trade)
        
        max_open_positions = max(len(trades) for trades in trades_by_date.values()) if trades_by_date else 0
        
        stocks_data = {
            "maxPositionPerTrade": stocks_max_position,
            "avgPositionPerTrade": round(stocks_avg_position),
            "medianPositionPerTrade": stocks_median_position,
            "avgLeverage": 0.85,
            "recommendedPosition": stocks_recommended,
            "riskPerTrade": round(avg_trade_risk * 10, 2)  # A√ß√µes t√™m risco por trade maior
        }
        
        # Para Futuros - usar dados reais
        futures_data = {
            "maxPositionPerTrade": max_position if not is_stocks else max_position,
            "avgPositionPerTrade": round(avg_position) if not is_stocks else avg_position,
            "medianPositionPerTrade": median_position if not is_stocks else median_position,
            "avgLeverage": 3.2,
            "recommendedPosition": recommended_position if not is_stocks else recommended_position,
            "riskPerTrade": round(avg_trade_risk, 2)
        }
        
        # Se n√£o h√° dados de posi√ß√£o, estimar para ambos
        if avg_position == 0:
            # Estimar posi√ß√£o baseada no trade value
            estimated_position = max(1, int(avg_trade_value / 100))
            
            # Para a√ß√µes - estimativa mais conservadora
            stocks_estimated = max(1, int(avg_trade_value * 5))
            stocks_data.update({
                "maxPositionPerTrade": stocks_estimated * 2,
                "avgPositionPerTrade": stocks_estimated,
                "medianPositionPerTrade": stocks_estimated,
                "recommendedPosition": max(1, int(account_risk / (avg_trade_risk * 5)))
            })
            
            # Para futuros - estimativa baseada no trade value
            futures_data.update({
                "maxPositionPerTrade": estimated_position * 2,
                "avgPositionPerTrade": estimated_position,
                "medianPositionPerTrade": estimated_position,
                "recommendedPosition": estimated_position
            })
        
        result = {
            "stocks": stocks_data,
            "futures": futures_data,
            "general": {
                "maxOpenPositions": max_open_positions,
                "setupsMaximosPorDia": max_open_positions,
                "accountRisk": round(account_risk, 2),
                "maxRiskPerTrade": round(account_risk * 0.5, 2)  # 1% rule
            },
            "debug": {
                "totalTrades": len(trades),
                "tradesWithPosition": len(position_data),
                "assetType": "Stocks" if is_stocks else "Futures",
                "avgTradeValue": round(avg_trade_value, 2),
                "avgPosition": round(avg_position, 2),
                "isStocks": is_stocks,
                "hasPositionData": len(position_data) > 0
            }
        }
        
        print(f"üìä Position sizing calculado: {result}")
        return jsonify(result)
        
    except Exception as e:
        print(f"‚ùå Erro em api_position_sizing: {e}")
        return jsonify({"error": str(e)}), 500

def debug_drawdown_calculation(df: pd.DataFrame) -> Dict[str, float]:
    """
    Fun√ß√£o de debug para verificar se todos os c√°lculos de drawdown est√£o padronizados
    """
    if df.empty:
        return {}
    
    # CORRE√á√ÉO: Normalizar o DataFrame se necess√°rio
    from FunCalculos import _normalize_trades_dataframe
    if 'entry_date' not in df.columns or 'pnl' not in df.columns:
        df = _normalize_trades_dataframe(df)
        if df.empty:
            return {}
    
    print("üîç DEBUG - Verifica√ß√£o de padroniza√ß√£o do drawdown:")
    
    # CORRE√á√ÉO: Validar que temos as colunas necess√°rias ANTES de usar
    if 'entry_date' not in df.columns:
        print("‚ùå Coluna 'entry_date' n√£o encontrada ap√≥s normaliza√ß√£o")
        print(f"   Colunas dispon√≠veis: {list(df.columns)}")
        return {}
    
    if 'pnl' not in df.columns:
        print("‚ùå Coluna 'pnl' n√£o encontrada ap√≥s normaliza√ß√£o")
        print(f"   Colunas dispon√≠veis: {list(df.columns)}")
        return {}
    
    # Verificar se h√° valores v√°lidos
    entry_date_valid = df['entry_date'].notna().sum()
    pnl_valid = df['pnl'].notna().sum()
    
    if entry_date_valid == 0:
        print(f"‚ö†Ô∏è Nenhuma data v√°lida encontrada (todas s√£o NaT)")
        print(f"   Tentando continuar sem valida√ß√£o de data...")
    
    if pnl_valid == 0:
        print(f"‚ö†Ô∏è Nenhum PnL v√°lido encontrado")
        return {}
    
    # M√©todo 1: FunCalculos.py (trades individuais)
    # Filtrar apenas linhas que t√™m AMBOS os valores v√°lidos
    df_valid = df[df['entry_date'].notna() & df['pnl'].notna()].copy()
    
    if df_valid.empty:
        print("‚ö†Ô∏è Nenhuma linha com entry_date e pnl v√°lidos")
        return {}
    
    df_valid = df_valid.sort_values('entry_date').reset_index(drop=True)
    
    equity = df_valid['pnl'].cumsum()
    peak = equity.cummax()
    dd_ser = equity - peak
    max_dd_funcalculos = abs(dd_ser.min()) if not dd_ser.empty else 0
    pct_dd_funcalculos = (max_dd_funcalculos / equity.iloc[-1] * 100) if equity.iloc[-1] != 0 else 0
    
    print(f"  FunCalculos.py: R$ {max_dd_funcalculos:.2f} ({pct_dd_funcalculos:.2f}%)")
    
    # M√©todo 2: An√°lise di√°ria (dias consolidados)
    df_valid['date'] = pd.to_datetime(df_valid['entry_date']).dt.date
    daily_stats = df_valid.groupby('date').agg({
        'pnl': ['sum', 'count', 'mean'],
    }).round(2)
    
    daily_stats.columns = ['total_pnl', 'total_trades', 'avg_pnl']
    daily_stats['cumulative_pnl'] = daily_stats['total_pnl'].cumsum()
    daily_stats['running_max'] = daily_stats['cumulative_pnl'].expanding().max()
    daily_stats['drawdown'] = daily_stats['cumulative_pnl'] - daily_stats['running_max']
    
    max_dd_daily = abs(daily_stats['drawdown'].min()) if not daily_stats['drawdown'].empty else 0
    pct_dd_daily = (max_dd_daily / daily_stats['cumulative_pnl'].iloc[-1] * 100) if daily_stats['cumulative_pnl'].iloc[-1] != 0 else 0
    
    print(f"  An√°lise Di√°ria: R$ {max_dd_daily:.2f} ({pct_dd_daily:.2f}%)")
    
    # M√©todo 3: Gr√°fico (calcular_dados_grafico)
    grafico_data = calcular_dados_grafico(df_valid)
    if grafico_data:
        drawdowns_grafico = [abs(item['drawdown']) for item in grafico_data if not item.get('isStart', False)]
        max_dd_grafico = max(drawdowns_grafico) if drawdowns_grafico else 0
        print(f"  Gr√°fico: R$ {max_dd_grafico:.2f}")
    else:
        print(f"  Gr√°fico: N/A")
    
    # Verificar se todos os m√©todos produzem o mesmo resultado
    methods = [
        ("FunCalculos.py", max_dd_funcalculos),
        ("An√°lise Di√°ria", max_dd_daily),
        ("Gr√°fico", max_dd_grafico if 'max_dd_grafico' in locals() else 0)
    ]
    
    all_equal = len(set(method[1] for method in methods)) == 1
    print(f"  ‚úÖ Todos os m√©todos iguais: {all_equal}")
    
    if not all_equal:
        print("  ‚ö†Ô∏è DIFEREN√áAS ENCONTRADAS:")
        for method_name, value in methods:
            print(f"    {method_name}: R$ {value:.2f}")
    
    return {
        "funcalculos": max_dd_funcalculos,
        "daily": max_dd_daily,
        "grafico": max_dd_grafico if 'max_dd_grafico' in locals() else 0,
        "all_equal": all_equal
    }

def calcular_drawdown_padronizado(df: pd.DataFrame) -> Dict[str, float]:
    """
    Fun√ß√£o centralizada para calcular drawdown de forma padronizada
    Usada em todas as se√ß√µes para garantir consist√™ncia
    """
    if df.empty:
        return {
            "max_drawdown": 0.0,
            "max_drawdown_pct": 0.0,
            "saldo_final": 0.0,
            "capital_inicial": 0.0
        }
    
    # CORRE√á√ÉO: Normalizar o DataFrame se necess√°rio
    from FunCalculos import _normalize_trades_dataframe
    if 'entry_date' not in df.columns or 'pnl' not in df.columns:
        df = _normalize_trades_dataframe(df)
        if df.empty:
            return {
                "max_drawdown": 0.0,
                "max_drawdown_pct": 0.0,
                "saldo_final": 0.0,
                "capital_inicial": 0.0
            }
    
    # Filtrar trades v√°lidas
    df_valid = df.dropna(subset=['pnl', 'entry_date']).copy()
    df_valid = df_valid.sort_values('entry_date').reset_index(drop=True)
    
    if df_valid.empty:
        return {
            "max_drawdown": 0.0,
            "max_drawdown_pct": 0.0,
            "saldo_final": 0.0,
            "capital_inicial": 0.0
        }
    
    # Calcular equity curve trade por trade com baseline zero (padronizado)
    pnl_series = df_valid['pnl'].fillna(0).astype(float)
    equity = pnl_series.cumsum()
    equity_with_start = pd.concat([pd.Series([0.0]), equity], ignore_index=True)
    peak = equity_with_start.cummax()
    drawdown = equity_with_start - peak
    
    # Remover o ponto inicial artificial para an√°lises
    equity = equity_with_start.iloc[1:]
    peak = peak.iloc[1:]
    drawdown = drawdown.iloc[1:]
    
    # Drawdown m√°ximo (valor positivo)
    max_drawdown = float(abs(drawdown.min())) if not drawdown.empty else 0.0
    
    # Saldo final
    saldo_final = float(equity.iloc[-1]) if not equity.empty else 0.0
    
    # Capital inicial estimado: maior pico observado (considerando baseline 0)
    peak_max = float(peak.max()) if not peak.empty else 0.0
    capital_inicial = peak_max if peak_max > 0 else max_drawdown
    
    # Percentual do drawdown (baseado no capital inicial)
    max_drawdown_pct = (max_drawdown / capital_inicial * 100) if capital_inicial not in (0, np.nan) else 0.0
    
    # Logs de debug
    print(f"üîç DEBUG - Drawdown Padronizado:")
    print(f"  Max Drawdown: R$ {max_drawdown:.2f}")
    print(f"  Max Drawdown %: {max_drawdown_pct:.2f}%")
    print(f"  Saldo Final: R$ {saldo_final:.2f}")
    print(f"  Capital Inicial: R$ {capital_inicial:.2f}")
    
    return {
        "max_drawdown": max_drawdown,
        "max_drawdown_pct": max_drawdown_pct,
        "saldo_final": saldo_final,
        "capital_inicial": capital_inicial
    }

# ============ ROTAS DE CONFIGURA√á√ÉO DE COMISS√ïES ============

@app.route('/api/user/commission-settings', methods=['GET'])
@require_auth
def get_commission_settings():
    """
    Buscar as configura√ß√µes de comiss√£o do usu√°rio logado
    CORRE√á√ÉO: Separa corretagem e emolumentos
    Retorna defaults se n√£o existir configura√ß√£o salva
    """
    try:
        user_id = request.user_id
        
        if not supabase_client:
            print(f"[ERROR] Supabase client n√£o inicializado. SUPABASE_URL: {bool(SUPABASE_URL)}, SUPABASE_KEY: {bool(SUPABASE_KEY)}")
            return jsonify({
                "error": "Supabase n√£o configurado. Configure SUPABASE_URL e SUPABASE_KEY nas vari√°veis de ambiente."
            }), 500
        
        print(f"[DEBUG] Buscando configura√ß√µes para user_id: {user_id}")
        
        # Buscar configura√ß√µes do banco
        try:
            # Verificar se a tabela existe tentando fazer uma query simples
            response = supabase_client.table('user_commission_settings')\
                .select('*')\
                .eq('user_id', user_id)\
                .execute()
            print(f"[DEBUG] Resposta do Supabase: {len(response.data) if response.data else 0} registros encontrados")
        except Exception as db_error:
            error_msg = str(db_error)
            print(f"[ERROR] Erro ao buscar no Supabase: {error_msg}")
            import traceback
            traceback.print_exc()
            
            # Se o erro for sobre tabela n√£o encontrada, retornar defaults
            if 'relation' in error_msg.lower() and 'does not exist' in error_msg.lower():
                print(f"[WARN] Tabela user_commission_settings n√£o existe. Retornando defaults.")
                return jsonify({
                    "corretagem": {
                        "method": "fixed_per_roda",
                        "value": 0.50,
                        "overrideExisting": True
                    },
                    "emolumentos": {
                        "method": "fixed_per_roda",
                        "value": 0.03,
                        "overrideExisting": True
                    },
                    "applyDifferenceToPnl": True,
                    "configs": []
                }), 200
            
            return jsonify({
                "error": f"Erro ao buscar configura√ß√µes no banco: {error_msg}",
                "details": error_msg
            }), 500
        
        # Se n√£o encontrou, retornar defaults separados
        if not response.data or len(response.data) == 0:
            return jsonify({
                # Configura√ß√µes de corretagem
                "corretagem": {
                    "method": "fixed_per_roda",  # "fixed_per_roda" ou "fixed_per_trade"
                    "value": 0.50,  # R$ 0,50 por roda (padr√£o mercado brasileiro)
                    "overrideExisting": True
                },
                # Configura√ß√µes de emolumentos
                "emolumentos": {
                    "method": "fixed_per_roda",  # "fixed_per_roda" ou "percentage"
                    "value": 0.03,  # R$ 0,03 por roda (padr√£o mercado brasileiro)
                    "overrideExisting": True
                },
                "applyDifferenceToPnl": True,
                "configs": []  # Configura√ß√µes por ativo
            }), 200
        
        # Retornar configura√ß√µes encontradas (compatibilidade com formato antigo)
        data = response.data[0]
        
        asset_configs_count = len(data.get('asset_configs', [])) if isinstance(data.get('asset_configs'), list) else 0
        print(f"[DEBUG] Encontradas {asset_configs_count} configura√ß√µes de ativo no banco")
        if asset_configs_count > 0:
            print(f"[DEBUG] Primeira asset_config do banco: {data.get('asset_configs', [])[0]}")
            if asset_configs_count > 1:
                print(f"[DEBUG] √öltima asset_config do banco: {data.get('asset_configs', [])[-1]}")
        
        # Se tem formato antigo, converter para novo formato
        if 'corretagem' not in data and 'emolumentos' not in data:
            # Formato antigo - converter
            return jsonify({
                "corretagem": {
                    "method": data.get('corretagem_method', 'fixed_per_roda'),
                    "value": float(data.get('corretagem_value', 0.50)),
                    "overrideExisting": data.get('corretagem_override_existing', True)
                },
                "emolumentos": {
                    "method": data.get('emolumentos_method', 'fixed_per_roda'),
                    "value": float(data.get('emolumentos_value', 0.03)),
                    "overrideExisting": data.get('emolumentos_override_existing', True)
                },
                "applyDifferenceToPnl": data.get('apply_difference_to_pnl', True),
                "configs": data.get('asset_configs', [])
            }), 200
        
        # Formato novo
        return jsonify({
            "corretagem": data.get('corretagem', {
                "method": "fixed_per_roda",
                "value": 0.50,
                "overrideExisting": True
            }),
            "emolumentos": data.get('emolumentos', {
                "method": "fixed_per_roda",
                "value": 0.03,
                "overrideExisting": True
            }),
            "applyDifferenceToPnl": data.get('apply_difference_to_pnl', True),
            "configs": data.get('asset_configs', [])
        }), 200
        
    except Exception as e:
        print(f"[ERROR] Erro ao buscar configura√ß√µes de comiss√£o: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({
            "error": f"Erro ao buscar configura√ß√µes: {str(e)}",
            "details": str(e),
            "type": type(e).__name__
        }), 500

@app.route('/api/user/commission-settings', methods=['PUT'])
@require_auth
def save_commission_settings():
    """
    Salvar/atualizar as configura√ß√µes de comiss√£o do usu√°rio logado
    CORRE√á√ÉO: Separa corretagem e emolumentos
    """
    try:
        user_id = request.user_id
        
        if not supabase_client:
            return jsonify({
                "error": "Supabase n√£o configurado. Configure SUPABASE_URL e SUPABASE_KEY nas vari√°veis de ambiente."
            }), 500
        
        data = request.get_json()
        if not data:
            return jsonify({"error": "Body da requisi√ß√£o vazio"}), 400
        
        # CORRE√á√ÉO: Extrair configura√ß√µes separadas de corretagem e emolumentos
        corretagem_config = data.get('corretagem', {})
        emolumentos_config = data.get('emolumentos', {})
        
        # Compatibilidade com formato antigo
        if not corretagem_config and not emolumentos_config:
            # Formato antigo - converter
            default_method = data.get('defaultMethod', 'fixed')
            default_value = data.get('defaultValue', 0)
            corretagem_config = {
                "method": default_method,
                "value": default_value,
                "overrideExisting": data.get('overrideExisting', True)
            }
            emolumentos_config = {
                "method": default_method,
                "value": default_value,
                "overrideExisting": data.get('overrideExisting', True)
            }
        
        # Valida√ß√µes para corretagem
        corretagem_method = corretagem_config.get('method', 'fixed_per_roda')
        corretagem_value = corretagem_config.get('value', 0.50)
        if corretagem_method not in ['fixed_per_roda', 'fixed_per_trade']:
            return jsonify({"error": "corretagem.method deve ser 'fixed_per_roda' ou 'fixed_per_trade'"}), 400
        if not isinstance(corretagem_value, (int, float)) or corretagem_value < 0:
            return jsonify({"error": "corretagem.value deve ser um n√∫mero >= 0"}), 400
        
        # Valida√ß√µes para emolumentos
        emolumentos_method = emolumentos_config.get('method', 'fixed_per_roda')
        emolumentos_value = emolumentos_config.get('value', 0.03)
        if emolumentos_method not in ['fixed_per_roda', 'percentage']:
            return jsonify({"error": "emolumentos.method deve ser 'fixed_per_roda' ou 'percentage'"}), 400
        if not isinstance(emolumentos_value, (int, float)) or emolumentos_value < 0:
            return jsonify({"error": "emolumentos.value deve ser um n√∫mero >= 0"}), 400
        
        apply_difference_to_pnl = data.get('applyDifferenceToPnl', True)
        configs = data.get('configs', [])
        
        print(f"[DEBUG] Recebido {len(configs) if isinstance(configs, list) else 0} configura√ß√µes de ativo")
        if isinstance(configs, list) and len(configs) > 0:
            print(f"[DEBUG] Primeira config: {configs[0]}")
            if len(configs) > 1:
                print(f"[DEBUG] √öltima config: {configs[-1]}")
        
        if not isinstance(configs, list):
            return jsonify({"error": "configs deve ser um array"}), 400
        
        # Validar cada configura√ß√£o de ativo
        for i, config in enumerate(configs):
            if not isinstance(config, dict):
                return jsonify({"error": f"configs[{i}] deve ser um objeto"}), 400
            
            asset = config.get('asset')
            if not asset or not isinstance(asset, str) or asset.strip() == '':
                return jsonify({"error": f"configs[{i}].asset deve ser uma string n√£o vazia"}), 400
            
            # Validar corretagem do ativo
            if 'corretagem' in config:
                if config['corretagem'].get('method') not in ['fixed_per_roda', 'fixed_per_trade']:
                    return jsonify({"error": f"configs[{i}].corretagem.method deve ser 'fixed_per_roda' ou 'fixed_per_trade'"}), 400
                if not isinstance(config['corretagem'].get('value'), (int, float)) or config['corretagem'].get('value') < 0:
                    return jsonify({"error": f"configs[{i}].corretagem.value deve ser um n√∫mero >= 0"}), 400
            
            # Validar emolumentos do ativo
            if 'emolumentos' in config:
                if config['emolumentos'].get('method') not in ['fixed_per_roda', 'percentage']:
                    return jsonify({"error": f"configs[{i}].emolumentos.method deve ser 'fixed_per_roda' ou 'percentage'"}), 400
                if not isinstance(config['emolumentos'].get('value'), (int, float)) or config['emolumentos'].get('value') < 0:
                    return jsonify({"error": f"configs[{i}].emolumentos.value deve ser um n√∫mero >= 0"}), 400
        
        # Preparar dados para salvar
        asset_configs = []
        for config in configs:
            asset_config = {
                "asset": config['asset'].upper().strip(),
                "corretagem": config.get('corretagem', corretagem_config),
                "emolumentos": config.get('emolumentos', emolumentos_config)
            }
            asset_configs.append(asset_config)
        
        print(f"[DEBUG] Preparando para salvar {len(asset_configs)} configura√ß√µes de ativo")
        if len(asset_configs) > 0:
            print(f"[DEBUG] Primeira asset_config: {asset_configs[0]}")
            if len(asset_configs) > 1:
                print(f"[DEBUG] √öltima asset_config: {asset_configs[-1]}")
        
        # Salvar no banco (upsert)
        upsert_data = {
            "user_id": user_id,
            "corretagem": {
                "method": corretagem_method,
                "value": float(corretagem_value),
                "override_existing": bool(corretagem_config.get('overrideExisting', True))
            },
            "emolumentos": {
                "method": emolumentos_method,
                "value": float(emolumentos_value),
                "override_existing": bool(emolumentos_config.get('overrideExisting', True))
            },
            "apply_difference_to_pnl": bool(apply_difference_to_pnl),
            "asset_configs": asset_configs
        }
        
        # Tentar salvar usando o cliente Supabase
        # Se estiver usando SERVICE_ROLE_KEY, bypassa RLS automaticamente
        try:
            response = supabase_client.table('user_commission_settings')\
                .upsert(upsert_data, on_conflict='user_id')\
                .execute()
            
            if not response.data:
                return jsonify({"error": "Erro ao salvar configura√ß√µes"}), 500
        except Exception as db_error:
            error_str = str(db_error)
            # Verificar se √© erro de RLS
            if 'row-level security' in error_str.lower() or '42501' in error_str:
                print(f"[ERROR] Erro de RLS ao salvar configura√ß√µes: {db_error}")
                print(f"[ERROR] Isso geralmente acontece quando n√£o est√° usando SERVICE_ROLE_KEY")
                return jsonify({
                    "error": "Erro ao salvar configura√ß√µes: viola√ß√£o de pol√≠tica de seguran√ßa (RLS). Configure SUPABASE_SERVICE_ROLE_KEY no backend para bypassar RLS.",
                    "details": str(db_error)
                }), 500
            # Re-raise outros erros
            raise
        
        # Verificar o que foi salvo
        saved_data = response.data[0] if response.data else {}
        saved_asset_configs = saved_data.get('asset_configs', [])
        print(f"[DEBUG] Dados salvos no banco: {len(saved_asset_configs) if isinstance(saved_asset_configs, list) else 0} configura√ß√µes")
        if isinstance(saved_asset_configs, list) and len(saved_asset_configs) > 0:
            print(f"[DEBUG] Primeira config salva: {saved_asset_configs[0]}")
            if len(saved_asset_configs) > 1:
                print(f"[DEBUG] √öltima config salva: {saved_asset_configs[-1]}")
        
        return jsonify({
            "success": True,
            "message": "Configura√ß√µes salvas com sucesso",
            "settings": upsert_data
        }), 200
        
    except Exception as e:
        print(f"[ERROR] Erro ao salvar configura√ß√µes de comiss√£o: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({"error": f"Erro ao salvar configura√ß√µes: {str(e)}"}), 500

if __name__ == '__main__':
    try:
        app.run(host='0.0.0.0',
                port=5002,
                debug=False,
                use_reloader=False)
    except Exception as e:
        print(f"Erro ao iniciar servidor: {e}")
        import traceback
        traceback.print_exc()
        traceback.print_exc()